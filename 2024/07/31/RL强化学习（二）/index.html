<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【RL】Value-Based Learning | XcloveHsy's Blog</title><meta name="author" content="XuCong"><meta name="copyright" content="XuCong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="【RL】Value-Based Learning 基于价值学习 Discounted Return \[ \begin{aligned} U_t&amp; &#x3D;R_t+\gamma\cdot R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot R_{t+3}+\gamma^4\cdot R_{t+4} \\ &amp; &#x3D;R_t+\gamma\cdo">
<meta property="og:type" content="article">
<meta property="og:title" content="【RL】Value-Based Learning">
<meta property="og:url" content="https://xclovehsy.github.io/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="XcloveHsy&#39;s Blog">
<meta property="og:description" content="【RL】Value-Based Learning 基于价值学习 Discounted Return \[ \begin{aligned} U_t&amp; &#x3D;R_t+\gamma\cdot R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot R_{t+3}+\gamma^4\cdot R_{t+4} \\ &amp; &#x3D;R_t+\gamma\cdo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png">
<meta property="article:published_time" content="2024-07-31T10:05:03.000Z">
<meta property="article:modified_time" content="2024-08-02T03:41:05.404Z">
<meta property="article:author" content="XuCong">
<meta property="article:tag" content="价值学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png"><link rel="shortcut icon" href="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg"><link rel="canonical" href="https://xclovehsy.github.io/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【RL】Value-Based Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-02 11:41:05'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><meta name="generator" content="Hexo 5.4.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png')"><nav id="nav"><span id="blog-info"><a href="/" title="XcloveHsy's Blog"><img class="site-icon" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg"/><span class="site-name">XcloveHsy's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【RL】Value-Based Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-31T10:05:03.000Z" title="发表于 2024-07-31 18:05:03">2024-07-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-02T03:41:05.404Z" title="更新于 2024-08-02 11:41:05">2024-08-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【RL】Value-Based Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="rlvalue-based-learning">【RL】Value-Based Learning</h1>
<h2 id="基于价值学习">基于价值学习</h2>
<h3 id="discounted-return">Discounted Return</h3>
<p><span class="math display">\[
\begin{aligned}
U_t&amp; =R_t+\gamma\cdot R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot
R_{t+3}+\gamma^4\cdot R_{t+4} \\
&amp; =R_t+\gamma\cdot (R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot
R_{t+3}+\gamma^4\cdot R_{t+4}) \\
&amp; =R_t+\gamma\cdot U_{t+1}
\end{aligned}
\]</span></p>
<p><strong>Identity:</strong> <span class="math inline">\(U_t=R_t+\gamma\cdot U_{t+1}\)</span></p>
<h3 id="td-target">TD Target</h3>
<p>Assume <span class="math inline">\(R_t\)</span> depends on <span class="math inline">\((S_t, A_t, S_{t+1})\)</span> <span class="math display">\[
\begin{aligned}
Q_\pi\left(s_t, a_t\right) &amp; =\mathbb{E}\left[U_t \mid s_t,
a_t\right] \\
&amp; =\mathbb{E}\left[R_t+\gamma \cdot U_{t+1} \mid s_t, a_t\right] \\
&amp; =\mathbb{E}\left[R_t \mid s_t, a_t\right]+\gamma \mathbb{\mathbb {
E } [ U _ { t + 1 } | s _ { t } , a _ { t } ]} \\
&amp; =\mathbb{E}\left[R_t \mid s_t, a_t\right]+\gamma \cdot
\mathbb{E}\left[Q_\pi\left(s_{t+1}, A_{t+1}\right) \mid s_t, a_t\right]
.
\end{aligned}
\]</span> <strong>Identity:</strong> <span class="math inline">\(Q_\pi(s_t,a_t)=\mathbb{E}[R_t+\gamma\cdot
Q_\pi(S_{t+1},A_{t+1})]\)</span> , for all <span class="math inline">\(\pi\)</span></p>
<p>左边是该动作的action-value
function，右边含有期望，直接计算比较困难，故利用蒙特卡洛近似计算。</p>
<p><span class="math inline">\(R_t\)</span> 近似为观测值<span class="math inline">\(r_t\)</span>, <span class="math inline">\(Q_\pi(S_{t+1},A_{t+1})\)</span>中<span class="math inline">\(S_{t+1},A_{t+1}\)</span>都是随机变量，利用观测值<span class="math inline">\(s_{t+1},a_{t+1}\)</span>近似 <span class="math display">\[
R_t+\gamma\cdot Q_\pi(S_{t+1},A_{t+1}) \approx r_t+\gamma\cdot
Q_\pi(s_{t+1},a_{t+1}) = TD\ target
\]</span> 期望<span class="math inline">\(\mathbb{E}[R_t+\gamma\cdot
Q_\pi(S_{t+1},A_{t+1})] \approx y_t\)</span></p>
<p><strong>TD learning: Encourage <span class="math inline">\(Q_\pi(s_t,a_t)\)</span> to approach <span class="math inline">\(y_t\)</span></strong></p>
<p>因为<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>完全是估计，而<span class="math inline">\(y_t\)</span>部分基于真实的奖励，相对于更加可靠</p>
<h3 id="td-error">TD Error</h3>
<p><span class="math display">\[
\delta_t=Q_\pi(s_t,a_t)-y_t
\]</span></p>
<h2 id="sarsa">Sarsa</h2>
<p><strong>学习目标</strong>：更新<span class="math inline">\(Q_\pi\)</span> 使其更加接近真实值</p>
<p>每一轮迭代更新都依据五元组<span class="math inline">\((s_t,a_t,r_t,s_{t+1},a_{t+1})\)</span>更新<span class="math inline">\(Q_\pi\)</span></p>
<p>因此命名State-Action-Reward-State-Action（SARSA）</p>
<p>更新q-table时采用下一步真实的s‘和a’更新q值，也称为on-policy在线学习</p>
<h3 id="表格形式">表格形式</h3>
<p>State和Action的数量是有限的（离散），通过一个表格来更新<span class="math inline">\(Q_\pi\)</span></p>
<p>每次更新表格中的一个元素，使得TD Error变小</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>Sample <span class="math inline">\(a_{t+1} \sim \pi\left(\cdot \mid
s_{t+1}\right)\)</span>, where <span class="math inline">\(\pi\)</span>
is the policy function.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q_\pi\left(s_{t+1}, a_{t+1}\right)\)</span>. 下一个动作的q值</li>
<li>TD error: <span class="math inline">\(\delta_t=Q_\pi\left(s_t,
a_t\right)-y_t\)</span>.</li>
<li>Update: <span class="math inline">\(Q_\pi\left(s_t, a_t\right)
\leftarrow Q_\pi\left(s_t, a_t\right)-\alpha \cdot
\delta_t\)</span>.</li>
</ol>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185059478.png" alt="image-20240731185059478">
<figcaption aria-hidden="true">image-20240731185059478</figcaption>
</figure>
<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>():</span></span><br><span class="line">    ....</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q = self.q_table.loc[s,a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            <span class="comment"># 下个行动所采取的真实值</span></span><br><span class="line">            q_ = r + self.gamma * self.q_table.loc[s_, a_]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Q learning 中采用下个阶段可能行动的最大值</span></span><br><span class="line">            <span class="comment"># q_ = r + self.gamma * self.q_table.loc[s_, :].max()</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_ = r</span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_ - q)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> epis <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        observation = env.reset()</span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            env.render()</span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取下个阶段的action，并利用s_和a_同时更新参数</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 记录下个阶段的a_和s_</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="神经网络形式">神经网络形式</h3>
<p>价值网络<span class="math inline">\(q(s,a;w)\)</span>是对动作价值函数<span class="math inline">\(Q_\pi\)</span>的近似，利用sarsa算法更新网络参数</p>
<p>在actor-critic方法中用于对价值网络的参数进行改进</p>
<p>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot
q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)\)</span>.</p>
<p>TD error: <span class="math inline">\(\delta_t=q\left(s_t, a_t ;
\mathbf{w}\right)-y_t\)</span>.</p>
<p>Loss: <span class="math inline">\(\delta_t^2 / 2\)</span>.</p>
<p>Gradient: <span class="math inline">\(\frac{\partial \delta_t^2 /
2}{\partial \mathbf{w}}=\delta_t \cdot \frac{\partial q\left(s_t, a_t ;
\mathbf{w}\right)}{\partial \mathbf{w}}\)</span>.</p>
<p>Gradient descent: <span class="math inline">\(\quad \mathbf{w}
\leftarrow \mathbf{w}-\alpha \delta_t \cdot \frac{\partial q\left(s_t,
a_t ; \mathbf{w}\right)}{\partial \mathbf{w}}\)</span></p>
<h3 id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</h3>
<p>这种方法针对于表格形式Sarsa</p>
<p>Sarsa(<span class="math inline">\(\lambda\)</span>)就是更新获取到
reward 的前 lambda 步. lambda 是在 [0, 1] 之间取值。</p>
<p>不仅更新当前的<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>同时更新当前回合前面路径state-action的<span class="math inline">\(Q_\pi(s,a)\)</span></p>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185138592.png" alt="image-20240731185138592">
<figcaption aria-hidden="true">image-20240731185138592</figcaption>
</figure>
<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line">        self.lambda_ = trace_decay <span class="comment"># backward view, eligibility trace.</span></span><br><span class="line">        self.eligibility_trace = self.q_table.copy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            to_be_append = pd.Series([<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),index=self.q_table.columns,name=state,)</span><br><span class="line">            self.q_table = self.q_table.append(to_be_append) <span class="comment"># append new state to q table</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append) <span class="comment"># also update eligibility trace</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="comment"># next state is not terminal</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># next state is terminal</span></span><br><span class="line">        error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里开始不同:</span></span><br><span class="line">        <span class="comment"># 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环</span></span><br><span class="line">        <span class="comment"># Method 1:</span></span><br><span class="line">        <span class="comment"># self.eligibility_trace.loc[s, a] += 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Method 2:</span></span><br><span class="line">        self.eligibility_trace.loc[s, :] *= <span class="number">0</span></span><br><span class="line">        self.eligibility_trace.loc[s, a] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q update</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decay eligibility trace after update</span></span><br><span class="line">        self.eligibility_trace *= self.gamma*self.lambda_</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> epis <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        observation = env.reset()</span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">        RL.eligibility_trace *= <span class="number">0</span> <span class="comment"># eligibility trace 只是记录每个回合的每一步, 新回合开始的时候需要将 Trace 清零.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            ....</span><br></pre></td></tr></table></figure>
<h2 id="q-learning">Q-Learning</h2>
<h3 id="学习目标">学习目标</h3>
<p>训练最优价值函数<span class="math inline">\(Q^\star(s,a)\)</span></p>
<p><strong>TD Target:</strong> <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q^{\star}\left(s_{t+1}, a\right) .
\]</span> 利用Q-learning更新DQN</p>
<h3 id="数学推导">数学推导</h3>
<p>通过Sarsa部分推导,对于所有的 <span class="math inline">\(\pi\)</span>, <span class="math display">\[
Q_\pi\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot
Q_\pi\left(S_{t+1}, A_{t+1}\right)\right] .
\]</span> 如果选择最优的策略<span class="math inline">\(\pi^\star\)</span> <span class="math display">\[
Q_{\pi^{\star}}\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot
Q_{\pi^{\star}}\left(S_{t+1}, A_{t+1}\right)\right] .
\]</span> <span class="math inline">\(Q_{\pi^{\star}}\)</span> 和 <span class="math inline">\(Q^{\star}\)</span> 都表示最优动作价值函数</p>
<p>Identity: <span class="math inline">\(Q^{\star}\left(s_t,
a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot Q^{\star}\left(S_{t+1},
A_{t+1}\right)\right]\)</span>.</p>
<p>动作<span class="math inline">\(A_{t+1}\)</span>通过以下公式计算
<span class="math display">\[
A_{t+1}=\underset{a}{\operatorname{argmax}} Q^{\star}\left(S_{t+1},
a\right) .
\]</span></p>
<p>因此 <span class="math inline">\(Q^{\star}\left(S_{t+1},
A_{t+1}\right)=\max _a Q^{\star}\left(S_{t+1}, a\right)\)</span>.</p>
<p>所以<span class="math inline">\(Q^{\star}\left(s_t,
a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot \max
_aQ^{\star}\left(S_{t+1}, a\right)\right]\)</span>.</p>
<p>期望直接求比较困难,所以利用蒙特卡洛近似,<span class="math inline">\(R_t\)</span>利用观测值<span class="math inline">\(r_t\)</span>近似,<span class="math inline">\(S_{t+1}\)</span>利用观测值<span class="math inline">\(s_{t+1}\)</span>近似得到: <span class="math display">\[
Q^{\star}\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot \max
_aQ^{\star}\left(S_{t+1},
a\right)\right]\approx\mathbb{E}\left[r_t+\gamma \cdot \max
_aQ^{\star}\left(s_{t+1}, a\right)\right]=TD\ Target\ y_t
\]</span></p>
<p>由此，可以利用表格形式学习<span class="math inline">\(Q_\pi\)</span>,针对离散action、state的情况。也可利用神经网络学习<span class="math inline">\(Q_\pi\)</span>
可以用于连续action、state的DQN方法</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png" alt="image-20231205105624651">
<figcaption aria-hidden="true">image-20231205105624651</figcaption>
</figure>
<h3 id="表格形式-1">表格形式</h3>
<p>针对<strong>离散状态、离散动作</strong>情况,当状态和动作数量变多时，表格会越来越大</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q^{\star}\left(s_{t+1}, a\right)\)</span>.</li>
<li>TD error: <span class="math inline">\(\delta_t=Q^{\star}\left(s_t,
a_t\right)-y_t\)</span>.</li>
<li>Update: <span class="math inline">\(\quad Q^{\star}\left(s_t,
a_t\right) \leftarrow Q^{\star}\left(s_t, a_t\right)-\alpha \cdot
\delta_t\)</span>.</li>
</ol>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185159947.png" alt="image-20240731185159947">
<figcaption aria-hidden="true">image-20240731185159947</figcaption>
</figure>
<p>实现代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Q-learning算法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 横轴为ations, 纵轴为states</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        选择 action</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.check_state_exist(<span class="built_in">str</span>(observation))</span><br><span class="line">        p_actions = self.q_table.loc[observation, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> np.random.choice(p_actions[p_actions == np.<span class="built_in">max</span>(p_actions)].index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.random.choice(self.actions)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新模型参数 动作-状态价值函数（Q值）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q(st,at) 原始估计</span></span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用reward,state_计算Q_(st,at)新的估计值</span></span><br><span class="line">        q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用贝尔曼方程更新Q(st,at)的值</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        检查当前的状态是否在q-table表中出现，不存在则添加一行</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            self.q_table = self.q_table.append(pd.Series([<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),index=self.q_table.columns,name=state))</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate the reward of q-table</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollout</span>(<span class="params">RL, env, printout=<span class="literal">False</span></span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q_table, env</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    train the q-table with compilerGym env</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, FLAGS.episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step_episode <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.episode_length):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = q_table.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">            action_label = FLAGS.flags[action]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据action给出下一个state，reward，是否终止</span></span><br><span class="line">            observation_, reward, done, info = env.step(env.action_space.flags.index(action_label))</span><br><span class="line">            observation_ = observation_[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 通过给出反馈，更新模型参数</span></span><br><span class="line">            q_table.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state_ 变为 下次循环的 state</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="dqn网络形式">DQN（网络形式）</h3>
<p><strong>学习目标：</strong>更新网络参数<span class="math inline">\(w\)</span>,学习最优动作价值函数<span class="math inline">\(Q^\star\)</span></p>
<p><strong>局限性：</strong>动作要求是<strong>离散</strong>的，当动作维度增大时，神经网络的规模会指数增加</p>
<p>利用神经网络 DQN, <span class="math inline">\(Q(s, a ;
\mathbf{w})\)</span> 近似 <span class="math inline">\(Q^{\star}(s,
a)\)</span>，用神经网络拟合状态到Q值之间的映射关系。状态作为神经网络的输入，可以在<strong>连续</strong>范围内取值，最终输出得到的是该状态下对应每个动作的Q值，然后我们可以在其中选择一个令Q值最大的作为最优动作。</p>
<p>网络的输入为State，输出为各Action的Q</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205111712543.png" alt="image-20231205111712543">
<figcaption aria-hidden="true">image-20231205111712543</figcaption>
</figure>
<p>通过DQN网络输出的最大Q值的动作 <span class="math inline">\(a_t=\underset{a}{\operatorname{argmax}}
Q\left(s_t, a ; \mathbf{w}\right)\)</span>控制agent做动作</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q\left(s_{t+1}, a;w\right)\)</span>.</li>
<li>TD error: <span class="math inline">\(\delta_t=Q\left(s_t,
a_t;w\right)-y_t\)</span>.</li>
<li>Loss: <span class="math inline">\(L_t=\frac{1}{2}[Q(s_t,a_t;w)-y_t]^2\)</span></li>
<li>Update: <span class="math inline">\(w \leftarrow w-\alpha \cdot
\delta_t\cdot\frac{\partial Q(s_t,a_t;w)}{\partial w}\)</span>.</li>
</ol>
<h2 id="multi-step-td-target">Multi-Step TD Target</h2>
<h3 id="sarsa-vs-q-learning">Sarsa vs Q-learning</h3>
<p>Sarsa 训练动作价值函数（action-value function）<span class="math inline">\(Q\pi(s,a)\)</span></p>
<p>TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q_\pi\left(s_{t+1}, a_{t+1}\right)\)</span></p>
<p>Q-Learning训练最优动作价值函数（optimal action-value function）<span class="math inline">\(Q^\star (s,a)\)</span></p>
<p>TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q^{\star}\left(s_{t+1}, a\right)\)</span></p>
<p>前面学习的Sarsa以及Q-Learning都是只使用一个transition<span class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>
其中包含一个reward，称为One-Step TD Target</p>
<p>此外可以同时使用多步transition中的<span class="math inline">\(r_t,r_{t+1},r_{t+2}...r_{t+m-1}\)</span>进行更新，这样比单步的学习效果更好。由于包含多步reward，因此Q值更加接近于真实值</p>
<h3 id="数学推导-1">数学推导</h3>
<p><span class="math display">\[
\begin{aligned}
U_{t}&amp;=R_{t}+\gamma \cdot U_{t+1} \\&amp;=R_{t}+\gamma \cdot
R_{t+1}+\gamma^{2} \cdot U_{t+2} \\&amp;=R_{t}+\gamma \cdot
R_{t+1}+\gamma^{2} \cdot R_{t+2}+\gamma^{3} \cdot U_{t+3}
\end{aligned}
\]</span></p>
<p>Identity: <span class="math inline">\(U_{t}=\sum_{i=0}^{m-1}
\gamma^{i} \cdot R_{t+i}+\gamma^{m} \cdot U_{t+m} .\)</span></p>
<p>m-step TD target应用于Sarsa算法 <span class="math display">\[
y_{t}=\sum_{i=0}^{m-1} \gamma^{i} \cdot r_{t+i}+\gamma^{m} \cdot
Q_\pi(s_{t+m},a_{t+m}) .
\]</span></p>
<p>m-step TD target应用于Q-Learing算法 <span class="math display">\[
y_{t}=\sum_{i=0}^{m-1} \gamma^{i} \cdot r_{t+i}+\gamma^{m} \cdot \max
_aQ^\star(s_{t+m},a) .
\]</span></p>
<p>如果将m设置为1，则为标准的Sarsa、Q-Learning算法</p>
<h2 id="经验回放">经验回放</h2>
<p>在前面DQN+TD-Learning中依次利用各<span class="math inline">\(transition(s_t,a_t,r_t,s_{t+1}),t=1,2,..\)</span>来更新网络参数<span class="math inline">\(w\)</span>，更新后丢弃此transition不再使用</p>
<p>在实际环境中<span class="math inline">\(s_t\)</span> 与<span class="math inline">\(s_{t+1}\)</span>之间有非常明显的相关性，这种相关性对模型的训练是有害的，如果能打散这种相关性，则有利于提升模型的训练效率</p>
<p>因此可以将最近的n条transition保存在一个大小为n队列（memory）中，如果memory满了，则利用最老的memory进行替换</p>
<p>Find <span class="math inline">\(\mathbf{w}\)</span> by minimizing
<span class="math inline">\(L(\mathbf{w})=\frac{1}{T} \sum_{t=1}^T
\frac{\delta_t^2}{2}\)</span>.</p>
<p>Stochastic gradient descent (SGD):
(一般使用mini-bach的方法，抽取一组transition，计算loss的平均值来更新网络参数)</p>
<ul>
<li>Randomly sample a transition, <span class="math inline">\(\left(s_i,
a_i, r_i, s_{i+1}\right)\)</span>, from the buffer.</li>
<li>Compute TD error, <span class="math inline">\(\delta_i\)</span>.</li>
<li>Stochastic gradient: <span class="math inline">\(\mathbf{g}_i=\frac{\partial \delta_i^2 /
2}{\partial \mathbf{w}}=\delta_i \cdot \frac{\partial Q\left(s_i, a_i ;
\mathbf{w}\right)}{\partial \mathbf{w}}\)</span></li>
<li>SGD: <span class="math inline">\(\mathbf{w} \leftarrow
\mathbf{w}-\alpha \cdot \mathbf{g}_i\)</span>.</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>可重复利用历史transition，记忆库 (用于重复学习)</li>
<li>打破transition之间的相关性，暂时冻结 <code>q_target</code> 参数
(切断相关性)</li>
</ul>
<p>算法</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731183751203.png" alt="image-20240731183751203">
<figcaption aria-hidden="true">image-20240731183751203</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算DQN的reward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollout</span>(<span class="params">RL, env, printout=<span class="literal">False</span></span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">RL, env</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练DQN</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 记录历史reward</span></span><br><span class="line">    history_reword = []</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, FLAGS.episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()[FLAGS.features_indices]</span><br><span class="line">        <span class="keyword">for</span> step_episode <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.episode_length):</span><br><span class="line">            action = RL.choose_action(observation) <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action_label = FLAGS.flags[action]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据action给出下一个state，reward，是否终止</span></span><br><span class="line">            observation_, reward, done, info = env.step(env.action_space.flags.index(action_label))</span><br><span class="line">            observation_ = observation_[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 记录s, a, r, s_,用于获取更新网络参数</span></span><br><span class="line">            RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 控制学习的起始时间和频率（先累积一些记忆再开始学习）</span></span><br><span class="line">            <span class="keyword">if</span> (step &gt; <span class="number">200</span>) <span class="keyword">and</span> (step % <span class="number">5</span> == <span class="number">0</span>):</span><br><span class="line">                RL.learn()</span><br><span class="line"></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">          ...</span><br></pre></td></tr></table></figure>
<h3 id="神经网络与思维决策">神经网络与思维决策</h3>
<p>搭建两个神经网络（见高估问题 Target Network）</p>
<ul>
<li>Target Network 用于计算值TD Target, 不会及时更新参数</li>
<li>DQN用于控制agent做运动，并收集transition,
这个神经网络拥有最新的神经网络参数.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义网络结构，三层神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.el = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.q = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.el(x)</span><br><span class="line">        x = F.relu(x)  <span class="comment"># relu作为激活函数</span></span><br><span class="line">        x = self.q(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_actions, n_features, n_hidden=<span class="number">20</span>, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 replace_target_iter=<span class="number">200</span>, memory_size=<span class="number">500</span>, batch_size=<span class="number">32</span>, seed=<span class="literal">None</span>, e_greedy_increment=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        self.n_actions = n_actions  <span class="comment"># action维度</span></span><br><span class="line">        self.n_features = n_features  <span class="comment"># observation/state维度</span></span><br><span class="line">        self.n_hidden = n_hidden  <span class="comment"># 隐藏层神经元个数</span></span><br><span class="line">        self.lr = learning_rate  <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay  <span class="comment"># 回报衰退率</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 替换target网络参数的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记录的size</span></span><br><span class="line">        self.memory_counter = <span class="number">0</span>  <span class="comment"># memory当前记录的idx</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.epsilon_max = e_greedy</span><br><span class="line">        self.epsilon_increment = e_greedy_increment</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选择网络预测action的概率，一开始随机选择action，随着训练逐步增大</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max  </span><br><span class="line">        </span><br><span class="line">        self.learn_step_counter = <span class="number">0</span>  <span class="comment"># total learning step</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features * <span class="number">2</span> + <span class="number">2</span>))  <span class="comment"># initialize zero memory [s, a, r, s_]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.set_random_seed(seed)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()  <span class="comment"># 均方误差</span></span><br><span class="line">        self.cost_his = []  <span class="comment"># 记录cost的历史值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建网络</span></span><br><span class="line">        self._build_net()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化网络</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 创建 eval 神经网络, 及时提升参数</span></span><br><span class="line">        self.q_eval = Net(self.n_features, self.n_hidden, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 target 神经网络, 提供 target Q</span></span><br><span class="line">        self.q_target = Net(self.n_features, self.n_hidden, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用RMSprop作为优化器</span></span><br><span class="line">        self.optimizer = torch.optim.RMSprop(self.q_eval.parameters(), lr=self.lr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        记录历史的state、action、reward、state_</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size</span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        利用q_eval网络 根据观测值选择行为</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将一维数据转换成矩阵</span></span><br><span class="line">        <span class="comment"># x[:, np.newaxis] ：放在后面，会给列上增加维度；</span></span><br><span class="line">        <span class="comment"># x[np.newaxis, :] ：放在前面，会给行上增加维度；</span></span><br><span class="line">        observation = torch.Tensor(observation[np.newaxis, :])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="comment"># 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action</span></span><br><span class="line">            actions_value = self.q_eval(observation)</span><br><span class="line">            action = np.argmax(actions_value.data.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 随机选择action</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新网络参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 更新 target_net 参数</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target.load_state_dict(self.q_eval.state_dict())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 memory 中随机抽取 batch_size 大小记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算target_net的q值</span></span><br><span class="line">        q_next = self.q_target(torch.Tensor(batch_memory[:, -self.n_features:])) </span><br><span class="line">        <span class="comment"># 计算eval_net的q值</span></span><br><span class="line">        q_eval = self.q_eval(torch.Tensor(batch_memory[:, :self.n_features])) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># =========== 计算loss值 ==============</span></span><br><span class="line">        <span class="comment"># 将q_eval的值复制到q_target</span></span><br><span class="line">        q_target = torch.Tensor(q_eval.data.numpy().copy())	</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每transition的action</span></span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每transition的reward</span></span><br><span class="line">        reward = torch.Tensor(batch_memory[:, self.n_features + <span class="number">1</span>])	</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q_target每个样本对应action的概率赋值为reward + gamma * maxQ(s_)</span></span><br><span class="line">        <span class="comment"># torch.max(q_next, 1)[0] 选择每一行的最大值组成一个列表</span></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * torch.<span class="built_in">max</span>(q_next, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        loss = self.loss_func(q_eval, q_target)	<span class="comment"># 将 (q_target - q_eval) 作为误差</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        只需要选择好的 action 的值, 其他的并不需要.</span></span><br><span class="line"><span class="string">        所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line"><span class="string">        这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line"><span class="string">        q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.</span></span><br><span class="line"><span class="string">        q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,</span></span><br><span class="line"><span class="string">        我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line"><span class="string">        q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        q_target = q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line"><span class="string">        比如在:</span></span><br><span class="line"><span class="string">            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;</span></span><br><span class="line"><span class="string">            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:</span></span><br><span class="line"><span class="string">        q_target =</span></span><br><span class="line"><span class="string">        [[-1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, -2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        所以 (q_target - q_eval) 就变成了:</span></span><br><span class="line"><span class="string">        [[(-1)-(1), 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, (-2)-(6)]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.</span></span><br><span class="line"><span class="string">        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line"><span class="string">        我们只反向传递之前选择的 action 的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传递会神经网络</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐步增大选择网络预测aciton的概率</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.cost_his.append(loss) <span class="comment"># 记录历史的cost值</span></span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="优先经验回放">优先经验回放</h3>
<p>前面的算法在抽样时平等的看待每条transition，但是真实环境中每条transition的重要性是不同的，利用带权重的抽样来代替均匀抽样体现不同transition的重要性</p>
<p>Option 1: Sampling probability <span class="math inline">\(p_t
\propto\left|\delta_t\right|+\epsilon\)</span>.</p>
<p>Option 2: Sampling probabilit <span class="math inline">\(p_t \propto
\frac{1}{\operatorname{rank}(t)}\)</span>.</p>
<ul>
<li>The transitions are sorted so that <span class="math inline">\(\left|\delta_t\right|\)</span> is in the
descending order.</li>
<li><span class="math inline">\(\operatorname{rank}(t)\)</span> is the
rank of the <span class="math inline">\(t\)</span>-th transition.</li>
</ul>
<p>TD Error <span class="math inline">\(|\delta_t|\)</span>越大表示transition被抽到的概率也就越大</p>
<p>不同的transition有不同的抽样概率，这样会导致DQN的预测有偏差，需要相应的调整学习率，较大的抽样概率应该将learning-rate降低。</p>
<p>Scale the learning rate by <span class="math inline">\(\left(n
p_t\right)^{-\beta}\)</span> where <span class="math inline">\(\beta
\in(0,1)\)</span></p>
<p>需要记录每条transition的TD Error <span class="math inline">\(\delta_t\)</span>，如果一条transition刚收集到并不知道<span class="math inline">\(\delta_t\)</span>,则直接设置为最大值（最高的权重）
<span class="math display">\[
\begin{aligned}
&amp;\begin{array}{ccc}
\text { Transitions }
&amp;\text {Sampling Probabilities }
&amp;\text {Learning Rates } \\
\ldots &amp; \ldots &amp; \ldots \\\left(s_t, a_t, r_t, s_{t+1}\right),
\delta_t &amp; p_t \propto\left|\delta_t\right|+\epsilon &amp; \alpha
\cdot\left(n p_t\right)^{-\beta} \\
\left(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2}\right), \delta_{t+1} &amp;
p_{t+1} \propto\left|\delta_{t+1}\right|+\epsilon &amp; \alpha
\cdot\left(n p_{t+1}\right)^{-\beta} \\
\left(s_{t+2}, a_{t+2}, r_{t+2}, s_{t+3}\right), \delta_{t+2} &amp;
p_{t+2} \propto\left|\delta_{t+2}\right|+\epsilon &amp; \alpha
\cdot\left(n p_{t+2}\right)^{-\beta} \\
\ldots &amp; \ldots &amp; \ldots
\end{array}
\end{aligned}
\]</span> 越高的TD Error <span class="math inline">\(|\delta_t|\)</span>
，更高的概率（权重）被选择，更小的learning-rate</p>
<h2 id="高估问题">高估问题</h2>
<p>TD
Learning会导致DQN高估动作价值（action-value），由于以下两个原因</p>
<ul>
<li>计算TD Target时，<span class="math inline">\(y_t=r_t+\gamma \cdot
\max _a Q\left(s_{t+1}, a;w\right) .\)</span>用到了最大化操作，导致TD
Target大于真实值</li>
<li>Bootstrapping，计算TD Target时，利用了t+1时刻的网络估计</li>
</ul>
<p>两个原因导致高估问题越来越严重（恶性循环）</p>
<p>非均匀的高估会导致问题，例如某一时刻： <span class="math display">\[
\begin{aligned}
&amp; Q^{\star}\left(s, a^1\right)=200, Q^{\star}\left(s,
a^2\right)=100, and\ Q^{\star}\left(s, a^3\right)=230 \\
&amp; Q\left(s, a^1 ; \mathbf{w}\right)=280, Q\left(s, a^2 ;
\mathbf{w}\right)=300, Q\left(s, a^3 ; \mathbf{w}\right)=240 .
\end{aligned}
\]</span></p>
<p>Then <span class="math inline">\(a^2\)</span> (which is bad) will be
selected.</p>
<h3 id="target-network">Target Network</h3>
<p><strong>核心思想：</strong>利用一个Target Network计算TD</p>
<p>Target network: <span class="math inline">\(Q\left(s, a ;
\mathbf{w}^{-}\right)\)</span></p>
<ul>
<li>与 DQN, <span class="math inline">\(Q(s, a ; \mathbf{w})\)</span>
具有相同的网络结构.</li>
<li>但网络参数不同 <span class="math inline">\(\mathbf{w}^{-} \neq
\mathbf{w}\)</span>.</li>
</ul>
<p>利用 <span class="math inline">\(Q(s, a ; \mathbf{w})\)</span>
控制agent做运动，并收集transition: <span class="math display">\[
\left\{\left(s_t, a_t, r_t, s_{t+1}\right)\right\} .
\]</span></p>
<p>利用Target Network <span class="math inline">\(Q\left(s, a ;
\mathbf{w}^{-}\right)\)</span> 计算TD target: <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q\left(s_{t+1}, a ; \mathbf{w}^{-}\right) .
\]</span></p>
<p>Target Network参数更新（每隔一段时间）：</p>
<ul>
<li>Option 1: <span class="math inline">\(\mathbf{w}^{-} \leftarrow
\mathbf{w}\)</span>.（直接赋值）</li>
<li>Option 2: <span class="math inline">\(\mathbf{w}^{-} \leftarrow \tau
\cdot \mathbf{w}+(1-\tau) \cdot
\mathbf{w}^{-}\)</span>.（做加权平均）</li>
</ul>
<h3 id="double-dqn">Double DQN</h3>
<p><strong>核心思想：</strong>利用Double
DQN缓解因为maximization导致的高估</p>
<p>原始方法计算TD Target <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q\left(s_{t+1}, a ; \mathbf{w}\right) .
\]</span> 计算TD Target的过程可以分解为两步</p>
<ol type="1">
<li><p>选择action: <span class="math inline">\(a^{\star}=\underset{a}{\operatorname{argmax}}
Q\left(s_{t+1}, a ; \mathbf{w}\right) .\)</span></p></li>
<li><p>计算TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q\left(s_{t+1}, a^{\star} ; \mathbf{w}\right) .\)</span></p></li>
</ol>
<p>原始方法中两步均使用DQN，Target
Network方法中两步均使用target-net。</p>
<p>Double DQN 第一步使用DQN选择aciton，第二步使用target-net计算TD
Target</p>
<p>Selection using DQN: <span class="math display">\[
a^{\star}=\underset{a}{\operatorname{argmax}} Q\left(s_{t+1}, a ;
\mathbf{w}\right) .
\]</span></p>
<p>Evaluation using target network: <span class="math display">\[
y_t=r_t+\gamma \cdot Q\left(s_{t+1}, a^{\star} ; \mathbf{w}^{-}\right) .
\]</span> 通过Double
DQN的方法大幅提高性能，减少maximization带来的高估问题（但没有彻底根除）</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Selection</th>
<th style="text-align: center;">Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Naive Update</td>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">DQN</td>
</tr>
<tr>
<td style="text-align: center;">Using Target</td>
<td style="text-align: center;">Target Network</td>
<td style="text-align: center;">Target Network</td>
</tr>
<tr>
<td style="text-align: center;">Double DQN</td>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">Target Network</td>
</tr>
</tbody>
</table>
<h2 id="dueling-network">Dueling Network</h2>
<h3 id="advantage-function">Advantage Function</h3>
<p>Discounted return: <span class="math inline">\(U_t=R_t+\gamma \cdot
R_{t+1}+\gamma^2 \cdot R_{t+2}+\gamma^3 \cdot
R_{t+3}+\cdots\)</span></p>
<p>Action-value function: <span class="math inline">\(Q_\pi\left(s_t,
a_t\right)=\mathbb{E}\left[U_t \mid S_t=s_t, A_t=a_t\right] \text {.
}\)</span></p>
<p>State-value function: $ V_(s_t)=_A$</p>
<p>Optimal action-value function: <span class="math inline">\(Q^{\star}(s, a)=\max _\pi Q_\pi(s, a) \text {.
}\)</span></p>
<p>Optimal state-value function: <span class="math inline">\(V^{\star}(s)=\max _\pi V_\pi(s) \text {.
}\)</span></p>
<p><strong>Definition: Optimal advantage function.</strong>（优势函数）
<span class="math display">\[
A^{\star}(s, a)=Q^{\star}(s, a)-V^{\star}(s) \text {. }
\]</span> <span class="math inline">\(V^{\star}(s)\)</span>评价状态s的好坏，<span class="math inline">\(Q^{\star}(s,
a)\)</span>评价在状态s的情况下做动作a的好坏</p>
<p><span class="math inline">\(A^{\star}(s,
a)\)</span>表示动作a相对于baseline的优势，a越好，优势越大</p>
<p>经过数学推导得到以下两个等式</p>
<ul>
<li>Equation 1: <span class="math inline">\(Q^{\star}(s,
a)=V^{\star}(s)+A^{\star}(s, a)\)</span>.</li>
<li>Equation 2: <span class="math inline">\(Q^{\star}(s,
a)=V^{\star}(s)+A^{\star}(s, a)-\max _a A^{\star}(s, a)\)</span>.</li>
</ul>
<p><span class="math inline">\(\max _a A^{\star}(s, a)\)</span>
恒等于0，在训练中能保持神经网络的稳定，避免<span class="math inline">\(V^{\star}(s;w),A^{\star}(s,
a;w)\)</span>两个神经网络的输出随意上下波动</p>
<p>对于Equation 2 <span class="math display">\[
Q^{\star}(s, a)=V^{\star}(s)+A^{\star}(s, a)-\max _a A^{\star}(s, a)
\]</span></p>
<ul>
<li>利用神经网络<span class="math inline">\(V\left(s ;
\mathbf{w}^V\right)\)</span>近似<span class="math inline">\(V^{\star}(s)\)</span><br>
</li>
<li>利用神经网络<span class="math inline">\(A\left(s, a ;
\mathbf{w}^A\right)\)</span>近似<span class="math inline">\(A^{\star}(s,
a)\)</span></li>
</ul>
<p>因此 <span class="math inline">\(Q^{\star}(s, a)\)</span>
可以利用dueling network表示为： <span class="math display">\[
Q(s, a ; \mathbf{w}^A, \mathbf{w}^V)=V(s ; \mathbf{w}^V)+A(s, a ;
\mathbf{w}^A)-\max _a A(s, a ; \mathbf{w}^A) ..
\]</span> <img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205115127738.png" alt="image-20231205115127738"></p>
<p>红色对应<span class="math inline">\(A(s, a ; \mathbf{w}^A)\)</span>，
蓝色对应<span class="math inline">\(V(s ;
\mathbf{w}^V)\)</span>，将蓝色实数与上面向量每个元素相加，再减去红色向量中的最大元素得到紫色向量（最终输出）</p>
<p>Dueling Network的输入输出、功能、训练方式(TD
Learning)、优化方法(Experience Replay, Target Network,double
DQN)与DQN完全一样</p>
<p>Dueling
Network改变了网络结构，提高了训练效果，此外在实际训练中max可以取mean，有更好的效果
<span class="math display">\[
Q(s, a ; \mathbf{w})=V(s ; \mathbf{w}^V)+A(s, a ;
\mathbf{w}^A)-\operatorname{mean}_a A(s, a ; \mathbf{w}^A) .
\]</span></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://xclovehsy.github.io">XuCong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://xclovehsy.github.io/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/">https://xclovehsy.github.io/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xclovehsy.github.io" target="_blank">XcloveHsy's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0/">价值学习</a></div><div class="post_share"><div class="social-share" data-image="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/" title="【RL】Policy-Based Learning"><img class="cover" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/8135273c114f9f1c57671f081af754a.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【RL】Policy-Based Learning</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/" title="【RL】基本概念"><img class="cover" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205150300996.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【RL】基本概念</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">XuCong</div><div class="author-info__description">Nothing is impossible</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xclovehsy" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xclovehsy@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#rlvalue-based-learning"><span class="toc-number">1.</span> <span class="toc-text">【RL】Value-Based Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.</span> <span class="toc-text">基于价值学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#discounted-return"><span class="toc-number">1.1.1.</span> <span class="toc-text">Discounted Return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#td-target"><span class="toc-number">1.1.2.</span> <span class="toc-text">TD Target</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#td-error"><span class="toc-number">1.1.3.</span> <span class="toc-text">TD Error</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sarsa"><span class="toc-number">1.2.</span> <span class="toc-text">Sarsa</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">表格形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.2.2.</span> <span class="toc-text">神经网络形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sarsalambda"><span class="toc-number">1.2.3.</span> <span class="toc-text">Sarsa(\(\lambda\))</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q-learning"><span class="toc-number">1.3.</span> <span class="toc-text">Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87"><span class="toc-number">1.3.1.</span> <span class="toc-text">学习目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.3.2.</span> <span class="toc-text">数学推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F-1"><span class="toc-number">1.3.3.</span> <span class="toc-text">表格形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dqn%E7%BD%91%E7%BB%9C%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.3.4.</span> <span class="toc-text">DQN（网络形式）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-step-td-target"><span class="toc-number">1.4.</span> <span class="toc-text">Multi-Step TD Target</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sarsa-vs-q-learning"><span class="toc-number">1.4.1.</span> <span class="toc-text">Sarsa vs Q-learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC-1"><span class="toc-number">1.4.2.</span> <span class="toc-text">数学推导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">1.5.</span> <span class="toc-text">经验回放</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%80%9D%E7%BB%B4%E5%86%B3%E7%AD%96"><span class="toc-number">1.5.1.</span> <span class="toc-text">神经网络与思维决策</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%85%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">1.5.2.</span> <span class="toc-text">优先经验回放</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E4%BC%B0%E9%97%AE%E9%A2%98"><span class="toc-number">1.6.</span> <span class="toc-text">高估问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#target-network"><span class="toc-number">1.6.1.</span> <span class="toc-text">Target Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#double-dqn"><span class="toc-number">1.6.2.</span> <span class="toc-text">Double DQN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dueling-network"><span class="toc-number">1.7.</span> <span class="toc-text">Dueling Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#advantage-function"><span class="toc-number">1.7.1.</span> <span class="toc-text">Advantage Function</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/be1439d8c7544c8d966f356fd11f62c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Decoder"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder">【Transformer】Decoder</a><time datetime="2024-08-05T01:28:22.000Z" title="发表于 2024-08-05 09:28:22">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/17b264d340f81d56756a5e4ab518a2a.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Encoder"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder">【Transformer】Encoder</a><time datetime="2024-08-05T01:28:07.000Z" title="发表于 2024-08-05 09:28:07">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805144748910.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Residual and LayerNorm"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm">【Transformer】Residual and LayerNorm</a><time datetime="2024-08-05T01:21:48.000Z" title="发表于 2024-08-05 09:21:48">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805140934105.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Position-wise FFN"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN">【Transformer】Position-wise FFN</a><time datetime="2024-08-05T01:12:38.000Z" title="发表于 2024-08-05 09:12:38">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805113648793.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Attention"/></a><div class="content"><a class="title" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention">【Transformer】Attention</a><time datetime="2024-08-02T11:30:57.000Z" title="发表于 2024-08-02 19:30:57">2024-08-02</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By XuCong</div><div class="footer_custom_text">Hi, welcome to xclovehsy's blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>