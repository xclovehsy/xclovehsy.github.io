<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Leetcode-n皇后</title>
    <url>/2024/07/31/Leetcode-n%E7%9A%87%E5%90%8E/</url>
    <content><![CDATA[<h1 id="leetcode-n皇后">Leetcode-n皇后</h1>
<h2 id="题目描述">题目描述</h2>
<p>按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。</p>
<p><strong>n 皇后问题</strong> 研究的是如何将 <code>n</code>
个皇后放置在 <code>n×n</code>
的棋盘上，并且使皇后彼此之间不能相互攻击。</p>
<p>给你一个整数 <code>n</code> ，返回所有不同的 <strong>n
皇后问题</strong> 的解决方案。</p>
<p>每一种解法包含一个不同的 <strong>n 皇后问题</strong>
的棋子放置方案，该方案中 <code>'Q'</code> 和 <code>'.'</code>
分别代表了皇后和空位。</p>
<h2 id="示例">示例</h2>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731172857954.png" alt="image-20240731172857954">
<figcaption aria-hidden="true">image-20240731172857954</figcaption>
</figure>
<p>输入：n = 4</p>
<p>输出：[[".Q..","...Q","Q...","..Q."],["..Q.","Q...","...Q",".Q.."]]</p>
<p>解释：如上图所示，4 皇后问题存在两个不同的解法。</p>
<p>1 &lt;= n &lt;= 9</p>
<h2 id="思路">思路</h2>
<p>经典的回溯问题。</p>
<p>定义数组Q存储已经放皇后的位置<span class="math inline">\((i,
Q[i])\)</span></p>
<p>根据row进行回溯，判断皇后放在<span class="math inline">\([1,n]\)</span> col是否有效，执行路径如下：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731173548183.png" alt="image-20240731173548183">
<figcaption aria-hidden="true">image-20240731173548183</figcaption>
</figure>
<h2 id="ac代码">AC代码</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; Q;</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; ans;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;Q.<span class="built_in">size</span>(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(Q[i] == y || <span class="built_in">abs</span>(i - x) == <span class="built_in">abs</span>(Q[i] - y)) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> row, <span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(row &gt;= n)&#123;</span><br><span class="line">            <span class="function">vector&lt;string&gt; <span class="title">temp</span><span class="params">(n, string(n, <span class="string">&#x27;.&#x27;</span>))</span></span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;Q.<span class="built_in">size</span>(); i++) temp[i][Q[i]] = <span class="string">&#x27;Q&#x27;</span>;</span><br><span class="line">            ans.<span class="built_in">push_back</span>(temp);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">check</span>(row, i))&#123;</span><br><span class="line">                Q.<span class="built_in">push_back</span>(i);</span><br><span class="line">                <span class="built_in">dfs</span>(row+<span class="number">1</span>, n);</span><br><span class="line">                Q.<span class="built_in">pop_back</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; <span class="built_in">solveNQueens</span>(<span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="built_in">dfs</span>(<span class="number">0</span>, n);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="复杂度分析">复杂度分析</h2>
<ul>
<li>时间复杂度：<span class="math inline">\(O(n!)\)</span>。搜索树中至多有 <span class="math inline">\(O(n!)\)</span> 个叶子。</li>
<li>空间复杂度：O(n)。返回值的空间不计入。</li>
</ul>
]]></content>
      <categories>
        <category>算法</category>
        <category>回溯</category>
      </categories>
      <tags>
        <tag>回溯</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>【RL】基本概念</title>
    <url>/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="rl基本概念">【RL】基本概念</h1>
<h2 id="概率统计">概率统计</h2>
<p><strong>随机变量：</strong>一个未知的量，它的值取决于一个随机事件的结果。</p>
<p><strong>概率密度函数：</strong>随机变量在某个确定的取值点附近的可能性（连续概率分布
&amp; 离散概率分布）</p>
<p><strong>期望：</strong>连续分布（p(x)和f(x)的乘积做定积分） &amp;
离散分布（对p(x)和f(x)的乘积进行连加）</p>
<p>随机抽样： from numpy.random import choice / sample =
choice(['a','b','c'], size=100, p=[0.2,0,5,0,3])</p>
<h2 id="简介">简介</h2>
<h3 id="基本概念">基本概念</h3>
<ul>
<li><p>监督学习：
用于处理<strong>分类与回归问题</strong>，这类问题的任务是对输入X与输入Y之间的映射关系进行拟合，从而对不同X对应的Y进行预测</p></li>
<li><p>强化学习：用于求解<strong>序列决策问题</strong>，这一类问题的任务是求解一个策略用于指导机器在不同状态下选择最佳动作。</p></li>
</ul>
<p>核心思想：强化学习的做法是让机器自行对动作进行探索，然后根据环境的反馈，不断地对策略进行调整，最终目标是使得环境收益尽可能地大。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205150300996.png" alt="image-20231205150300996">
<figcaption aria-hidden="true">image-20231205150300996</figcaption>
</figure>
<h3 id="state-action">State &amp; Action</h3>
<blockquote>
<p>马尔可夫决策过程</p>
</blockquote>
<p>马尔可夫决策：<span class="math inline">\(\left(S_t, A_t, R_t,
P_t\right)\)</span></p>
<ul>
<li><p><span class="math inline">\(S_t:\)</span> 环境状态</p></li>
<li><p><span class="math inline">\(A_t\)</span> : 智能体的动作</p></li>
<li><p><span class="math inline">\(Policy\ \pi:\)</span>
状态到动作之间的映射</p></li>
<li><p><span class="math inline">\(\boldsymbol{R}_t\)</span> : 奖励函数,
取决于当前状态与动作</p></li>
<li><p><span class="math inline">\(P_t=p\left(S_{t+1}=s^{\prime} \mid
S_t=s, A_t=a\right)\)</span> : 状态转移概率</p></li>
<li><p>学习与决策者被称为<strong>智能体</strong> （agent）</p></li>
<li><p>与智能体交互的部分则称为<strong>环境</strong> （state）</p></li>
</ul>
<p><strong>环境下一时刻的状态与奖励均只取决于当前时刻的状态与智能体的动作</strong>（action）</p>
<h3 id="policy-pi">Policy <span class="math inline">\(\pi\)</span></h3>
<p>本质是一个概率密度函数，根据观测到的状态来进行决策，来控制agent运动。
<span class="math display">\[
\pi(a \mid s)=P(A=a \mid S=s)
\]</span></p>
<h3 id="state-transition">State Transition</h3>
<p>状态转移可以使用p函数表示，条件概率密度函数,意思是如果观测到当前的状态s以及动作a，p函数输出s’的概率</p>
<p><span class="math display">\[
p\left(s^{\prime} \mid s, a\right)=P\left(S^{\prime}=s^{\prime} \mid
S=s, A=a\right)
\]</span></p>
<h3 id="rewardr_t-returnu_t">Reward（<span class="math inline">\(R_t\)</span>） &amp; Return（<span class="math inline">\(U_t\)</span>）</h3>
<blockquote>
<p>Reward（奖励）</p>
</blockquote>
<p><span class="math display">\[
R_t=R\left(s_t, a_t\right) \quad
\]</span></p>
<p><strong>短期利益，</strong>在某状态下采取某动作的好坏，在当前环境中获得的<strong>短期利益</strong>，只考虑动作对当前状态的影响</p>
<blockquote>
<p>Return（收益）Discounted Return</p>
</blockquote>
<p><span class="math display">\[
\quad U_t = R_{t+1}+\gamma R_{t+2}+\gamma^2
R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \quad
\]</span></p>
<p><strong>长远利益，</strong>t时刻的return叫做 <span class="math inline">\(U_t\)</span> or <span class="math inline">\(G_t\)</span>。
指将t时刻到未来所有折扣奖励之和（<strong><span class="math inline">\(\gamma\)</span>
折扣因子</strong>，超参数，并且越往后，衰减越大），<strong>当前的奖励比未来奖励影响大</strong>。</p>
<p>某一时刻<span class="math inline">\(t\)</span>，对于未来<span class="math inline">\(R_i(i&gt;t)\)</span>，依赖于所有<span class="math inline">\(S_i\)</span>以及<span class="math inline">\(A_i\)</span>，由于Action、State的转移是随机的，因此
<span class="math inline">\(U_t\)</span> 是一个随机变量</p>
<h3 id="action-value-functionq_pi">Action-Value Function（<span class="math inline">\(Q_\pi\)</span>）</h3>
<blockquote>
<p>动作-状态价值函数 <span class="math inline">\(Q_\pi(s_t,a_t)\)</span></p>
</blockquote>
<p><strong>表示在给定状态-动作对的条件下未来依据某策略做决策所能获得的平均收益</strong></p>
<p>Discounted Return: <span class="math inline">\(U_t=R_t+\gamma
R_{t+1}+\gamma^2 R_{t+2}+\gamma^3 R_{t+3}+\cdots\)</span></p>
<p>定义<span class="math inline">\(Policy\ \pi\)</span>的 Action-value
function： <span class="math display">\[
Q_\pi\left(s_t, a_t\right)={\mathbb{E}}\left[U_t \mid S_t=s_t,
A_t=a_t\right] .
\]</span></p>
<p>由于 <span class="math inline">\(U_t\)</span> 依赖于 <span class="math inline">\(A_t, A_{t+1}, A_{t+2}, \cdots\)</span> 以及 <span class="math inline">\(S_t, S_{t+1}, S_{t+2},
\cdots\)</span>，而Action和State的转移是随机的。</p>
<p>因此<span class="math inline">\(U_t\)</span>是一个随机变量，它依赖所有的动作和状态，在t时刻并不知道<span class="math inline">\(U_t\)</span>是什么，因此求期望<span class="math inline">\(\mathbb{E}\)</span>积掉随机性。</p>
<p>利用期望<span class="math inline">\(\mathbb{E}\)</span>消除<span class="math inline">\(A_i,S_i(i&gt;t)\)</span>，留下<span class="math inline">\(A_t,S_t\)</span></p>
<blockquote>
<p>最优动作价值函数（Optimal Action-Value Function）<span class="math inline">\(Q^\star(s_t,a_t)\)</span></p>
</blockquote>
<p><strong>定义最佳策略下的动作价值函数为最优价值函数</strong> <span class="math display">\[
Q^*\left(s_t, a_t\right)=\max _\pi Q_\pi\left(s_t, a_t\right) .
\]</span> <strong>核心：</strong>求解最优策略也就等价于求解最优Q值。</p>
<p><strong>学习目标</strong>：学习到一个最优策略 <span class="math inline">\(\pi^*\)</span>,
使智能体在任意状态下的价值最大最优动作-状态价值函数（最优 <span class="math inline">\(\mathrm{Q}\)</span> 值）</p>
<h3 id="state-value-function-v_pi">State-Value Function (<span class="math inline">\(V_\pi\)</span>)</h3>
<blockquote>
<p>状态价值函数 <span class="math inline">\(V_\pi(s_t)\)</span></p>
</blockquote>
<p><span class="math inline">\(V_\pi\)</span>是动作价值函数<span class="math inline">\(Q_\pi\)</span>的期望。</p>
<p><span class="math inline">\(Q_\pi(s_t,a_t)\)</span>和<span class="math inline">\(Policy\ \pi,\ s_t,\ a_t\)</span>有关，将动作<span class="math inline">\(a_t\)</span>作为随机变量，然后期望<span class="math inline">\(\mathbb{E}\)</span>把<span class="math inline">\(A_t\)</span>消掉，求期望得到的<span class="math inline">\(V_\pi\)</span>只与<span class="math inline">\(Policy\ \pi,\ s_t\)</span>有关。 <span class="math display">\[
V_\pi\left(s_t\right)=\mathrm{E}_A\left[Q_\pi\left(s_t, A\right)\right]
\]</span> <strong>作用</strong>：</p>
<ol type="1">
<li><p>对于给定的 <span class="math inline">\(\pi\)</span> ，<span class="math inline">\(V_\pi\)</span> 评价当前的<span class="math inline">\(s_t\)</span>好不好</p></li>
<li><p>评估 <span class="math inline">\(Policy\ \pi\)</span>
的好坏</p></li>
</ol>
<h3 id="随机性">随机性</h3>
<blockquote>
<p>动作随机</p>
</blockquote>
<p>动作是根据<span class="math inline">\(Policy\
\pi\)</span>随机抽样得到的，利用<span class="math inline">\(Policy\
\pi\)</span>来控制Agent.</p>
<p>给定当前状态 S，Agent的动作A是按照<span class="math inline">\(Policy\
\pi\)</span>输出的概率来随机抽样 <span class="math display">\[
\begin{aligned}
&amp; \mathbb{P}[A=a \mid S=s]=\pi(a \mid s)
\end{aligned}
\]</span></p>
<blockquote>
<p>状态转移随机</p>
</blockquote>
<p>Environment用状态转移函数p算出概率，然后用随机抽样得到下一个状态S’
<span class="math display">\[
\mathbb{P}\left[S^{\prime}=s^{\prime} \mid S=s,
A=a\right]=p\left(s^{\prime} \mid s, a\right)
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>概率统计</tag>
      </tags>
  </entry>
  <entry>
    <title>【RL】Policy-Based Learning</title>
    <url>/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="rlpolicy-based-learning">【RL】Policy-Based Learning</h1>
<h2 id="基于策略学习">基于策略学习</h2>
<p><strong>思想</strong>：直接对最优策略进行估计。</p>
<ol type="1">
<li><p>随机性策略：对状态到最优动作概率分布之间的映射进行估计，然后从该概率分布中进行采样得到输出动作。</p></li>
<li><p>确定性策略： 直接对状态到最优动作之间的映射进行估计。</p></li>
</ol>
<p><strong>算法特点</strong>：神经网络的输出即为最优动作，动作空间既可以是离散的也可以是连续的。</p>
<p>直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的,
比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这,
它可吃不消.</p>
<h2 id="核心内容">核心内容</h2>
<p>利用策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>去近似价值函数<span class="math inline">\(\pi(a|s)\)</span></p>
<p>Agent根据策略网络的输出，概率抽样选择Action <span class="math inline">\(a_t \sim \pi\left(\cdot \mid
s_t\right)\)</span></p>
<p>通过策略梯度（Policy Gradient）寻览策略网络</p>
<p>策略梯度算法通过最大化<span class="math inline">\(\mathbb{E}_s[V(s;\theta)]\)</span>，训练网络参数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205120520822.png" alt="image-20231205120520822">
<figcaption aria-hidden="true">image-20231205120520822</figcaption>
</figure>
<h3 id="数学推导">数学推导</h3>
<p>Discounted return: <span class="math inline">\(U_t=R_t+\gamma \cdot
R_{t+1}+\gamma^2 \cdot R_{t+2}+\gamma^3 \cdot
R_{t+3}+\cdots\)</span></p>
<p>Action-value function: <span class="math inline">\(Q_\pi\left(s_t,
a_t\right)=\mathbb{E}\left[U_t \mid S_t=s_t, A_t=a_t\right] \text {.
}\)</span></p>
<p>State-value function: $ V_(s_t)=_A=<em>a(a|s_t)Q</em>(s_t,a)$</p>
<p>利用策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span>近似policy <span class="math inline">\(\pi\)</span>，因此状态价值函数同样可以利用<span class="math inline">\(\theta\)</span>近似得到<span class="math inline">\(V(s;\theta)\)</span> <span class="math display">\[
V(s ; \boldsymbol{\theta})=\sum_a \pi(a \mid s ; \boldsymbol{\theta})
\cdot Q_\pi(s, a) \text {. }
\]</span></p>
<p><span class="math inline">\(V(s ;
\boldsymbol{\theta})\)</span>表示对当前状态State的获胜概率，再对状态State求期望得到<span class="math inline">\(J(\theta)\)</span> <span class="math display">\[
J(\boldsymbol{\theta})=\mathbb{E}_S[V(S ; \boldsymbol{\theta})]
\]</span> 策略学习目标就是通过更新网络参数<span class="math inline">\(\theta\)</span> 使得<span class="math inline">\(J(\theta)\)</span>的值越大越好</p>
<p>利用Policy Gradient更新网络梯度 <span class="math display">\[
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta\cdot
\frac{\partial v(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
\]</span> <span class="math inline">\(\beta\)</span>是学习率，<span class="math inline">\(\frac{\partial v(s ;
\boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}\)</span>是策略梯度，利用梯度上升来更新<span class="math inline">\(\theta\)</span></p>
<h2 id="policy-gradient">Policy Gradient</h2>
<p>State-value function: $ V_(s_t)=_A=<em>a(a|s_t)Q</em>(s_t,a)$</p>
<p>计算策略梯度： <span class="math display">\[
\begin{aligned}
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
&amp; =\frac{\partial \sum_a \pi(a \mid s ; \boldsymbol{\theta}) \cdot
Q_\pi(s, a)}{\partial \boldsymbol{\theta}} \\
&amp; =\sum_a \frac{\partial \pi(a \mid s ; \boldsymbol{\theta}) \cdot
Q_\pi(s, a)}{\partial \boldsymbol{\theta}} \\
&amp; =\sum_a \frac{\partial \pi(a \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a)\\
&amp; =\sum_a \pi(a \mid s ; \boldsymbol{\theta}) \cdot \frac{\partial
\log \pi(a \mid s ; \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
\cdot Q_\pi(s, a) \\
&amp; =\mathbb{E}_A\left[\frac{\partial \log \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s,
A)\right] .
\end{aligned}
\]</span></p>
<p>以上假设<span class="math inline">\(Q_\pi\)</span>独立于<span class="math inline">\(\theta\)</span>以简化推导，但实际中并非独立，因此并不严谨</p>
<p>通过推导，得到策略梯度的两种表示形式 <span class="math display">\[
\begin{aligned}
&amp; Form 1: \frac{\partial V(s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}=\sum_a \frac{\partial \pi(a \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s, a)$.
\\
&amp; Form 2: \frac{\partial V(s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}=\mathbb{E}_{A \sim \pi(\cdot \mid s ;
\boldsymbol{\theta})}\left[\frac{\partial \log \pi(A \mid s,
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s,
A)\right]$.
\end{aligned}
\]</span></p>
<h3 id="离散的动作">离散的动作</h3>
<p>如果Action是离散的，可以使用Form1: <span class="math inline">\(\frac{\partial V(s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\sum_a
\frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}} \cdot Q_\pi(s, a)\)</span>计算策略梯度</p>
<ol type="1">
<li><p>Calculate <span class="math inline">\(\mathbf{f}({a},
\boldsymbol{\theta})=\frac{\partial \pi(a \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s,
a)\)</span>, for every action <span class="math inline">\(a \in
\mathcal{A}\)</span>.</p></li>
<li><p>Policy gradient: <span class="math inline">\(\frac{\partial V(s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbf{f}(a_1,
\boldsymbol{\theta})+\mathbf{f}(a_2,
\boldsymbol{\theta})+\mathbf{f}(a_3,
\boldsymbol{\theta})\)</span>.</p></li>
</ol>
<h3 id="连续的动作">连续的动作</h3>
<p>动作连续则使用第二种方式计算策略梯度，由于积分非常难求，因此采用蒙特卡洛近似计算策略梯度</p>
<p>蒙特卡洛近似，需要求期望 <span class="math inline">\(\mathbb{E}_A[\mathbf{g}(A,
\boldsymbol{\theta})]=\frac{\partial V(s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\)</span>，从Action
space中根据<span class="math inline">\(\pi(.|s;\theta)\)</span>的概率分布随机抽取一个（或很多个）样本<span class="math inline">\(\hat{a}\)</span>，计算<span class="math inline">\(g(\hat{a},\theta)\)</span>（平均值）将其近似为策略梯度，无偏估计</p>
<p>假设动作是连续的，Action space <span class="math inline">\(\mathcal{A}=[0,1], \ldots\)</span> <span class="math display">\[
Form2:\quad \frac{\partial V(s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}=\mathbb{E}_{A \sim \pi(\cdot \mid s ;
\boldsymbol{\theta})}\left[\frac{\partial \log \pi(A \mid s,
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s,
A)\right]
\]</span></p>
<ol type="1">
<li>Randomly sample an action <span class="math inline">\(\hat{a}\)</span> according to the PDF <span class="math inline">\(\pi(\cdot \mid s ;
\boldsymbol{\theta})\)</span>.</li>
<li>Calculate <span class="math inline">\(\mathbf{g}(\hat{a},
\boldsymbol{\theta})=\frac{\partial \log \pi(\hat{a} \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot Q_\pi(s,
\hat{a})\)</span>.</li>
<li>Use <span class="math inline">\(\mathbf{g}(\hat{a},
\boldsymbol{\theta})\)</span> as an approximation to the policy gradient
<span class="math inline">\(\frac{\partial V(s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\)</span>.</li>
</ol>
<h2 id="策略学习算法">策略学习算法</h2>
<ol type="1">
<li>Observe the state <span class="math inline">\(s_t\)</span>.</li>
<li>Randomly sample action <span class="math inline">\(a_t\)</span>
according to <span class="math inline">\(\pi\left(\cdot \mid s_t ;
\boldsymbol{\theta}_t\right)\)</span>.</li>
<li>Compute <span class="math inline">\(q_t \approx Q_\pi\left(s_t,
a_t\right)\)</span> (some estimate).</li>
<li>Differentiate policy network: <span class="math inline">\(\mathbf{d}_{\theta, t}=\left.\frac{\partial \log
\pi\left(a_t \mid s_t, \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_t}\)</span>.</li>
<li>(Approximate) policy gradient: <span class="math inline">\(\mathbf{g}\left(a_t,
\boldsymbol{\theta}_t\right)=q_t \cdot \mathbf{d}_{\theta,
t}\)</span>.</li>
<li>Update policy network: <span class="math inline">\(\boldsymbol{\theta}_{t+1}={\boldsymbol{\theta}_t+\beta}
\cdot \mathbf{g}\left({a_t},
{\left.\boldsymbol{\theta}_t\right)}\right.\)</span>.</li>
</ol>
<p>但是第3步<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>的值无法计算，因此可以采用REINFORCE、actor-critic的方法计算<span class="math inline">\(Q_\pi(s_t,a_t)\)</span></p>
<h2 id="baseline">Baseline</h2>
<p>设定一个独立于A的baseline,b <span class="math display">\[
\begin{aligned}
\mathbb{E}_{A \sim \pi}\left[b \cdot \frac{\partial \ln \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\right]&amp; =b \cdot
\mathbb{E}_{A \sim \pi}\left[\frac{\partial \ln \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\right] \\
&amp; =b \cdot \sum_a \pi(a \mid s ; \boldsymbol{\theta})
\cdot\left[\frac{1}{\pi(a \mid s ; \boldsymbol{\theta})} \cdot
\frac{\partial \pi(a \mid s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}\right] \\
&amp; =b \cdot \sum_a \frac{\partial \pi(a \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \\
&amp; =b \cdot \frac{\partial \sum_a \pi(a \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \\
&amp; =b \frac{\partial 1}{\partial \theta}=0 . \\
\end{aligned}
\]</span> 如果b独立于A，可以得到<span class="math inline">\(\mathbb{E}_{A \sim \pi}\left[b \cdot
\frac{\partial \ln \pi(A \mid s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}\right]=0\)</span></p>
<p>根据前面Policy Gradient公式可以得到，后面一项为0 <span class="math display">\[
\begin{aligned}
\frac{\partial V_\pi(s)}{\partial \boldsymbol{\theta}}
&amp;=\mathbb{E}_{A \sim \pi}\left[\frac{\partial \ln \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} Q_\pi(s,
A)\right]-\mathbb{E}_{A \sim \pi}\left[\frac{\partial \ln \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\right]. \\
&amp; =\mathbb{E}_{A \sim \pi}\left[\frac{\partial \ln \pi(A \mid s ;
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\left(Q_\pi(s,
A)-b\right)\right] .
\end{aligned}
\]</span> 无论b的值等于多少，只要独立于A，得到的期望与之前一样</p>
<p>在蒙特卡洛近似中，依照策略网络的输出概率抽样得到<span class="math inline">\(a_t\)</span>，利用<span class="math inline">\(a_t\)</span>计算<span class="math inline">\(g(a_t)\)</span>来近似期望 <span class="math display">\[
\mathbf{g}\left(a_t\right)=\frac{\partial \ln \pi\left(a_t \mid s_t ;
\boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}
\cdot\left(Q_\pi\left(s_t, a_t\right)-b\right) .
\]</span> 显然b（独立于<span class="math inline">\(A_t\)</span>）不会影响期望<span class="math inline">\((\mathbb{E}_{A_t \sim
\pi}\left[\mathbf{g}\left(A_t\right)\right])\)</span>，但可以改变<span class="math inline">\(g(a_t)\)</span>的大小,</p>
<p>如果b的值接近<span class="math inline">\(Q_\pi\)</span>，则会是蒙特卡洛近似<span class="math inline">\(g(a_t)\)</span>的方差降低，算法收敛更快</p>
<p><strong>可以设置<span class="math inline">\(b=V_\pi\)</span>，由于<span class="math inline">\(V_\pi(s_t)\)</span>独立于<span class="math inline">\(A_t\)</span>，并且<span class="math inline">\(V_\pi(s_t)\)</span>是<span class="math inline">\(Q_\pi(s_t,A_t)\)</span>的期望，两者比较接近</strong>
<span class="math display">\[
\frac{\partial V_\pi\left(s_t\right)}{\partial
\boldsymbol{\theta}}=\mathbb{E}_{A_t \sim \pi}\left[\frac{\partial \ln
\pi\left(A_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} \cdot\left(Q_\pi\left(s_t,
A_t\right)-V_\pi\left(s_t\right)\right)\right] .
\]</span></p>
<h2 id="reinforce">REINFORCE</h2>
<p>基于 <strong>整条回合数据</strong> 的更新</p>
<p>利用策略网络<span class="math inline">\(\pi\)</span>控制agent运动，从一开始玩到游戏结束，记录每个state、action、reward。
<span class="math display">\[
s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_T, a_T, r_T .
\]</span></p>
<p>观测到所有的<span class="math inline">\(r_t\)</span>，就可以计算所有Discounted Return
<span class="math inline">\(u_t(t=1,2,3,..,T)\)</span> <span class="math display">\[
u_t=\sum_{k=t}^T \gamma^{k-t} r_k
\]</span> 由于<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>是<span class="math inline">\(U_t\)</span>的期望，因此可以使用<span class="math inline">\(u_t\)</span>近似<span class="math inline">\(q_t\)</span></p>
<h3 id="reinforce-with-baseline">REINFORCE With Baseline</h3>
<p>结合前边的Baseline方法，近似计算策略梯度： <span class="math display">\[
\frac{\partial V_\pi\left(s_t\right)}{\partial \boldsymbol{\theta}}
\approx \mathbf{g}\left(a_t\right) \approx \frac{\partial \ln
\pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} \cdot (u_t-v(s_t;w))
\]</span> 使用了两次蒙特卡洛近似，一次神经网络近似</p>
<ol type="1">
<li><p>Approximate expectation using one sample, <span class="math inline">\(a_t\)</span>. (Monte Carlo.)</p></li>
<li><p>Approximate <span class="math inline">\(Q_\pi\left(s_t,
a_t\right)\)</span> by <span class="math inline">\(u_t\)</span>.
(Another Monte Carlo.)</p></li>
<li><p>Approximate <span class="math inline">\(V_\pi(s)\)</span> by the
value network, <span class="math inline">\(v(s ;
\mathbf{w})\)</span>.</p></li>
</ol>
<p>建立神经网络，由于两个网络都使用卷积层（马里奥demo），因此可以共享网络参数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205133639628.png" alt="image-20231205133639628">
<figcaption aria-hidden="true">image-20231205133639628</figcaption>
</figure>
<h3 id="结合策略学习">结合策略学习</h3>
<p><span class="math display">\[
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \frac{\partial
\ln \pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} \cdot\left(u_t-v\left(s_t ;
\mathbf{w}\right)\right) .
\]</span> 令<span class="math inline">\(\delta=u_t-v(s_t ;
\mathbf{w})\)</span></p>
<p>可以写成 <span class="math display">\[
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\beta \cdot \delta_t
\cdot \frac{\partial \ln \pi\left(a_t \mid s_t ;
\boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}
\]</span> <strong>利用回归训练价值网络<span class="math inline">\(v(s;w)\)</span> （需要尽可能接近<span class="math inline">\(u_t\)</span>）</strong></p>
<p><span class="math inline">\(v(s;w)\)</span>是对<span class="math inline">\(V_\pi(s_t)\)</span>的估计，因此可以让<span class="math inline">\(v(s;w)\)</span>拟合回报<span class="math inline">\(u_t\)</span></p>
<p>预测误差 <span class="math inline">\(\delta_t=v\left(s_t ;
\mathbf{w}\right)-u_t\)</span>.</p>
<p>梯度 <span class="math inline">\(\frac{\partial \delta_t^2 /
2}{\partial \mathbf{w}}=\delta_t \cdot \frac{\partial v\left(s_t ;
\mathbf{w}\right)}{\partial \mathbf{w}}\)</span>.</p>
<p>梯度下降更新网络参数 <span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w}-\alpha \cdot \delta_t \cdot
\frac{\partial v\left(s_t ; \mathbf{w}\right)}{\partial \mathbf{w}} .
\]</span></p>
<p>每轮epsilon完成以下步骤</p>
<ol type="1">
<li>玩一局记录所有State、Action、Reword， <span class="math inline">\(s_1, a_1, r_1, s_2, a_2, r_2, \cdots, s_n, a_n,
r_n .\)</span></li>
<li>计算 <span class="math inline">\(u_t=\sum_{i=t}^n \gamma^{i-t} \cdot
r_i\)</span> 和 <span class="math inline">\(\delta_t=v\left(s_t ;
\mathbf{w}\right)-u_t\)</span>.</li>
<li>利用策略梯度更新策略网络 <span class="math inline">\(\boldsymbol{\theta} \leftarrow
\boldsymbol{\theta}-\beta \cdot \delta_t \cdot \frac{\partial \ln
\pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} .\)</span></li>
<li>利用回归更新价值网络 <span class="math inline">\(\mathbf{w}
\leftarrow \mathbf{w}-\alpha \cdot \delta_t \cdot \frac{\partial
v\left(s_t ; \mathbf{w}\right)}{\partial \mathbf{w}}\)</span></li>
</ol>
<p>算法实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义三层网络结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.l2 = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.l1(x)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        x = self.l2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyGradient</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_actions, n_features, n_hidden=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.95</span>, seed=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.n_hidden = n_hidden</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储每个episode的state、action、reward</span></span><br><span class="line">        self.ep_obs, self.ep_as, self.ep_rs = [], [], []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            torch.manual_seed(seed)</span><br><span class="line">            np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 建立网络</span></span><br><span class="line">        self._build_net()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        建立策略网络、优化器</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.policy_net = Net(self.n_features, self.n_hidden, self.n_actions)</span><br><span class="line">        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters(), lr=self.lr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        利用policy_net计算动作概率分布，依概率选择action</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = torch.FloatTensor(observation[np.newaxis, :])</span><br><span class="line">        prob = torch.softmax(self.policy_net(x), dim=<span class="number">1</span>).data.numpy()[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(prob)</span></span><br><span class="line">        <span class="keyword">return</span> np.random.choice(<span class="built_in">range</span>(self.n_actions), p=prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, state, action, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        保存一个episode中的s,a,r</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.ep_obs.append(state)</span><br><span class="line">        self.ep_as.append(action)</span><br><span class="line">        self.ep_rs.append(reward)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        一轮结束迭代后更新网络参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># compute the discounted return ut for all t</span></span><br><span class="line">        ep_ut = torch.tensor(self._discount_and_norm_rewards())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个state的action概率</span></span><br><span class="line">        all_prob = self.policy_net(torch.FloatTensor(np.vstack(self.ep_obs)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 各state选择的action</span></span><br><span class="line">        all_acts = torch.tensor(self.ep_as)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cross_entropy combines nn.LogSoftmax() and nn.NLLLoss() in one single class</span></span><br><span class="line">        neg_log_prob = torch.nn.functional.cross_entropy(all_prob, all_acts, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 取均值</span></span><br><span class="line">        loss = torch.mean(neg_log_prob * ep_ut)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播更新参数</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清空记录</span></span><br><span class="line">        self.ep_rs, self.ep_as, self.ep_obs = [], [], []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ep_ut</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_discount_and_norm_rewards</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        compute the discounted return ut for all t</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        ep_ut = np.zeros_like(self.ep_rs)</span><br><span class="line">        discount_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.ep_rs))):</span><br><span class="line">            discount_reward = self.ep_rs[i] + discount_reward * self.gamma</span><br><span class="line">            ep_ut[i] = discount_reward</span><br><span class="line"></span><br><span class="line">        ep_ut -= np.mean(ep_ut)</span><br><span class="line">        ep_ut /= np.std(ep_ut)</span><br><span class="line">        <span class="keyword">return</span> ep_ut</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">argv</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">with</span> gym.make(<span class="string">&quot;llvm-ic-v0&quot;</span>, benchmark=benchmark) <span class="keyword">as</span> env:</span><br><span class="line">        </span><br><span class="line">        RL = PolicyGradient(...)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, FLAGS.episodes + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 初始化环境</span></span><br><span class="line">            observation = env.reset()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> step_episode <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.episode_length):</span><br><span class="line">                action = <span class="built_in">int</span>(RL.choose_action(observation))</span><br><span class="line"></span><br><span class="line">                observation_, reward, done, info = env.step(action)</span><br><span class="line">                RL.store_transition(observation, action, reward)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done <span class="keyword">or</span> (step_episode == FLAGS.episode_length - <span class="number">1</span>):</span><br><span class="line">                    RL.learn()</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                observation = observation_</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>策略学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【RL】Multi-Agent Learning</title>
    <url>/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[<h1 id="rlmulti-agent-learning">【RL】Multi-Agent Learning</h1>
<p>一个系统中存在多个决策者</p>
<p><strong>面临的困难</strong></p>
<ul>
<li><p>环境非平稳变化（马尔科夫特性不再满足），每个智能体所对应的环境包含了其他智能体的策略，其他智能体策略的变化导致环境变化的非平稳性。</p></li>
<li><p>局部可观，智能体只能观测到环境的部分状态</p></li>
<li><p>集中式学习不可行，需要一个集中式控制中心与智能体之间进行大量的信息交互，神经网络的输入输出维度会随智能体数目指数增大，难以收敛。</p></li>
</ul>
<h2 id="基本概念">基本概念</h2>
<h3 id="agent的关系">Agent的关系</h3>
<ul>
<li><p>Fully
cooperative，两个Agent需要相互配合才能完成任务（工业机器人）</p></li>
<li><p>Fully
competitive，一方的收益是另一方的损失（捕食者和猎物）</p></li>
<li><p>Mixed Cooperative &amp;
competitive，既有竞争又有合作（足球机器人，队伍内是合作，队伍间是博弈）</p></li>
<li><p>Self-interested，利己主义，只想最大化自己的利益，动作可能让别人获益or损失，不关心别人的利益（股票的期货交易系统、无人车）</p></li>
</ul>
<h3 id="state-action">State &amp; Action</h3>
<p>There are <span class="math inline">\(n\)</span> agents.</p>
<p>Let <span class="math inline">\(S\)</span> be the state.</p>
<p>Let <span class="math inline">\(A^i\)</span> be the <span class="math inline">\(i\)</span>-th agent's action.</p>
<p>State transition: <span class="math inline">\(p\left(s^{\prime} \mid
s, a^1, \cdots, a^n\right)=\mathbb{P}\left(S^{\prime}=s^{\prime} \mid
S=s, A^1=a^1, \cdots, A^n=a^n\right).\)</span></p>
<p>The next state, <span class="math inline">\(S^{\prime}\)</span>,
depends on all the agents' actions.</p>
<p>每个Agent的Action都可能影响Env的下个State，所以每个Agent都可以影响其他Agent</p>
<h3 id="rewards">Rewards</h3>
<p>Let <span class="math inline">\(R^i\)</span> be the reward received
by the <span class="math inline">\(i\)</span>-th agent.</p>
<p>Fully cooperative: <span class="math inline">\(R^1=R^2=\cdots=R^n\)</span>.</p>
<p>Fully competitive: <span class="math inline">\(R^1
\propto-R^2\)</span>.</p>
<p><span class="math inline">\(R^i\)</span> depends on <span class="math inline">\(A^i\)</span> as well as all the other agents'
actions <span class="math inline">\(\left\{A^j\right\}_{j \neq
i}\)</span>.</p>
<h3 id="returns">Returns</h3>
<p>Let <span class="math inline">\(R_t^i\)</span> be the reward received
by the <span class="math inline">\(i\)</span>-th agent at time <span class="math inline">\(t\)</span>. Return (of the <span class="math inline">\(i\)</span>-th agent): <span class="math display">\[
U_t^i=R_t^i+R_{t+1}^i+R_{t+2}^i+R_{t+3}^i+\cdots
\]</span></p>
<p>Discounted return (of the <span class="math inline">\(i\)</span>-th
agent): <span class="math display">\[
U_t^i=R_t^i+\gamma \cdot R_{t+1}^i+\gamma^2 \cdot R_{t+2}^i+\gamma^3
\cdot R_{t+3}^i+\cdots
\]</span></p>
<p>Here, <span class="math inline">\(\gamma \in[0,1]\)</span> is the
discount rate.</p>
<h3 id="policy-network">Policy Network</h3>
<p>Each agent has its own policy network: <span class="math inline">\(\pi\left(a^i \mid s ;
\boldsymbol{\theta}^i\right)\)</span>.</p>
<p>Policy networks can be exchangeable: <span class="math inline">\(\boldsymbol{\theta}^1=\boldsymbol{\theta}^2=\cdots=\boldsymbol{\theta}^n\)</span>.</p>
<ul>
<li>Self-driving cars can have the same policy.</li>
</ul>
<p>Policy networks can be nonexchangeable: <span class="math inline">\(\boldsymbol{\theta}^i \neq
\boldsymbol{\theta}^j\)</span>.</p>
<ul>
<li>Soccer players have different roles, e.g., striker, defender,
goalkeeper.</li>
</ul>
<h3 id="state-value-function">State-Value Function</h3>
<p>State-value of the <span class="math inline">\(i\)</span>-th agent:
(不仅依赖自己的策略，同时依赖其他所有Agent的策略) <span class="math display">\[
V^i\left(s_t ; \boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\right)=\mathbb{E}\left[U_t^i \mid S_t=s_t\right] .
\]</span></p>
<p>The expectation is taken w.r.t. all the future actions and states
except <span class="math inline">\(S_t\)</span>.</p>
<p>Randomness in actions: <span class="math inline">\(A_t^j \sim
\pi\left(\cdot \mid s_t ; \boldsymbol{\theta}^j\right)\)</span>, for all
<span class="math inline">\(j=1, \cdots, n\)</span>. (That is why the
state-value <span class="math inline">\(V^i\)</span> depends on <span class="math inline">\(\boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\)</span>.)</p>
<h2 id="nash-equilibrium纳什均衡">Nash Equilibrium（纳什均衡）</h2>
<p>Multi-Agent问题收敛的条件</p>
<p>当其他所有Agent的政策保持不变时，第 <span class="math inline">\(i\)</span>
个Agent无法通过改变自己的Policy获得更好的预期收益。</p>
<p>每个Agent都在对其他Agent的Policy做出最佳反应。</p>
<p>在这种平衡状态下，大家都没动机去改变自己策略</p>
<p>如果使用Single-Agent的策略梯度算法直接用于Mulit-Agent问题上</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206092127067.png" alt="image-20231206092127067">
<figcaption aria-hidden="true">image-20231206092127067</figcaption>
</figure>
<p>The <span class="math inline">\(i\)</span>-th agent's policy network:
<span class="math inline">\(\pi\left(a^i \mid s ;
\boldsymbol{\theta}^i\right)\)</span>.</p>
<p>The <span class="math inline">\(i\)</span>-th agent's state-value
function: <span class="math inline">\(V^i\left(s ;
\boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\right)\)</span>.</p>
<p>Objective function: <span class="math inline">\(J^i\left(\boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\right)=\mathbb{E}_S\left[V^i\left(S ;
\boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\right)\right]\)</span>.</p>
<p>Learn the policy network's parameter, <span class="math inline">\(\boldsymbol{\theta}^i\)</span>, by <span class="math display">\[
\max _{\boldsymbol{\theta}^i} J^i\left(\boldsymbol{\theta}^1, \cdots,
\boldsymbol{\theta}^n\right)
\]</span></p>
<ul>
<li>The <span class="math inline">\(1^{\text {st }}\)</span> agent
solves: <span class="math inline">\(\quad \max
_{\theta^1}J^1\left(\boldsymbol{\theta}^1, \boldsymbol{\theta}^2,
\cdots, \boldsymbol{\theta}^n)\right.\)</span>.</li>
<li>The <span class="math inline">\(2^{\text {nd }}\)</span> agent
solves: <span class="math inline">\(\quad \max _{\theta^2}
J^2\left(\boldsymbol{\theta}^1, \boldsymbol{\theta}^2, \cdots,
\boldsymbol{\theta}^n\right)\)</span>.</li>
<li>...</li>
<li>The <span class="math inline">\(n^{\text {th }}\)</span> agent
solves: <span class="math inline">\(\quad \max _{\boldsymbol{\theta}^n}
J^n\left(\boldsymbol{\theta}^1, \boldsymbol{\theta}^2, \cdots,
\boldsymbol{\theta}^n\right)\)</span>.</li>
</ul>
<p>每个Agent没有共同的目标，各自更新自己的<span class="math inline">\(\theta^i\)</span>，一个Agent更新策略，可能导致其他所有Agent的目标函数发生变化，大家的目标函数都在不停的发生变化，导致训练很难收敛</p>
<p>所以不能将Single-Agent的训练策略应用与Multi-Agent问题</p>
<h2 id="centralized-vs-decentralized">Centralized Vs Decentralized</h2>
<h3 id="architectures">Architectures</h3>
<ul>
<li><p>Fully
decentralized，每个Agent都用自己观测到的observation、reward去训练策略，Agent之间不通信</p></li>
<li><p>Fully
centralized，所有Agent将信息发给中央控制器，中央控制器负责做decision，Agent负责执行</p></li>
<li><p>Centralized training with decentralized
execution，中央控制器在训练阶段收集各Agent信息，帮助训练Agent策略网络，训练结束后，Agent自己利用策略网络做decision，不依赖中央控制器通信</p></li>
</ul>
<h3 id="partial-observation">Partial Observation</h3>
<p>An agent may or may not have full knowledge of the state, <span class="math inline">\(s\)</span>.</p>
<p>Let <span class="math inline">\(o^i\)</span> be the <span class="math inline">\(i\)</span>-th agent's observation.</p>
<p>Partial observation: <span class="math inline">\(o^i \neq
S\)</span>.</p>
<p>Full observation: <span class="math inline">\(\quad
o^1=\cdots=o^n=s\)</span>.</p>
<h3 id="fully-decentralized">Fully Decentralized</h3>
<p>本质就是Single-Agent reinforce learning</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206094004380.png" alt="image-20231206094004380">
<figcaption aria-hidden="true">image-20231206094004380</figcaption>
</figure>
<p>The <span class="math inline">\(i\)</span>-th agent has a policy
network (actor): <span class="math inline">\(\pi\left(a^i \mid o^i ;
\boldsymbol{\theta}^i\right)\)</span>.</p>
<p>The <span class="math inline">\(i\)</span>-th agent has a value
network (critic): <span class="math inline">\(q\left(o^i, a^i ;
\mathbf{w}^i\right)\)</span>.</p>
<p>Agents do not share observations and actions.</p>
<p>Train the policy and value networks in the same way as the
single-agent setting.</p>
<p>This does not work well.</p>
<h3 id="fully-centralized">Fully Centralized</h3>
<p>各Agent中没有策略网络，将观测到的信息传递给中央控制器，控制器负责做决策。
<span class="math display">\[
\pi\left(a^i \mid o^1, \cdots, o^n ; \boldsymbol{\theta}^i\right) \text
{, for all } i=1,2, \cdots, n \text {. }
\]</span> <img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206094349263.png" alt="image-20231206094349263"></p>
<p>Let <span class="math inline">\(\mathbf{a}=\left[a^1, a^2, \cdots,
a^n\right]\)</span> contain all the agents' actions.</p>
<p>Let <span class="math inline">\(\mathbf{o}=\left[o^1, o^2, \cdots,
o^n\right]\)</span> contain all the agents' observations.</p>
<p>The central controller knows <span class="math inline">\(\mathbf{a},
\mathbf{o}\)</span>, and all the rewards.</p>
<p>The controller has <span class="math inline">\(n\)</span> policy
networks and <span class="math inline">\(n\)</span> value networks:</p>
<ul>
<li>Policy network (actor) for the <span class="math inline">\(i\)</span>-th agent: <span class="math inline">\(\pi\left(a^i \mid o ;
\boldsymbol{\theta}^i\right)\)</span>.</li>
<li>Value network (critic) for the <span class="math inline">\(i\)</span>-th agent: <span class="math inline">\(q\left(o, \mathbf{a} ;
\mathbf{w}^i\right)\)</span>.</li>
</ul>
<p><strong>Centralized Training:</strong> Training is performed by the
controller.</p>
<ul>
<li>The controller knows all the observations, actions, and
rewards.</li>
<li>Train <span class="math inline">\(\pi\left(a^i \mid 0 ;
\boldsymbol{\theta}^i\right)\)</span> using policy gradient.</li>
<li>Train <span class="math inline">\(q\left(\mathbf{o}, \mathbf{a} ;
\mathbf{w}^i\right)\)</span> using TD algorithm.</li>
</ul>
<p><strong>Centralized Execution:</strong> Decisions are made by the
controller.</p>
<ul>
<li>For all <span class="math inline">\(i\)</span>, the <span class="math inline">\(i\)</span>-th agent sends its observation, <span class="math inline">\(o^i\)</span>, to the controller.</li>
<li>The controller knows <span class="math inline">\(\mathbf{o}=\left[o^1, o^2, \cdots,
o^n\right]\)</span>.</li>
<li>For all <span class="math inline">\(i\)</span>, the controller
samples action by <span class="math inline">\(a^i \sim \pi\left(\cdot
\mid \mathbf{o} ; \boldsymbol{\theta}^i\right)\)</span> and sends <span class="math inline">\(a^i\)</span> to the <span class="math inline">\(i\)</span>-th agent.</li>
</ul>
<p>中央控制器知道全局的信息，可以帮所有Agent做出好的决策</p>
<p>但执行速度较慢，Agent都没有决策权，需要等中央来做决策（发送、同步信息都需要花时间，等最慢的Agent）</p>
<h3 id="centralized-training-with-decentralized-execution">Centralized
Training with Decentralized Execution</h3>
<p>Each agent has its own policy network (actor): <span class="math inline">\(\pi\left(a^i \mid o^i ;
\boldsymbol{\theta}^i\right)\)</span>.</p>
<p>The central controller has <span class="math inline">\(n\)</span>
value networks (critics): <span class="math inline">\(q\left(0,
\mathbf{a} ; \mathbf{w}^i\right)\)</span>.</p>
<p><strong>Centralized Training:</strong> During training, the central
controller knows all the agents' observations, actions, and rewards.</p>
<p><strong>Decentralized Execution:</strong> During execution, the
central controller and its value networks are not used.</p>
<h4 id="训练阶段">训练阶段</h4>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206095643649.png" alt="image-20231206095643649">
<figcaption aria-hidden="true">image-20231206095643649</figcaption>
</figure>
<p>The central controller trains the critics, <span class="math inline">\(q\left(\mathbf{o}, \mathbf{a} ;
\mathbf{w}^i\right)\)</span>, for all <span class="math inline">\(i\)</span>.</p>
<p>To update <span class="math inline">\(\mathbf{w}^i\)</span>,
<strong>TD algorithm</strong> takes as inputs:</p>
<ul>
<li>All the actions: <span class="math inline">\(\mathrm{a}=\left[a^1,
a^2, \cdots, a^n\right]\)</span>.</li>
<li>All the observations: <span class="math inline">\(\mathbf{o}=\left[o^1, o^2, \cdots,
o^n\right]\)</span>.</li>
<li>The <span class="math inline">\(i\)</span>-th reward: <span class="math inline">\(r^i\)</span>.</li>
</ul>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206095819592.png" alt="image-20231206095819592">
<figcaption aria-hidden="true">image-20231206095819592</figcaption>
</figure>
<p>Each agent locally trains the actor, <span class="math inline">\(\pi\left(a^i \mid o^i ;
\boldsymbol{\theta}^i\right)\)</span>, using <strong>policy
gradient.</strong></p>
<p>To update <span class="math inline">\(\boldsymbol{\theta}^i\)</span>,
the policy gradient algorithm takes as input <span class="math inline">\(\left(a^i, o^i, a^i\right)\)</span>,</p>
<h4 id="执行阶段">执行阶段</h4>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206100104303.png" alt="image-20231206100104303">
<figcaption aria-hidden="true">image-20231206100104303</figcaption>
</figure>
<p>每个Agent独立接受observation，依据自己的策略网络做决策，Agent之间不通信</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
  </entry>
  <entry>
    <title>【RL】Value-Based Learning</title>
    <url>/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="rlvalue-based-learning">【RL】Value-Based Learning</h1>
<h2 id="基于价值学习">基于价值学习</h2>
<h3 id="discounted-return">Discounted Return</h3>
<p><span class="math display">\[
\begin{aligned}
U_t&amp; =R_t+\gamma\cdot R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot
R_{t+3}+\gamma^4\cdot R_{t+4} \\
&amp; =R_t+\gamma\cdot (R_{t+1}+\gamma^2\cdot R_{t+2}+\gamma^3\cdot
R_{t+3}+\gamma^4\cdot R_{t+4}) \\
&amp; =R_t+\gamma\cdot U_{t+1}
\end{aligned}
\]</span></p>
<p><strong>Identity:</strong> <span class="math inline">\(U_t=R_t+\gamma\cdot U_{t+1}\)</span></p>
<h3 id="td-target">TD Target</h3>
<p>Assume <span class="math inline">\(R_t\)</span> depends on <span class="math inline">\((S_t, A_t, S_{t+1})\)</span> <span class="math display">\[
\begin{aligned}
Q_\pi\left(s_t, a_t\right) &amp; =\mathbb{E}\left[U_t \mid s_t,
a_t\right] \\
&amp; =\mathbb{E}\left[R_t+\gamma \cdot U_{t+1} \mid s_t, a_t\right] \\
&amp; =\mathbb{E}\left[R_t \mid s_t, a_t\right]+\gamma \mathbb{\mathbb {
E } [ U _ { t + 1 } | s _ { t } , a _ { t } ]} \\
&amp; =\mathbb{E}\left[R_t \mid s_t, a_t\right]+\gamma \cdot
\mathbb{E}\left[Q_\pi\left(s_{t+1}, A_{t+1}\right) \mid s_t, a_t\right]
.
\end{aligned}
\]</span> <strong>Identity:</strong> <span class="math inline">\(Q_\pi(s_t,a_t)=\mathbb{E}[R_t+\gamma\cdot
Q_\pi(S_{t+1},A_{t+1})]\)</span> , for all <span class="math inline">\(\pi\)</span></p>
<p>左边是该动作的action-value
function，右边含有期望，直接计算比较困难，故利用蒙特卡洛近似计算。</p>
<p><span class="math inline">\(R_t\)</span> 近似为观测值<span class="math inline">\(r_t\)</span>, <span class="math inline">\(Q_\pi(S_{t+1},A_{t+1})\)</span>中<span class="math inline">\(S_{t+1},A_{t+1}\)</span>都是随机变量，利用观测值<span class="math inline">\(s_{t+1},a_{t+1}\)</span>近似 <span class="math display">\[
R_t+\gamma\cdot Q_\pi(S_{t+1},A_{t+1}) \approx r_t+\gamma\cdot
Q_\pi(s_{t+1},a_{t+1}) = TD\ target
\]</span> 期望<span class="math inline">\(\mathbb{E}[R_t+\gamma\cdot
Q_\pi(S_{t+1},A_{t+1})] \approx y_t\)</span></p>
<p><strong>TD learning: Encourage <span class="math inline">\(Q_\pi(s_t,a_t)\)</span> to approach <span class="math inline">\(y_t\)</span></strong></p>
<p>因为<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>完全是估计，而<span class="math inline">\(y_t\)</span>部分基于真实的奖励，相对于更加可靠</p>
<h3 id="td-error">TD Error</h3>
<p><span class="math display">\[
\delta_t=Q_\pi(s_t,a_t)-y_t
\]</span></p>
<h2 id="sarsa">Sarsa</h2>
<p><strong>学习目标</strong>：更新<span class="math inline">\(Q_\pi\)</span> 使其更加接近真实值</p>
<p>每一轮迭代更新都依据五元组<span class="math inline">\((s_t,a_t,r_t,s_{t+1},a_{t+1})\)</span>更新<span class="math inline">\(Q_\pi\)</span></p>
<p>因此命名State-Action-Reward-State-Action（SARSA）</p>
<p>更新q-table时采用下一步真实的s‘和a’更新q值，也称为on-policy在线学习</p>
<h3 id="表格形式">表格形式</h3>
<p>State和Action的数量是有限的（离散），通过一个表格来更新<span class="math inline">\(Q_\pi\)</span></p>
<p>每次更新表格中的一个元素，使得TD Error变小</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>Sample <span class="math inline">\(a_{t+1} \sim \pi\left(\cdot \mid
s_{t+1}\right)\)</span>, where <span class="math inline">\(\pi\)</span>
is the policy function.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q_\pi\left(s_{t+1}, a_{t+1}\right)\)</span>. 下一个动作的q值</li>
<li>TD error: <span class="math inline">\(\delta_t=Q_\pi\left(s_t,
a_t\right)-y_t\)</span>.</li>
<li>Update: <span class="math inline">\(Q_\pi\left(s_t, a_t\right)
\leftarrow Q_\pi\left(s_t, a_t\right)-\alpha \cdot
\delta_t\)</span>.</li>
</ol>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185059478.png" alt="image-20240731185059478">
<figcaption aria-hidden="true">image-20240731185059478</figcaption>
</figure>
<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>():</span></span><br><span class="line">    ....</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q = self.q_table.loc[s,a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            <span class="comment"># 下个行动所采取的真实值</span></span><br><span class="line">            q_ = r + self.gamma * self.q_table.loc[s_, a_]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Q learning 中采用下个阶段可能行动的最大值</span></span><br><span class="line">            <span class="comment"># q_ = r + self.gamma * self.q_table.loc[s_, :].max()</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_ = r</span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_ - q)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> epis <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        observation = env.reset()</span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            env.render()</span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取下个阶段的action，并利用s_和a_同时更新参数</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 记录下个阶段的a_和s_</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="神经网络形式">神经网络形式</h3>
<p>价值网络<span class="math inline">\(q(s,a;w)\)</span>是对动作价值函数<span class="math inline">\(Q_\pi\)</span>的近似，利用sarsa算法更新网络参数</p>
<p>在actor-critic方法中用于对价值网络的参数进行改进</p>
<p>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot
q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)\)</span>.</p>
<p>TD error: <span class="math inline">\(\delta_t=q\left(s_t, a_t ;
\mathbf{w}\right)-y_t\)</span>.</p>
<p>Loss: <span class="math inline">\(\delta_t^2 / 2\)</span>.</p>
<p>Gradient: <span class="math inline">\(\frac{\partial \delta_t^2 /
2}{\partial \mathbf{w}}=\delta_t \cdot \frac{\partial q\left(s_t, a_t ;
\mathbf{w}\right)}{\partial \mathbf{w}}\)</span>.</p>
<p>Gradient descent: <span class="math inline">\(\quad \mathbf{w}
\leftarrow \mathbf{w}-\alpha \delta_t \cdot \frac{\partial q\left(s_t,
a_t ; \mathbf{w}\right)}{\partial \mathbf{w}}\)</span></p>
<h3 id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</h3>
<p>这种方法针对于表格形式Sarsa</p>
<p>Sarsa(<span class="math inline">\(\lambda\)</span>)就是更新获取到
reward 的前 lambda 步. lambda 是在 [0, 1] 之间取值。</p>
<p>不仅更新当前的<span class="math inline">\(Q_\pi(s_t,a_t)\)</span>同时更新当前回合前面路径state-action的<span class="math inline">\(Q_\pi(s,a)\)</span></p>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185138592.png" alt="image-20240731185138592">
<figcaption aria-hidden="true">image-20240731185138592</figcaption>
</figure>
<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line">        self.lambda_ = trace_decay <span class="comment"># backward view, eligibility trace.</span></span><br><span class="line">        self.eligibility_trace = self.q_table.copy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            to_be_append = pd.Series([<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),index=self.q_table.columns,name=state,)</span><br><span class="line">            self.q_table = self.q_table.append(to_be_append) <span class="comment"># append new state to q table</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append) <span class="comment"># also update eligibility trace</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="comment"># next state is not terminal</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># next state is terminal</span></span><br><span class="line">        error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里开始不同:</span></span><br><span class="line">        <span class="comment"># 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环</span></span><br><span class="line">        <span class="comment"># Method 1:</span></span><br><span class="line">        <span class="comment"># self.eligibility_trace.loc[s, a] += 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Method 2:</span></span><br><span class="line">        self.eligibility_trace.loc[s, :] *= <span class="number">0</span></span><br><span class="line">        self.eligibility_trace.loc[s, a] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q update</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decay eligibility trace after update</span></span><br><span class="line">        self.eligibility_trace *= self.gamma*self.lambda_</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> epis <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        observation = env.reset()</span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">        RL.eligibility_trace *= <span class="number">0</span> <span class="comment"># eligibility trace 只是记录每个回合的每一步, 新回合开始的时候需要将 Trace 清零.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            ....</span><br></pre></td></tr></table></figure>
<h2 id="q-learning">Q-Learning</h2>
<h3 id="学习目标">学习目标</h3>
<p>训练最优价值函数<span class="math inline">\(Q^\star(s,a)\)</span></p>
<p><strong>TD Target:</strong> <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q^{\star}\left(s_{t+1}, a\right) .
\]</span> 利用Q-learning更新DQN</p>
<h3 id="数学推导">数学推导</h3>
<p>通过Sarsa部分推导,对于所有的 <span class="math inline">\(\pi\)</span>, <span class="math display">\[
Q_\pi\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot
Q_\pi\left(S_{t+1}, A_{t+1}\right)\right] .
\]</span> 如果选择最优的策略<span class="math inline">\(\pi^\star\)</span> <span class="math display">\[
Q_{\pi^{\star}}\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot
Q_{\pi^{\star}}\left(S_{t+1}, A_{t+1}\right)\right] .
\]</span> <span class="math inline">\(Q_{\pi^{\star}}\)</span> 和 <span class="math inline">\(Q^{\star}\)</span> 都表示最优动作价值函数</p>
<p>Identity: <span class="math inline">\(Q^{\star}\left(s_t,
a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot Q^{\star}\left(S_{t+1},
A_{t+1}\right)\right]\)</span>.</p>
<p>动作<span class="math inline">\(A_{t+1}\)</span>通过以下公式计算
<span class="math display">\[
A_{t+1}=\underset{a}{\operatorname{argmax}} Q^{\star}\left(S_{t+1},
a\right) .
\]</span></p>
<p>因此 <span class="math inline">\(Q^{\star}\left(S_{t+1},
A_{t+1}\right)=\max _a Q^{\star}\left(S_{t+1}, a\right)\)</span>.</p>
<p>所以<span class="math inline">\(Q^{\star}\left(s_t,
a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot \max
_aQ^{\star}\left(S_{t+1}, a\right)\right]\)</span>.</p>
<p>期望直接求比较困难,所以利用蒙特卡洛近似,<span class="math inline">\(R_t\)</span>利用观测值<span class="math inline">\(r_t\)</span>近似,<span class="math inline">\(S_{t+1}\)</span>利用观测值<span class="math inline">\(s_{t+1}\)</span>近似得到: <span class="math display">\[
Q^{\star}\left(s_t, a_t\right)=\mathbb{E}\left[R_t+\gamma \cdot \max
_aQ^{\star}\left(S_{t+1},
a\right)\right]\approx\mathbb{E}\left[r_t+\gamma \cdot \max
_aQ^{\star}\left(s_{t+1}, a\right)\right]=TD\ Target\ y_t
\]</span></p>
<p>由此，可以利用表格形式学习<span class="math inline">\(Q_\pi\)</span>,针对离散action、state的情况。也可利用神经网络学习<span class="math inline">\(Q_\pi\)</span>
可以用于连续action、state的DQN方法</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205105624651.png" alt="image-20231205105624651">
<figcaption aria-hidden="true">image-20231205105624651</figcaption>
</figure>
<h3 id="表格形式-1">表格形式</h3>
<p>针对<strong>离散状态、离散动作</strong>情况,当状态和动作数量变多时，表格会越来越大</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q^{\star}\left(s_{t+1}, a\right)\)</span>.</li>
<li>TD error: <span class="math inline">\(\delta_t=Q^{\star}\left(s_t,
a_t\right)-y_t\)</span>.</li>
<li>Update: <span class="math inline">\(\quad Q^{\star}\left(s_t,
a_t\right) \leftarrow Q^{\star}\left(s_t, a_t\right)-\alpha \cdot
\delta_t\)</span>.</li>
</ol>
<p>算法：</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731185159947.png" alt="image-20240731185159947">
<figcaption aria-hidden="true">image-20240731185159947</figcaption>
</figure>
<p>实现代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Q-learning算法</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 横轴为ations, 纵轴为states</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        选择 action</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.check_state_exist(<span class="built_in">str</span>(observation))</span><br><span class="line">        p_actions = self.q_table.loc[observation, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> np.random.choice(p_actions[p_actions == np.<span class="built_in">max</span>(p_actions)].index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.random.choice(self.actions)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新模型参数 动作-状态价值函数（Q值）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q(st,at) 原始估计</span></span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用reward,state_计算Q_(st,at)新的估计值</span></span><br><span class="line">        q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用贝尔曼方程更新Q(st,at)的值</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        检查当前的状态是否在q-table表中出现，不存在则添加一行</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            self.q_table = self.q_table.append(pd.Series([<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),index=self.q_table.columns,name=state))</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate the reward of q-table</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollout</span>(<span class="params">RL, env, printout=<span class="literal">False</span></span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q_table, env</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    train the q-table with compilerGym env</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, FLAGS.episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step_episode <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.episode_length):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = q_table.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line">            action_label = FLAGS.flags[action]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据action给出下一个state，reward，是否终止</span></span><br><span class="line">            observation_, reward, done, info = env.step(env.action_space.flags.index(action_label))</span><br><span class="line">            observation_ = observation_[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 通过给出反馈，更新模型参数</span></span><br><span class="line">            q_table.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state_ 变为 下次循环的 state</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="dqn网络形式">DQN（网络形式）</h3>
<p><strong>学习目标：</strong>更新网络参数<span class="math inline">\(w\)</span>,学习最优动作价值函数<span class="math inline">\(Q^\star\)</span></p>
<p><strong>局限性：</strong>动作要求是<strong>离散</strong>的，当动作维度增大时，神经网络的规模会指数增加</p>
<p>利用神经网络 DQN, <span class="math inline">\(Q(s, a ;
\mathbf{w})\)</span> 近似 <span class="math inline">\(Q^{\star}(s,
a)\)</span>，用神经网络拟合状态到Q值之间的映射关系。状态作为神经网络的输入，可以在<strong>连续</strong>范围内取值，最终输出得到的是该状态下对应每个动作的Q值，然后我们可以在其中选择一个令Q值最大的作为最优动作。</p>
<p>网络的输入为State，输出为各Action的Q</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205111712543.png" alt="image-20231205111712543">
<figcaption aria-hidden="true">image-20231205111712543</figcaption>
</figure>
<p>通过DQN网络输出的最大Q值的动作 <span class="math inline">\(a_t=\underset{a}{\operatorname{argmax}}
Q\left(s_t, a ; \mathbf{w}\right)\)</span>控制agent做动作</p>
<ol type="1">
<li>Observe a transition <span class="math inline">\(\left(s_t, a_t,
r_t, s_{t+1}\right)\)</span>.</li>
<li>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q\left(s_{t+1}, a;w\right)\)</span>.</li>
<li>TD error: <span class="math inline">\(\delta_t=Q\left(s_t,
a_t;w\right)-y_t\)</span>.</li>
<li>Loss: <span class="math inline">\(L_t=\frac{1}{2}[Q(s_t,a_t;w)-y_t]^2\)</span></li>
<li>Update: <span class="math inline">\(w \leftarrow w-\alpha \cdot
\delta_t\cdot\frac{\partial Q(s_t,a_t;w)}{\partial w}\)</span>.</li>
</ol>
<h2 id="multi-step-td-target">Multi-Step TD Target</h2>
<h3 id="sarsa-vs-q-learning">Sarsa vs Q-learning</h3>
<p>Sarsa 训练动作价值函数（action-value function）<span class="math inline">\(Q\pi(s,a)\)</span></p>
<p>TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q_\pi\left(s_{t+1}, a_{t+1}\right)\)</span></p>
<p>Q-Learning训练最优动作价值函数（optimal action-value function）<span class="math inline">\(Q^\star (s,a)\)</span></p>
<p>TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot \max _a
Q^{\star}\left(s_{t+1}, a\right)\)</span></p>
<p>前面学习的Sarsa以及Q-Learning都是只使用一个transition<span class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span>
其中包含一个reward，称为One-Step TD Target</p>
<p>此外可以同时使用多步transition中的<span class="math inline">\(r_t,r_{t+1},r_{t+2}...r_{t+m-1}\)</span>进行更新，这样比单步的学习效果更好。由于包含多步reward，因此Q值更加接近于真实值</p>
<h3 id="数学推导-1">数学推导</h3>
<p><span class="math display">\[
\begin{aligned}
U_{t}&amp;=R_{t}+\gamma \cdot U_{t+1} \\&amp;=R_{t}+\gamma \cdot
R_{t+1}+\gamma^{2} \cdot U_{t+2} \\&amp;=R_{t}+\gamma \cdot
R_{t+1}+\gamma^{2} \cdot R_{t+2}+\gamma^{3} \cdot U_{t+3}
\end{aligned}
\]</span></p>
<p>Identity: <span class="math inline">\(U_{t}=\sum_{i=0}^{m-1}
\gamma^{i} \cdot R_{t+i}+\gamma^{m} \cdot U_{t+m} .\)</span></p>
<p>m-step TD target应用于Sarsa算法 <span class="math display">\[
y_{t}=\sum_{i=0}^{m-1} \gamma^{i} \cdot r_{t+i}+\gamma^{m} \cdot
Q_\pi(s_{t+m},a_{t+m}) .
\]</span></p>
<p>m-step TD target应用于Q-Learing算法 <span class="math display">\[
y_{t}=\sum_{i=0}^{m-1} \gamma^{i} \cdot r_{t+i}+\gamma^{m} \cdot \max
_aQ^\star(s_{t+m},a) .
\]</span></p>
<p>如果将m设置为1，则为标准的Sarsa、Q-Learning算法</p>
<h2 id="经验回放">经验回放</h2>
<p>在前面DQN+TD-Learning中依次利用各<span class="math inline">\(transition(s_t,a_t,r_t,s_{t+1}),t=1,2,..\)</span>来更新网络参数<span class="math inline">\(w\)</span>，更新后丢弃此transition不再使用</p>
<p>在实际环境中<span class="math inline">\(s_t\)</span> 与<span class="math inline">\(s_{t+1}\)</span>之间有非常明显的相关性，这种相关性对模型的训练是有害的，如果能打散这种相关性，则有利于提升模型的训练效率</p>
<p>因此可以将最近的n条transition保存在一个大小为n队列（memory）中，如果memory满了，则利用最老的memory进行替换</p>
<p>Find <span class="math inline">\(\mathbf{w}\)</span> by minimizing
<span class="math inline">\(L(\mathbf{w})=\frac{1}{T} \sum_{t=1}^T
\frac{\delta_t^2}{2}\)</span>.</p>
<p>Stochastic gradient descent (SGD):
(一般使用mini-bach的方法，抽取一组transition，计算loss的平均值来更新网络参数)</p>
<ul>
<li>Randomly sample a transition, <span class="math inline">\(\left(s_i,
a_i, r_i, s_{i+1}\right)\)</span>, from the buffer.</li>
<li>Compute TD error, <span class="math inline">\(\delta_i\)</span>.</li>
<li>Stochastic gradient: <span class="math inline">\(\mathbf{g}_i=\frac{\partial \delta_i^2 /
2}{\partial \mathbf{w}}=\delta_i \cdot \frac{\partial Q\left(s_i, a_i ;
\mathbf{w}\right)}{\partial \mathbf{w}}\)</span></li>
<li>SGD: <span class="math inline">\(\mathbf{w} \leftarrow
\mathbf{w}-\alpha \cdot \mathbf{g}_i\)</span>.</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>可重复利用历史transition，记忆库 (用于重复学习)</li>
<li>打破transition之间的相关性，暂时冻结 <code>q_target</code> 参数
(切断相关性)</li>
</ul>
<p>算法</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731183751203.png" alt="image-20240731183751203">
<figcaption aria-hidden="true">image-20240731183751203</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算DQN的reward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollout</span>(<span class="params">RL, env, printout=<span class="literal">False</span></span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">RL, env</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练DQN</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 记录历史reward</span></span><br><span class="line">    history_reword = []</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, FLAGS.episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()[FLAGS.features_indices]</span><br><span class="line">        <span class="keyword">for</span> step_episode <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.episode_length):</span><br><span class="line">            action = RL.choose_action(observation) <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action_label = FLAGS.flags[action]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据action给出下一个state，reward，是否终止</span></span><br><span class="line">            observation_, reward, done, info = env.step(env.action_space.flags.index(action_label))</span><br><span class="line">            observation_ = observation_[FLAGS.features_indices]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 记录s, a, r, s_,用于获取更新网络参数</span></span><br><span class="line">            RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 控制学习的起始时间和频率（先累积一些记忆再开始学习）</span></span><br><span class="line">            <span class="keyword">if</span> (step &gt; <span class="number">200</span>) <span class="keyword">and</span> (step % <span class="number">5</span> == <span class="number">0</span>):</span><br><span class="line">                RL.learn()</span><br><span class="line"></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">          ...</span><br></pre></td></tr></table></figure>
<h3 id="神经网络与思维决策">神经网络与思维决策</h3>
<p>搭建两个神经网络（见高估问题 Target Network）</p>
<ul>
<li>Target Network 用于计算值TD Target, 不会及时更新参数</li>
<li>DQN用于控制agent做运动，并收集transition,
这个神经网络拥有最新的神经网络参数.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义网络结构，三层神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.el = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.q = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.el(x)</span><br><span class="line">        x = F.relu(x)  <span class="comment"># relu作为激活函数</span></span><br><span class="line">        x = self.q(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_actions, n_features, n_hidden=<span class="number">20</span>, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 replace_target_iter=<span class="number">200</span>, memory_size=<span class="number">500</span>, batch_size=<span class="number">32</span>, seed=<span class="literal">None</span>, e_greedy_increment=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        self.n_actions = n_actions  <span class="comment"># action维度</span></span><br><span class="line">        self.n_features = n_features  <span class="comment"># observation/state维度</span></span><br><span class="line">        self.n_hidden = n_hidden  <span class="comment"># 隐藏层神经元个数</span></span><br><span class="line">        self.lr = learning_rate  <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay  <span class="comment"># 回报衰退率</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 替换target网络参数的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记录的size</span></span><br><span class="line">        self.memory_counter = <span class="number">0</span>  <span class="comment"># memory当前记录的idx</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.epsilon_max = e_greedy</span><br><span class="line">        self.epsilon_increment = e_greedy_increment</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选择网络预测action的概率，一开始随机选择action，随着训练逐步增大</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max  </span><br><span class="line">        </span><br><span class="line">        self.learn_step_counter = <span class="number">0</span>  <span class="comment"># total learning step</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features * <span class="number">2</span> + <span class="number">2</span>))  <span class="comment"># initialize zero memory [s, a, r, s_]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.set_random_seed(seed)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()  <span class="comment"># 均方误差</span></span><br><span class="line">        self.cost_his = []  <span class="comment"># 记录cost的历史值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建网络</span></span><br><span class="line">        self._build_net()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化网络</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 创建 eval 神经网络, 及时提升参数</span></span><br><span class="line">        self.q_eval = Net(self.n_features, self.n_hidden, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 target 神经网络, 提供 target Q</span></span><br><span class="line">        self.q_target = Net(self.n_features, self.n_hidden, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用RMSprop作为优化器</span></span><br><span class="line">        self.optimizer = torch.optim.RMSprop(self.q_eval.parameters(), lr=self.lr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        记录历史的state、action、reward、state_</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size</span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        利用q_eval网络 根据观测值选择行为</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将一维数据转换成矩阵</span></span><br><span class="line">        <span class="comment"># x[:, np.newaxis] ：放在后面，会给列上增加维度；</span></span><br><span class="line">        <span class="comment"># x[np.newaxis, :] ：放在前面，会给行上增加维度；</span></span><br><span class="line">        observation = torch.Tensor(observation[np.newaxis, :])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="comment"># 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action</span></span><br><span class="line">            actions_value = self.q_eval(observation)</span><br><span class="line">            action = np.argmax(actions_value.data.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 随机选择action</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        更新网络参数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 更新 target_net 参数</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target.load_state_dict(self.q_eval.state_dict())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 memory 中随机抽取 batch_size 大小记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算target_net的q值</span></span><br><span class="line">        q_next = self.q_target(torch.Tensor(batch_memory[:, -self.n_features:])) </span><br><span class="line">        <span class="comment"># 计算eval_net的q值</span></span><br><span class="line">        q_eval = self.q_eval(torch.Tensor(batch_memory[:, :self.n_features])) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># =========== 计算loss值 ==============</span></span><br><span class="line">        <span class="comment"># 将q_eval的值复制到q_target</span></span><br><span class="line">        q_target = torch.Tensor(q_eval.data.numpy().copy())	</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每transition的action</span></span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每transition的reward</span></span><br><span class="line">        reward = torch.Tensor(batch_memory[:, self.n_features + <span class="number">1</span>])	</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q_target每个样本对应action的概率赋值为reward + gamma * maxQ(s_)</span></span><br><span class="line">        <span class="comment"># torch.max(q_next, 1)[0] 选择每一行的最大值组成一个列表</span></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * torch.<span class="built_in">max</span>(q_next, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        loss = self.loss_func(q_eval, q_target)	<span class="comment"># 将 (q_target - q_eval) 作为误差</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        只需要选择好的 action 的值, 其他的并不需要.</span></span><br><span class="line"><span class="string">        所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line"><span class="string">        这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line"><span class="string">        q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.</span></span><br><span class="line"><span class="string">        q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,</span></span><br><span class="line"><span class="string">        我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line"><span class="string">        q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        q_target = q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line"><span class="string">        比如在:</span></span><br><span class="line"><span class="string">            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;</span></span><br><span class="line"><span class="string">            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:</span></span><br><span class="line"><span class="string">        q_target =</span></span><br><span class="line"><span class="string">        [[-1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, -2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        所以 (q_target - q_eval) 就变成了:</span></span><br><span class="line"><span class="string">        [[(-1)-(1), 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, (-2)-(6)]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.</span></span><br><span class="line"><span class="string">        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line"><span class="string">        我们只反向传递之前选择的 action 的值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传递会神经网络</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐步增大选择网络预测aciton的概率</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.cost_his.append(loss) <span class="comment"># 记录历史的cost值</span></span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="优先经验回放">优先经验回放</h3>
<p>前面的算法在抽样时平等的看待每条transition，但是真实环境中每条transition的重要性是不同的，利用带权重的抽样来代替均匀抽样体现不同transition的重要性</p>
<p>Option 1: Sampling probability <span class="math inline">\(p_t
\propto\left|\delta_t\right|+\epsilon\)</span>.</p>
<p>Option 2: Sampling probabilit <span class="math inline">\(p_t \propto
\frac{1}{\operatorname{rank}(t)}\)</span>.</p>
<ul>
<li>The transitions are sorted so that <span class="math inline">\(\left|\delta_t\right|\)</span> is in the
descending order.</li>
<li><span class="math inline">\(\operatorname{rank}(t)\)</span> is the
rank of the <span class="math inline">\(t\)</span>-th transition.</li>
</ul>
<p>TD Error <span class="math inline">\(|\delta_t|\)</span>越大表示transition被抽到的概率也就越大</p>
<p>不同的transition有不同的抽样概率，这样会导致DQN的预测有偏差，需要相应的调整学习率，较大的抽样概率应该将learning-rate降低。</p>
<p>Scale the learning rate by <span class="math inline">\(\left(n
p_t\right)^{-\beta}\)</span> where <span class="math inline">\(\beta
\in(0,1)\)</span></p>
<p>需要记录每条transition的TD Error <span class="math inline">\(\delta_t\)</span>，如果一条transition刚收集到并不知道<span class="math inline">\(\delta_t\)</span>,则直接设置为最大值（最高的权重）
<span class="math display">\[
\begin{aligned}
&amp;\begin{array}{ccc}
\text { Transitions }
&amp;\text {Sampling Probabilities }
&amp;\text {Learning Rates } \\
\ldots &amp; \ldots &amp; \ldots \\\left(s_t, a_t, r_t, s_{t+1}\right),
\delta_t &amp; p_t \propto\left|\delta_t\right|+\epsilon &amp; \alpha
\cdot\left(n p_t\right)^{-\beta} \\
\left(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2}\right), \delta_{t+1} &amp;
p_{t+1} \propto\left|\delta_{t+1}\right|+\epsilon &amp; \alpha
\cdot\left(n p_{t+1}\right)^{-\beta} \\
\left(s_{t+2}, a_{t+2}, r_{t+2}, s_{t+3}\right), \delta_{t+2} &amp;
p_{t+2} \propto\left|\delta_{t+2}\right|+\epsilon &amp; \alpha
\cdot\left(n p_{t+2}\right)^{-\beta} \\
\ldots &amp; \ldots &amp; \ldots
\end{array}
\end{aligned}
\]</span> 越高的TD Error <span class="math inline">\(|\delta_t|\)</span>
，更高的概率（权重）被选择，更小的learning-rate</p>
<h2 id="高估问题">高估问题</h2>
<p>TD
Learning会导致DQN高估动作价值（action-value），由于以下两个原因</p>
<ul>
<li>计算TD Target时，<span class="math inline">\(y_t=r_t+\gamma \cdot
\max _a Q\left(s_{t+1}, a;w\right) .\)</span>用到了最大化操作，导致TD
Target大于真实值</li>
<li>Bootstrapping，计算TD Target时，利用了t+1时刻的网络估计</li>
</ul>
<p>两个原因导致高估问题越来越严重（恶性循环）</p>
<p>非均匀的高估会导致问题，例如某一时刻： <span class="math display">\[
\begin{aligned}
&amp; Q^{\star}\left(s, a^1\right)=200, Q^{\star}\left(s,
a^2\right)=100, and\ Q^{\star}\left(s, a^3\right)=230 \\
&amp; Q\left(s, a^1 ; \mathbf{w}\right)=280, Q\left(s, a^2 ;
\mathbf{w}\right)=300, Q\left(s, a^3 ; \mathbf{w}\right)=240 .
\end{aligned}
\]</span></p>
<p>Then <span class="math inline">\(a^2\)</span> (which is bad) will be
selected.</p>
<h3 id="target-network">Target Network</h3>
<p><strong>核心思想：</strong>利用一个Target Network计算TD</p>
<p>Target network: <span class="math inline">\(Q\left(s, a ;
\mathbf{w}^{-}\right)\)</span></p>
<ul>
<li>与 DQN, <span class="math inline">\(Q(s, a ; \mathbf{w})\)</span>
具有相同的网络结构.</li>
<li>但网络参数不同 <span class="math inline">\(\mathbf{w}^{-} \neq
\mathbf{w}\)</span>.</li>
</ul>
<p>利用 <span class="math inline">\(Q(s, a ; \mathbf{w})\)</span>
控制agent做运动，并收集transition: <span class="math display">\[
\left\{\left(s_t, a_t, r_t, s_{t+1}\right)\right\} .
\]</span></p>
<p>利用Target Network <span class="math inline">\(Q\left(s, a ;
\mathbf{w}^{-}\right)\)</span> 计算TD target: <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q\left(s_{t+1}, a ; \mathbf{w}^{-}\right) .
\]</span></p>
<p>Target Network参数更新（每隔一段时间）：</p>
<ul>
<li>Option 1: <span class="math inline">\(\mathbf{w}^{-} \leftarrow
\mathbf{w}\)</span>.（直接赋值）</li>
<li>Option 2: <span class="math inline">\(\mathbf{w}^{-} \leftarrow \tau
\cdot \mathbf{w}+(1-\tau) \cdot
\mathbf{w}^{-}\)</span>.（做加权平均）</li>
</ul>
<h3 id="double-dqn">Double DQN</h3>
<p><strong>核心思想：</strong>利用Double
DQN缓解因为maximization导致的高估</p>
<p>原始方法计算TD Target <span class="math display">\[
y_t=r_t+\gamma \cdot \max _a Q\left(s_{t+1}, a ; \mathbf{w}\right) .
\]</span> 计算TD Target的过程可以分解为两步</p>
<ol type="1">
<li><p>选择action: <span class="math inline">\(a^{\star}=\underset{a}{\operatorname{argmax}}
Q\left(s_{t+1}, a ; \mathbf{w}\right) .\)</span></p></li>
<li><p>计算TD Target: <span class="math inline">\(y_t=r_t+\gamma \cdot
Q\left(s_{t+1}, a^{\star} ; \mathbf{w}\right) .\)</span></p></li>
</ol>
<p>原始方法中两步均使用DQN，Target
Network方法中两步均使用target-net。</p>
<p>Double DQN 第一步使用DQN选择aciton，第二步使用target-net计算TD
Target</p>
<p>Selection using DQN: <span class="math display">\[
a^{\star}=\underset{a}{\operatorname{argmax}} Q\left(s_{t+1}, a ;
\mathbf{w}\right) .
\]</span></p>
<p>Evaluation using target network: <span class="math display">\[
y_t=r_t+\gamma \cdot Q\left(s_{t+1}, a^{\star} ; \mathbf{w}^{-}\right) .
\]</span> 通过Double
DQN的方法大幅提高性能，减少maximization带来的高估问题（但没有彻底根除）</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Selection</th>
<th style="text-align: center;">Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Naive Update</td>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">DQN</td>
</tr>
<tr>
<td style="text-align: center;">Using Target</td>
<td style="text-align: center;">Target Network</td>
<td style="text-align: center;">Target Network</td>
</tr>
<tr>
<td style="text-align: center;">Double DQN</td>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">Target Network</td>
</tr>
</tbody>
</table>
<h2 id="dueling-network">Dueling Network</h2>
<h3 id="advantage-function">Advantage Function</h3>
<p>Discounted return: <span class="math inline">\(U_t=R_t+\gamma \cdot
R_{t+1}+\gamma^2 \cdot R_{t+2}+\gamma^3 \cdot
R_{t+3}+\cdots\)</span></p>
<p>Action-value function: <span class="math inline">\(Q_\pi\left(s_t,
a_t\right)=\mathbb{E}\left[U_t \mid S_t=s_t, A_t=a_t\right] \text {.
}\)</span></p>
<p>State-value function: $ V_(s_t)=_A$</p>
<p>Optimal action-value function: <span class="math inline">\(Q^{\star}(s, a)=\max _\pi Q_\pi(s, a) \text {.
}\)</span></p>
<p>Optimal state-value function: <span class="math inline">\(V^{\star}(s)=\max _\pi V_\pi(s) \text {.
}\)</span></p>
<p><strong>Definition: Optimal advantage function.</strong>（优势函数）
<span class="math display">\[
A^{\star}(s, a)=Q^{\star}(s, a)-V^{\star}(s) \text {. }
\]</span> <span class="math inline">\(V^{\star}(s)\)</span>评价状态s的好坏，<span class="math inline">\(Q^{\star}(s,
a)\)</span>评价在状态s的情况下做动作a的好坏</p>
<p><span class="math inline">\(A^{\star}(s,
a)\)</span>表示动作a相对于baseline的优势，a越好，优势越大</p>
<p>经过数学推导得到以下两个等式</p>
<ul>
<li>Equation 1: <span class="math inline">\(Q^{\star}(s,
a)=V^{\star}(s)+A^{\star}(s, a)\)</span>.</li>
<li>Equation 2: <span class="math inline">\(Q^{\star}(s,
a)=V^{\star}(s)+A^{\star}(s, a)-\max _a A^{\star}(s, a)\)</span>.</li>
</ul>
<p><span class="math inline">\(\max _a A^{\star}(s, a)\)</span>
恒等于0，在训练中能保持神经网络的稳定，避免<span class="math inline">\(V^{\star}(s;w),A^{\star}(s,
a;w)\)</span>两个神经网络的输出随意上下波动</p>
<p>对于Equation 2 <span class="math display">\[
Q^{\star}(s, a)=V^{\star}(s)+A^{\star}(s, a)-\max _a A^{\star}(s, a)
\]</span></p>
<ul>
<li>利用神经网络<span class="math inline">\(V\left(s ;
\mathbf{w}^V\right)\)</span>近似<span class="math inline">\(V^{\star}(s)\)</span><br>
</li>
<li>利用神经网络<span class="math inline">\(A\left(s, a ;
\mathbf{w}^A\right)\)</span>近似<span class="math inline">\(A^{\star}(s,
a)\)</span></li>
</ul>
<p>因此 <span class="math inline">\(Q^{\star}(s, a)\)</span>
可以利用dueling network表示为： <span class="math display">\[
Q(s, a ; \mathbf{w}^A, \mathbf{w}^V)=V(s ; \mathbf{w}^V)+A(s, a ;
\mathbf{w}^A)-\max _a A(s, a ; \mathbf{w}^A) ..
\]</span> <img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205115127738.png" alt="image-20231205115127738"></p>
<p>红色对应<span class="math inline">\(A(s, a ; \mathbf{w}^A)\)</span>，
蓝色对应<span class="math inline">\(V(s ;
\mathbf{w}^V)\)</span>，将蓝色实数与上面向量每个元素相加，再减去红色向量中的最大元素得到紫色向量（最终输出）</p>
<p>Dueling Network的输入输出、功能、训练方式(TD
Learning)、优化方法(Experience Replay, Target Network,double
DQN)与DQN完全一样</p>
<p>Dueling
Network改变了网络结构，提高了训练效果，此外在实际训练中max可以取mean，有更好的效果
<span class="math display">\[
Q(s, a ; \mathbf{w})=V(s ; \mathbf{w}^V)+A(s, a ;
\mathbf{w}^A)-\operatorname{mean}_a A(s, a ; \mathbf{w}^A) .
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>价值学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【RL】Actor&amp;Critic</title>
    <url>/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<h1 id="rlactorcritic">【RL】Actor&amp;Critic</h1>
<h2 id="结合价值与策略学习">结合价值与策略学习</h2>
<p>同时对最优Q值以及最优策略进行学习，最终策略网络的输出即为最优动作。</p>
<ul>
<li><strong>Actor网络</strong>（策略网络）<span class="math inline">\(\pi(a|s;\theta)\)</span>，近似<span class="math inline">\(Policy\ \pi\)</span>控制Agent做动作，利用Policy
Gradient训练网络</li>
<li><strong>Critic网络</strong>（价值网络）<span class="math inline">\(Q_\pi(a,s;w)\)</span>对策略进行评价。通过TD
Learning训练网络</li>
</ul>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205150114928.png" alt="image-20231205150114928">
<figcaption aria-hidden="true">image-20231205150114928</figcaption>
</figure>
<p><strong>特点:</strong></p>
<ol type="1">
<li>策略梯度方法和价值估计方法的结合；</li>
<li>直接的得到最优动作；</li>
<li>动作空间既可以是离散的，也可以是连续的。</li>
</ol>
<h2 id="actorcritic">Actor&amp;Critic</h2>
<p>State-value function: <span class="math inline">\(V_\pi(s)=\sum_a
\pi(a \mid s) \cdot Q_\pi(s, a) \approx \sum_a \pi(a \mid s ;
\boldsymbol{\theta}) \cdot q(s, a ; \mathbf{w}) .\)</span></p>
<p><span class="math inline">\(\pi(a|s)\)</span>
是策略函数用于计算动作的概率值，控制agent做动作。利用策略网络<span class="math inline">\(\pi(a|s;\theta)\)</span> 近似<span class="math inline">\(\pi(a|s)\)</span></p>
<p><span class="math inline">\(Q_\pi(s,a)\)</span>
是动作价值函数，用于评价动作的好坏。利用价值网络<span class="math inline">\(q(s,a;w)\)</span>近似动作价值函数<span class="math inline">\(Q_\pi(s,a)\)</span></p>
<p>利用两个网络近似策略网络、动作价值函数，可以分别看作运动员（actor）和裁判（critic）</p>
<h3 id="actor策略网络">Actor策略网络</h3>
<p>输入为State，输出为每个Action的概率组成的向量</p>
<p>由于所有Action的概率之和为1 <span class="math inline">\(\sum_{a \in
\mathcal{A}} \pi(a \mid s, \boldsymbol{\theta})=1
.\)</span>，所以利用softmaxt激活函数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205143909771.png" alt="image-20231205143909771">
<figcaption aria-hidden="true">image-20231205143909771</figcaption>
</figure>
<p>训练策略网络，在价值网络的监督下，提高State-Value <span class="math inline">\(V(s;\theta,w)\)</span>，随着训练actor的表现会越来越好</p>
<p>利用 <strong>Policy Gradient</strong>
更新策略网络，让运动员做出的动作打分更高</p>
<p>计算Policy Gradient: <span class="math display">\[
\frac{\partial V(s ; \boldsymbol{\theta})}{\partial
\boldsymbol{\theta}}=\mathbb{E}_A\left[\frac{\partial \log \pi(A \mid s,
\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \cdot q(s, A ;
\mathbf{w})\right]
\]</span> l利用梯度上升更新策略网络</p>
<h3 id="critic价值网络">Critic价值网络</h3>
<p>输入为state s以及action a，输出为预估的动作价值<span class="math inline">\(q(s,a;w)\)</span></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205144107539.png" alt="image-20231205144107539">
<figcaption aria-hidden="true">image-20231205144107539</figcaption>
</figure>
<p>学习价值网络是为了让裁判打分更加精准，利用reward让<span class="math inline">\(q(s,a;w)\)</span>的预估值更加准确</p>
<p>利用 <strong>TD Learning</strong> 更新价值网络，让裁判打分更准</p>
<p>Predicted action-value: <span class="math inline">\(q_t=q\left(s_t,
a_t ; \mathbf{w}\right)\)</span>.</p>
<p>TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot
q\left(s_{t+1}, a_{t+1} ; \mathbf{w}\right)\)</span></p>
<p>Gradient: <span class="math display">\[
\frac{\partial\left(q_t-y_t\right)^2 / 2}{\partial
\mathbf{w}}=\left(q_t-y_t\right) \cdot \frac{\partial q\left(s_t, a_t ;
\mathbf{w}\right)}{\partial \mathbf{w}}
\]</span> 利用梯度下降更新网络</p>
<ul>
<li>训练两个网络的目的是使运动员作出的动作打分越来越高，裁判的打分越来越精准</li>
<li>两个网络之间可以共享参数也可不共享参数</li>
</ul>
<p><strong>Actor&amp;Critic算法</strong></p>
<ol type="1">
<li>Observe state <span class="math inline">\(s_t\)</span> and randomly
sample <span class="math inline">\(a_t \sim \pi\left(\cdot \mid s_t ;
\boldsymbol{\theta}_t\right)\)</span>.</li>
<li>Perform <span class="math inline">\(a_t\)</span>; then environment
gives new state <span class="math inline">\(s_{t+1}\)</span> and reward
<span class="math inline">\(r_t\)</span>.</li>
<li>Randomly sample <span class="math inline">\(\tilde{a}_{t+1} \sim
\pi\left(\cdot \mid s_{t+1} ; \boldsymbol{\theta}_t\right)\)</span>. (Do
not perform <span class="math inline">\(\tilde{a}_{t+1} !\)</span>
)</li>
<li>Evaluate value network: <span class="math inline">\(q_t=q\left(s_t,
a_t ; \mathbf{w}_t\right)\)</span> and <span class="math inline">\(q_{t+1}=q\left(s_{t+1}, \tilde{a}_{t+1} ;
\mathbf{w}_t\right)\)</span>.</li>
<li>Compute TD error: <span class="math inline">\(\delta_t=q_t-\left(r_t+\gamma \cdot
q_{t+1}\right)\)</span>.</li>
<li>Differentiate value network: <span class="math inline">\(\mathbf{d}_{w, t}=\left.\frac{\partial q\left(s_t,
a_t ; \mathbf{w}\right)}{\partial
\mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}_t}\)</span>.</li>
<li>Update value network: <span class="math inline">\(\mathbf{w}_{t+1}=\mathbf{w}_t-\alpha \cdot
\delta_t \cdot \mathbf{d}_{w, t}\)</span>.</li>
<li>Differentiate policy network: <span class="math inline">\(\mathbf{d}_{\theta, t}=\left.\frac{\partial \log
\pi\left(a_t \mid s_t, \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_t}\)</span>.</li>
<li>Update policy network: <span class="math inline">\(\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\beta
\cdot q_t \cdot \mathbf{d}_{\theta, t}\)</span> or <span class="math inline">\(\theta_{t+1}=\theta_t+\beta\cdot\delta_t\cdot
d_{\theta,t}\)</span></li>
</ol>
<h2 id="advantage-actor-critica2c">Advantage Actor-Critic(A2C)</h2>
<p>价值网络Value Network与上述有所不同，这里采用<span class="math inline">\(v(s;w)\)</span>来近似state-value function<span class="math inline">\(V_\pi(s)\)</span></p>
<p>状态价值只依赖于state，不依赖于action，更好训练</p>
<p>建立网络</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205151748629.png" alt="image-20231205151748629">
<figcaption aria-hidden="true">image-20231205151748629</figcaption>
</figure>
<h3 id="数学原理">数学原理</h3>
<p>简述A2C算法原理，以下是对策略梯度的近似，用于更新策略网络 <span class="math display">\[
\mathbf{g}\left(a_t\right) \approx \frac{\partial \ln \pi\left(a_t \mid
s_t ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}
\cdot\left(r_t+\gamma \cdot v\left(s_{t+1} ;
\mathbf{w}\right)-v\left(s_t ; \mathbf{w}\right)\right)
\]</span></p>
<p><span class="math inline">\(r_t+\gamma \cdot v\left(s_{t+1} ;
\mathbf{w}\right)-v\left(s_t ;
\mathbf{w}\right)\)</span>是价值网络做出的判断，可以评价<span class="math inline">\(a_t\)</span>的好坏，指导策略网络做改进</p>
<p>但这一项没有<span class="math inline">\(a_t\)</span>，如何评价<span class="math inline">\(a_t\)</span>的好坏？</p>
<ul>
<li><span class="math inline">\(v(s_t ;
\mathbf{w})\)</span>是价值网络对t时刻对<span class="math inline">\(s_t\)</span>的评价，与<span class="math inline">\(a_t\)</span>无关</li>
<li><span class="math inline">\(r_t+\gamma \cdot v\left(s_{t+1} ;
\mathbf{w}\right)\)</span>近似<span class="math inline">\(\mathbb{E}[U_t|s_t,s_{t+1}]\)</span>，是对t+1时刻<span class="math inline">\(s_{t+1}\)</span>价值的预测</li>
<li>在t+1时刻，<span class="math inline">\(a_t\)</span>已发生，<span class="math inline">\(\mathbb{E}[U_t|s_t,s_{t+1}]\)</span>依赖于<span class="math inline">\(a_t\)</span>，因此两者的差值Advantage可以反应<span class="math inline">\(a_t\)</span>带来的优势</li>
</ul>
<h3 id="算法流程">算法流程</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205154012287.png" alt="image-20231205154012287">
<figcaption aria-hidden="true">image-20231205154012287</figcaption>
</figure>
<ol type="1">
<li><p><strong>Observe a transition <span class="math inline">\(\left(s_t, a_t, r_t,
s_{t+1}\right)\)</span>.</strong></p></li>
<li><p><strong>TD target: <span class="math inline">\(y_t=r_t+\gamma
\cdot v\left(s_{t+1} ; \mathbf{w}\right)\)</span>.</strong></p></li>
<li><p><strong>TD error: <span class="math inline">\(\delta_t=v\left(s_t
; \mathbf{w}\right)-y_t\)</span>.</strong></p></li>
<li><p><strong>Update the policy network (actor) by:</strong></p></li>
</ol>
<p><span class="math display">\[
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\beta \cdot \delta_t
\cdot \frac{\partial \ln \pi\left(a_t \mid s_t ;
\boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}} .
\]</span></p>
<ol start="5" type="1">
<li><strong>Update the value network (critic) by:</strong></li>
</ol>
<p><span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w}-\alpha \cdot \delta_t \cdot
\frac{\partial v\left(s_t ; \mathbf{w}\right)}{\partial \mathbf{w}} .
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShareLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_input, n_hidden</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ShareLayer, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(n_input, n_hidden)</span><br><span class="line">        nn.init.normal_(self.l1.weight, mean=<span class="number">0</span>, std=<span class="number">0.1</span>)</span><br><span class="line">        nn.init.constant_(self.l1.bias, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, out</span>):</span></span><br><span class="line">        out = self.l1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, output_n, share_layer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.share_layer = share_layer</span><br><span class="line"></span><br><span class="line">        self.l2 = nn.Linear(share_layer.l1.out_features, output_n)</span><br><span class="line">        nn.init.normal_(self.l2.weight, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.constant_(self.l2.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = torch.FloatTensor([x])</span><br><span class="line"></span><br><span class="line">        out = self.share_layer(out)</span><br><span class="line">        out = self.l2(out)</span><br><span class="line">        prob = F.softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> prob, out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, share_layer</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.share_layer = share_layer</span><br><span class="line"></span><br><span class="line">        self.l2 = nn.Linear(share_layer.l1.out_features, <span class="number">1</span>)</span><br><span class="line">        nn.init.normal_(self.l2.weight, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.constant_(self.l2.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = torch.from_numpy(x).<span class="built_in">float</span>()</span><br><span class="line">        out = self.share_layer(out)</span><br><span class="line">        out = self.l2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">prob</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.random.choice(prob.shape[<span class="number">1</span>], p=prob[<span class="number">0</span>].detach().numpy())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>(<span class="params">optim, logits, a, delta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Policy Gradient 更新策略网络</span></span><br><span class="line"><span class="string">    :param optim: 优化器</span></span><br><span class="line"><span class="string">    :param logits: Action网络输出（无softmax）</span></span><br><span class="line"><span class="string">    :param a: a_t</span></span><br><span class="line"><span class="string">    :param delta: TD Error</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    a = torch.tensor([a]).long()</span><br><span class="line">    lnp_a = F.cross_entropy(logits, a)  <span class="comment"># softmax(logits)+ log + nllloss</span></span><br><span class="line">    loss = lnp_a * delta</span><br><span class="line"></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>(<span class="params">optim, critic, s, r, s_, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    TD Learning 训练策略网络</span></span><br><span class="line"><span class="string">    :param optim: critic优化器</span></span><br><span class="line"><span class="string">    :param critic:</span></span><br><span class="line"><span class="string">    :param s: s_t</span></span><br><span class="line"><span class="string">    :param r: r_t</span></span><br><span class="line"><span class="string">    :param s_: s_t+1</span></span><br><span class="line"><span class="string">    :param gamma: 回报折扣率</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    v_s = critic(s)</span><br><span class="line">    v_s_ = critic(s_)</span><br><span class="line"></span><br><span class="line">    td_target = r + gamma * v_s_.item()</span><br><span class="line">    td_error = td_target - v_s</span><br><span class="line">    loss = td_error ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optim.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> td_error.item()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="对比reinforce算法">对比REINFORCE算法</h3>
<p>两者网络结构一模一样，但价值网络的用途有所不同</p>
<p><strong>Advantage Actor-Critic（A2C）算法：</strong></p>
<ol type="1">
<li>Observing a trajectory from time <span class="math inline">\(t\)</span> to <span class="math inline">\(t+m-1\)</span>.</li>
<li>TD target: <span class="math inline">\(y_t=\sum_{i=0}^{m-1} \gamma^i
\cdot r_{t+i}+\gamma^m \cdot v\left(s_{t+m} ;
\mathbf{w}\right)\)</span>.
(部分基于真实观测，部分基于价值网络的估计)</li>
<li>TD error: <span class="math inline">\(\delta_t=v\left(s_t ;
\mathbf{w}\right)-y_t\)</span>.</li>
<li>Update the policy network (actor) by: <span class="math inline">\(\boldsymbol{\theta} \leftarrow
\boldsymbol{\theta}-\beta \cdot \delta_t \cdot \frac{\partial \ln
\pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} .\)</span></li>
<li>Update the value network (critic) by: <span class="math inline">\(\mathbf{w} \leftarrow \mathbf{w}-\alpha \cdot
\delta_t \cdot \frac{\partial v\left(s_t ; \mathbf{w}\right)}{\partial
\mathbf{w}}\)</span></li>
</ol>
<p><strong>REINFORCE算法</strong>：</p>
<ol type="1">
<li>Observing a trajectory from time <span class="math inline">\(t\)</span> to <span class="math inline">\(n\)</span>.</li>
<li>Return: <span class="math inline">\(u_t=\sum_{i=t}^n \gamma^{i-t}
\cdot r_i\)</span>. (使用回报<span class="math inline">\(u_t\)</span>,
完全基于真实观测到的奖励)</li>
<li>Error: <span class="math inline">\(\delta_t=v\left(s_t ;
\mathbf{w}\right)-u_t\)</span>.</li>
<li>Update the policy network by: <span class="math inline">\(\boldsymbol{\theta} \leftarrow
\boldsymbol{\theta}-\beta \cdot \delta_t \cdot \frac{\partial \ln
\pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)}{\partial
\boldsymbol{\theta}} .\)</span></li>
<li>Update the value network by: <span class="math inline">\(\mathbf{w}
\leftarrow \mathbf{w}-\alpha \cdot \delta_t \cdot \frac{\partial
v\left(s_t ; \mathbf{w}\right)}{\partial \mathbf{w}}\)</span></li>
</ol>
<h3 id="总结">总结</h3>
<ul>
<li><p>A2C with one-step TD target: <span class="math inline">\(y_t=r_t+\gamma \cdot v\left(s_{t+1} ;
\mathbf{w}\right)\)</span>. (Use only one reward <span class="math inline">\((m=1)\)</span>)</p></li>
<li><p>A2C with <span class="math inline">\(m\)</span>-step TD target:
<span class="math inline">\(y_t=\sum_{i=0}^{m-1} \gamma^i \cdot
r_{t+i}+\gamma^m \cdot v\left(s_{t+m} ;
\mathbf{w}\right)\)</span>.</p></li>
<li><p>REINFORCE: <span class="math inline">\(y_t\)</span> becomes <span class="math inline">\(u_t=\sum_{i=t}^n \gamma^{i-t} \cdot r_i\)</span>.
(Use all the rewards)</p></li>
</ul>
<p>A2C
使用了bootstrap，如果使用所有的rewards，不用价值网络自己的估计（bootstrap），则变为REINFORCE算法</p>
<h2 id="asynchronous-advantage-actor-critic-a3c">Asynchronous Advantage
Actor-Critic （A3C）</h2>
<p>在A2C的基础上使用异步训练的方式提高训练效率</p>
<p>将多个Actor放到不同的CPU上，并行运算，让多个拥有副结构的Agent同时在这些并行环境上更新主结构中的参数。</p>
<p>并行中的Agent们互不干扰，而主结构的参数更新收到副结构提交更新的不连续性干扰，所以更新的相关性被降低，收敛性提高。</p>
<p>利用python的Tread实现</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731191535286.png" alt="image-20240731191535286">
<figcaption aria-hidden="true">image-20240731191535286</figcaption>
</figure>
<h2 id="deep-deterministic-policy-gradientddpg">Deep Deterministic
Policy Gradient（DDPG）</h2>
<p>能够在连续动作上更有效地学习</p>
<ul>
<li>利用一个确定性的策略网络（Actor）: <span class="math inline">\(a=\pi(s;\theta)\)</span></li>
<li>一个价值网络（Critic）: <span class="math inline">\(q(s,a;w)\)</span></li>
<li>价值网络的输出评价了Actor的动作好坏</li>
</ul>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231205165132120.png" alt="image-20231205165132120">
<figcaption aria-hidden="true">image-20231205165132120</figcaption>
</figure>
<p>利用TD Learning的方法训练价值网络<span class="math inline">\((TD\
error:\delta_t=q_t-\left(r_t+\gamma \cdot
q_{t+1}\right))\)</span>，和前面的相同</p>
<p>训练策略网络的核心是使得做出动作<span class="math inline">\(a=\pi(s;\theta)\)</span>的<span class="math inline">\(q(s,a;w)\)</span>的值越大越好</p>
<p>因此可以<strong>利用<span class="math inline">\(q(s,a;w)\)</span>对<span class="math inline">\(\theta\)</span>求导，利用链式法则</strong>，求得DPG梯度：
<span class="math display">\[
\mathbf{g}=\frac{\partial q(s, \pi(s ; \boldsymbol{\theta}) ;
\mathbf{w})}{\partial \boldsymbol{\theta}}=\frac{\partial a}{\partial
\boldsymbol{\theta}} \cdot \frac{\partial q(s, a ; \mathbf{w})}{\partial
a} .
\]</span> 然后利用梯度上升更新 <span class="math inline">\(\boldsymbol{\theta} \leftarrow
\boldsymbol{\theta}+\beta \cdot \mathbf{g} .\)</span></p>
<p><strong>利用Target Network对DDPG算法进行优化</strong></p>
<ul>
<li>Policy network makes a decision: <span class="math inline">\(a=\pi(s
; \boldsymbol{\theta})\)</span>.</li>
<li>Update policy network by DPG: <span class="math inline">\(\quad
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\beta \cdot
\frac{\partial a}{\partial \boldsymbol{\theta}} \cdot \frac{\partial
q(s, a ; \boldsymbol{w})}{\partial a}\)</span>.</li>
<li>Value network computes <span class="math inline">\(q_t=q(s, a ;
\mathbf{w})\)</span>.</li>
<li>Target networks, <span class="math inline">\(\pi\left(s ;
\boldsymbol{\theta}^{-}\right)\)</span>and <span class="math inline">\(q\left(s, a ; \mathbf{w}^{-}\right)\)</span>,
compute <span class="math inline">\(q_{t+1}\)</span>.</li>
<li>TD error: <span class="math inline">\(\delta_t=q_t-\left(r_t+\gamma
\cdot q_{t+1}\right)\)</span>.</li>
<li>Update value network by TD: <span class="math inline">\(\mathbf{w}
\leftarrow \mathbf{w}-\alpha \cdot \delta_t \cdot \frac{\partial q(s, a
; \mathbf{w})}{\partial \mathbf{w}}\)</span></li>
</ul>
<p><strong>Target Network的参数更新</strong></p>
<ul>
<li>Set a hyper-parameter <span class="math inline">\(\tau
\in(0,1)\)</span>.</li>
<li>Update the target networks by weighted averaging:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; \mathbf{w}^{-} \leftarrow \tau \cdot \mathbf{w}+(1-\tau) \cdot
\mathbf{w}^{-} . \\
&amp; {\boldsymbol{\theta}^{-}} \leftarrow \tau \cdot
\boldsymbol{\theta}+(1-\tau) \cdot \boldsymbol{\theta}^{-} .
\end{aligned}
\]</span></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731191619144.png" alt="image-20240731191619144">
<figcaption aria-hidden="true">image-20240731191619144</figcaption>
</figure>
<h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization
（PPO）</h2>
<p>用于解决 PG 算法中学习率不好确定的问题</p>
<ul>
<li>如果学习率过大，训练的策略不易收敛</li>
<li>如果学习率太小，则会花费较长的时间。</li>
</ul>
<p>PPO 算法利用新策略和旧策略的比例，从而限制了新策略的更新幅度，让 PG
算法对于稍微大一点的学习率不那么敏感。</p>
<p>为了判定模型的更新什么时候停止，所以 PPO 在原目标函数的基础上添加了
KL
散度部分，用来表示两个分布之间的差别，差别越大值越大，惩罚也就越大。</p>
<p>所以可以使两个分布尽可能的相似。PPO 算法的损失函数如下 <span class="math display">\[
\mathrm{J}_{\mathrm{PPO}}^{\theta^{\prime}}(\theta)=\mathrm{J}^{\theta^{\prime}}(\theta)-\beta
\mathrm{KL}\left(\theta, \theta^{\prime}\right)
\]</span></p>
<p><span class="math display">\[
\mathrm{J}^{\theta^{\prime}}(\theta)=\mathrm{E}_{\left(\mathrm{s}_{\mathrm{t}},
\mathrm{a}_{\mathrm{t}}\right) \sim
\pi_{\theta^{\prime}}}\left[\frac{\mathrm{p}_\theta\left(\mathrm{a}_{\mathrm{t}}
\mid
\mathrm{s}_{\mathrm{t}}\right)}{\mathrm{p}_{\theta^{\prime}}\left(\mathrm{a}_{\mathrm{t}}
\mid \mathrm{s}_{\mathrm{t}}\right)}
\mathrm{A}^{\theta^{\prime}}\left(\mathrm{s}_{\mathrm{t}},
\mathrm{a}_{\mathrm{t}}\right)\right]
\]</span></p>
<p>PPO 在训练时可以采用适应性的 KL 惩罚因子.</p>
<ul>
<li>当 KL 过大时，增大 <span class="math inline">\(\beta\)</span>的值来增加惩罚力度. if <span class="math inline">\(\mathrm{KL}\left(\theta,
\theta^{\prime}\right)&gt;\mathrm{KL}_{\max }\)</span> increase <span class="math inline">\(\beta\)</span></li>
<li>当 kL 过小时，减小 <span class="math inline">\(\beta\)</span>值来降低惩罚力度. if <span class="math inline">\(\mathrm{KL}\left(\theta,
\theta^{\prime}\right)&lt;\mathrm{KL}_{\min }\)</span>, decrease <span class="math inline">\(\beta\)</span></li>
</ul>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240731191656327.png" alt="image-20240731191656327" style="zoom: 80%;"></p>
<h2 id="distributed-proximal-policy-optimizationdppo">Distributed
Proximal Policy Optimization（DPPO）</h2>
<p>相当于多线程并行版的 PPO。</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>actor&amp;critic</tag>
      </tags>
  </entry>
  <entry>
    <title>STL技巧</title>
    <url>/2024/07/30/STL/</url>
    <content><![CDATA[<h1 id="stl技巧">STL技巧</h1>
<h2 id="vector数组">vector数组</h2>
<p>变长数组，倍增的思想</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()  返回元素个数</span><br><span class="line"><span class="built_in">empty</span>()  返回是否为空</span><br><span class="line"><span class="built_in">clear</span>()  清空</span><br><span class="line"><span class="built_in">front</span>()/<span class="built_in">back</span>()</span><br><span class="line"><span class="built_in">push_back</span>()/<span class="built_in">pop_back</span>()</span><br><span class="line"><span class="built_in">begin</span>()/<span class="built_in">end</span>()</span><br><span class="line">[]</span><br><span class="line">\\ 支持比较运算，按字典序</span><br></pre></td></tr></table></figure>
<h2 id="pairint-int二元组">pair&lt;int, int&gt;二元组</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">first, 第一个元素</span><br><span class="line">second, 第二个元素</span><br><span class="line">支持比较运算，以first为第一关键字，以second为第二关键字（字典序）</span><br></pre></td></tr></table></figure>
<h2 id="string字符串">string字符串</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()/<span class="built_in">length</span>()  返回字符串长度</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">clear</span>()</span><br><span class="line"><span class="built_in">substr</span>(起始下标，(子串长度))  返回子串</span><br><span class="line"><span class="built_in">c_str</span>()  返回字符串所在字符数组的起始地址</span><br></pre></td></tr></table></figure>
<h2 id="queue队列">queue队列</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">push</span>()  向队尾插入一个元素</span><br><span class="line"><span class="built_in">front</span>()  返回队头元素</span><br><span class="line"><span class="built_in">back</span>()  返回队尾元素</span><br><span class="line"><span class="built_in">pop</span>()  弹出队头元素</span><br></pre></td></tr></table></figure>
<h2 id="priority_queue优先队列">priority_queue优先队列</h2>
<p>默认是大根堆 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">push</span>()  \\ 插入一个元素</span><br><span class="line"><span class="built_in">top</span>()  返回堆顶元素</span><br><span class="line"><span class="built_in">pop</span>()  弹出堆顶元素</span><br><span class="line">定义成小根堆的方式：priority_queue&lt;<span class="keyword">int</span>, vector&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt;&gt; q;</span><br></pre></td></tr></table></figure></p>
<h2 id="stack栈">stack栈</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">push</span>()  向栈顶插入一个元素</span><br><span class="line"><span class="built_in">top</span>()  返回栈顶元素</span><br><span class="line"><span class="built_in">pop</span>()  弹出栈顶元素</span><br></pre></td></tr></table></figure>
<h2 id="deque双端队列">deque双端队列</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">clear</span>()</span><br><span class="line"><span class="built_in">front</span>()/<span class="built_in">back</span>()</span><br><span class="line"><span class="built_in">push_back</span>()/<span class="built_in">pop_back</span>()</span><br><span class="line"><span class="built_in">push_front</span>()/<span class="built_in">pop_front</span>()</span><br><span class="line"><span class="built_in">begin</span>()/<span class="built_in">end</span>()</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>
<h2 id="set集合map字典">set集合&amp;map字典</h2>
<p>基于平衡二叉树（红黑树），动态维护有序序列 <figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">size</span>()</span><br><span class="line"><span class="built_in">empty</span>()</span><br><span class="line"><span class="built_in">clear</span>()</span><br><span class="line"><span class="built_in">begin</span>()/<span class="built_in">end</span>()</span><br><span class="line">++, -- 返回前驱和后继，时间复杂度 <span class="built_in">O</span>(logn)</span><br></pre></td></tr></table></figure></p>
<h3 id="setmultiset">set&amp;multiset</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">insert</span>()  插入一个数</span><br><span class="line"><span class="built_in">find</span>()  查找一个数</span><br><span class="line"><span class="built_in">count</span>()  返回某一个数的个数</span><br><span class="line"><span class="built_in">erase</span>()</span><br><span class="line">    (<span class="number">1</span>) 输入是一个数x，删除所有<span class="function">x   <span class="title">O</span><span class="params">(k + logn)</span></span></span><br><span class="line"><span class="function">    <span class="params">(<span class="number">2</span>)</span> 输入一个迭代器，删除这个迭代器</span></span><br><span class="line"><span class="function"><span class="title">lower_bound</span><span class="params">()</span>/<span class="title">upper_bound</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    <span class="title">lower_bound</span><span class="params">(x)</span>  返回大于等于x的最小的数的迭代器</span></span><br><span class="line"><span class="function">    <span class="title">upper_bound</span><span class="params">(x)</span>  返回大于x的最小的数的迭代器</span></span><br></pre></td></tr></table></figure>
<h3 id="mapmultimap">map&amp;multimap</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">insert</span>()  插入的数是一个<span class="function">pair</span></span><br><span class="line"><span class="function"><span class="title">erase</span><span class="params">()</span>  输入的参数是pair或者迭代器</span></span><br><span class="line"><span class="function"><span class="title">find</span><span class="params">()</span></span></span><br><span class="line"><span class="function">[]  注意multimap不支持此操作。 时间复杂度是 <span class="title">O</span><span class="params">(logn)</span></span></span><br><span class="line"><span class="function"><span class="title">lower_bound</span><span class="params">()</span>/<span class="title">upper_bound</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h2 id="unordered哈希表">unordered哈希表</h2>
<ul>
<li><p>unordered_set</p></li>
<li><p>unordered_map</p></li>
<li><p>unordered_multiset</p></li>
<li><p>unordered_multimap</p></li>
</ul>
<p>和上面类似，增删改查的时间复杂度是 O(1) 不支持
lower_bound()/upper_bound()， 迭代器的++，--</p>
<h2 id="bitset圧位">bitset圧位</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">bitset&lt;10000&gt; s;</span><br><span class="line">~, &amp;, |, ^</span><br><span class="line">&gt;&gt;, &lt;&lt;</span><br><span class="line">==, !=</span><br><span class="line">[]</span><br><span class="line"></span><br><span class="line"><span class="built_in">count</span>()  返回有多少个<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">any</span>()  判断是否至少有一个<span class="number">1</span></span><br><span class="line"><span class="built_in">none</span>()  判断是否全为<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>()  把所有位置成<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span>(k, v)  将第k位变成v</span><br><span class="line"><span class="built_in">reset</span>()  把所有位变成<span class="number">0</span></span><br><span class="line"><span class="built_in">flip</span>()  等价于~</span><br><span class="line"><span class="built_in">flip</span>(k) 把第k位取反</span><br></pre></td></tr></table></figure>
<h2 id="stringint转换">string&amp;int转换</h2>
<h3 id="string2int">string2int</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line">string num = <span class="string">&quot;123&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> x = <span class="built_in">atoi</span>(num.<span class="built_in">c_str</span>());</span><br></pre></td></tr></table></figure>
<h3 id="int2string">int2string</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">123</span>;</span><br><span class="line">string num = <span class="built_in">to_string</span>(x);</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title>UniStudent新生报道系统</title>
    <url>/2022/09/10/UniStudent%20%E6%96%B0%E7%94%9F%E6%8A%A5%E9%81%93%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="unistudent新生报道系统">UniStudent新生报道系统</h1>
<p>2022-8-19 中软国际实训项目</p>
<blockquote>
<p>github项目：https://github.com/xclovehsy/uniStudent</p>
</blockquote>
<h2 id="项目简介">1. 项目简介</h2>
<p><strong>软件名称：</strong> UnitStudent新生报道系统</p>
<p><strong>软件应用：</strong>
该系统将主要面向重庆财经学院软件学院的2022级新生以及所有老师，使整个新生报道流程信息化，让新生和老师能够实时了解到新生报道的进行情况。
新生报道系统结合新生报到时的需求，利用网络的即时性，提高新生报到工作效率，减少报道所需，为报道的新生提供即时准确的报道相关信息，同时可以对自己的性格进行简单的测评，帮助新生进一步了解自己；
在帮助新生顺利完成报道的同时也为新生报到的规范化和信息化管理打坚实的基础。</p>
<h2 id="需求分析">2. 需求分析</h2>
<p><strong>需求范围：</strong> 重庆财经学院软件学院</p>
<p><strong>系统包括的功能范围：</strong></p>
<ol type="1">
<li>学校功能</li>
<li>查看新生报道实时进展</li>
<li>查看新生相关信息（专业，性别，地区，年龄）</li>
<li>查看新生职业性格大体分布</li>
</ol>
<p><strong>新生功能：</strong></p>
<ol type="1">
<li>填写自身相关信息</li>
<li>进行职业性格测试</li>
<li>查看报道流程</li>
<li>查看报道实时进展</li>
<li>查看新生相关信息（专业，性别，地区，年龄）</li>
<li>查看学校概况</li>
</ol>
<h2 id="xxxxxxxxxx-bitset10000-s-count-返回有多少个1any-判断是否至少有一个1none-判断是否全为0set-把所有位置成1setk-v-将第k位变成vreset-把所有位变成0flip-等价于flipk-把第k位取反c">xxxxxxxxxx bitset&lt;10000&gt;
s;~, &amp;, |, ^&gt;&gt;, &lt;&lt;==, !=[]​count()  返回有多少个1​any()
 判断是否至少有一个1none()  判断是否全为0​set()  把所有位置成1set(k, v)
 将第k位变成vreset()  把所有位变成0flip()  等价于~flip(k)
把第k位取反c++</h2>
<p>本软件开发所使用的语言以及开发工具：</p>
<p>开发平台及工具；MySQL、Hbuilder X、Pycharm</p>
<p>通信协议：HTTP，TCP/IP</p>
<p>前端使用uniapp框架，后端使用python语言，fastapi进行接口的实现。</p>
<h2 id="项目预览">4. 项目预览</h2>
<h3 id="应用框架">4.1. 应用框架</h3>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021443731.png" alt="image-20220902144331571">
<figcaption aria-hidden="true">image-20220902144331571</figcaption>
</figure>
<h3 id="流程图">4.2. 流程图</h3>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021444357.png" alt="image-20220902144458239">
<figcaption aria-hidden="true">image-20220902144458239</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021446848.png" alt="image-20220902144602780">
<figcaption aria-hidden="true">image-20220902144602780</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021453406.png" alt="image-20220902145359327">
<figcaption aria-hidden="true">image-20220902145359327</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021454389.png" alt="image-20220902145410313">
<figcaption aria-hidden="true">image-20220902145410313</figcaption>
</figure>
<h3 id="项目界面">4.3. 项目界面</h3>
<h4 id="信息录入">4.3.1. 信息录入</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100813507.jpg" alt="Screenshot_20220909_212753_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212753_com.tencent.mm</figcaption>
</figure>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100822778.jpg"></p>
<h4 id="信息总览">4.3.2. 信息总览</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100812240.jpg" alt="Screenshot_20220909_212208_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212208_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100812765.jpg" alt="Screenshot_20220909_212216_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212216_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100812753.jpg" alt="Screenshot_20220909_212225_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212225_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100813592.jpg" alt="Screenshot_20220909_212230_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212230_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100813636.jpg" alt="Screenshot_20220909_212240_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212240_com.tencent.mm</figcaption>
</figure>
<h4 id="数据面板">4.3.3. 数据面板</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814029.jpg" alt="Screenshot_20220909_212823_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212823_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814845.jpg" alt="Screenshot_20220909_212828_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212828_com.tencent.mm</figcaption>
</figure>
<h4 id="新生指南">4.3.4. 新生指南</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100813181.jpg" alt="Screenshot_20220909_212342_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212342_com.tencent.mm</figcaption>
</figure>
<h4 id="性格测试">4.3.5. 性格测试</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814203.jpg" alt="Screenshot_20220909_212354_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212354_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814152.jpg" alt="Screenshot_20220909_212613_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212613_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814481.jpg" alt="Screenshot_20220909_212627_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212627_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814850.jpg" alt="Screenshot_20220909_212642_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212642_com.tencent.mm</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209100814057.jpg" alt="Screenshot_20220909_212702_com.tencent.mm">
<figcaption aria-hidden="true">Screenshot_20220909_212702_com.tencent.mm</figcaption>
</figure>
<h4 id="大数据面板">4.3.6. 大数据面板</h4>
<hr>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021514993.jpg" alt="image-20220902150435623">
<figcaption aria-hidden="true">image-20220902150435623</figcaption>
</figure>
<h2 id="程序入口">5. 程序入口</h2>
<p><strong>程序入口二维码：</strong></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209102321648.jpg" alt="-170c280b23612a5c">
<figcaption aria-hidden="true">-170c280b23612a5c</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209102321486.jpg" alt="-1bf9d8e9c0a82e3">
<figcaption aria-hidden="true">-1bf9d8e9c0a82e3</figcaption>
</figure>
<blockquote>
<p>大数据面板：http://v.yuntus.com/tcv/0590A5ac63f90BA555CE76d4686F0772</p>
</blockquote>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>uniapp</tag>
      </tags>
  </entry>
  <entry>
    <title>Git指令</title>
    <url>/2022/09/09/git%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="git指令">Git指令</h1>
<h2 id="仓库管理">仓库管理</h2>
<h3 id="初始化仓库">初始化仓库</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>
<h3 id="添加删除文件">添加/删除文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add/rm [filename]</span><br><span class="line">git add . # 添加所有文件</span><br></pre></td></tr></table></figure>
<h3 id="提交代码到本地仓库">提交代码到本地仓库</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git commit -m &#x27;commit file</span><br></pre></td></tr></table></figure>
<h3 id="查看仓库状态">查看仓库状态</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>
<h3 id="添加远程仓库url">添加远程仓库url</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add origin [repository url]</span><br></pre></td></tr></table></figure>
<h3 id="同步本地仓库代码">同步本地仓库代码</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git pull origin master</span><br></pre></td></tr></table></figure>
<h3 id="上传代码到远程仓库">上传代码到远程仓库</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一次提交</span><br><span class="line"># 将本地的master分支和远程仓库origin的master分支关联</span><br><span class="line">git push -u origin master   </span><br><span class="line"></span><br><span class="line"># 经常使用</span><br><span class="line">git push origin master</span><br><span class="line"></span><br><span class="line"># 强行提交，谨慎使用 </span><br><span class="line"># f表示force</span><br><span class="line">git push origin master -f </span><br></pre></td></tr></table></figure>
<h2 id="分支管理">分支管理</h2>
<h3 id="本地操作">本地操作</h3>
<h4 id="查看分支">查看分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch</span><br><span class="line"></span><br><span class="line"># 查看所用分支，以及其关联的远程分支</span><br><span class="line">git branch -a</span><br></pre></td></tr></table></figure>
<h4 id="创建分支">创建分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch [branch name]</span><br></pre></td></tr></table></figure>
<h4 id="切换分支">切换分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout [branch name]</span><br></pre></td></tr></table></figure>
<h4 id="分支快速创建和切换">分支快速创建和切换</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout -b [branch name]</span><br></pre></td></tr></table></figure>
<h4 id="分支合并">分支合并</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 首先需要切换到合并的主分支上去</span><br><span class="line">git merge [branch name]</span><br></pre></td></tr></table></figure>
<p><strong>遇到冲突时的分支合并</strong>
如果在两个不同的分支中，对同一个文件进行了不同的修改，Git
就没法干净的合并它们。
此时，我们需要打开这些包含冲突的文件然后手动解决冲突。</p>
<h4 id="删除分支">删除分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git branch -d [branch name]</span><br></pre></td></tr></table></figure>
<h3 id="远程操作">远程操作</h3>
<h4 id="查看远程仓库分支">查看远程仓库分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote show [repository name]</span><br></pre></td></tr></table></figure>
<h4 id="本地分支推送到远程仓库分支">本地分支推送到远程仓库分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -u 表示把本地分支和远程分支进行关联，只在第一次推送的时候需要带 -u 参数</span><br><span class="line">git push -u 远程仓库的别名 本地分支名称: 远程分支名称</span><br><span class="line"></span><br><span class="line"># 案例</span><br><span class="line">git push -u origin payment:pay</span><br><span class="line"></span><br><span class="line"># 如果希望远程分支的名称和本地分支名称保持一直，可以对命令进行简化：</span><br><span class="line">git push -u origin payment</span><br></pre></td></tr></table></figure>
<h4 id="跟踪分支">跟踪分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 从远程仓库中，把对应的远程分支下载到本地仓库，名字保持一致</span><br><span class="line">git checkout 远程分支的名称</span><br><span class="line"></span><br><span class="line"># 从远程仓库中，把对应的远程分支下载到本地仓库，并把下载的本地分支进行重命名</span><br><span class="line">git checkout -b 本地分支名称 远程仓库名称/远程分支名称</span><br><span class="line"></span><br><span class="line"># 案例  把远程仓库origin里面的pay分支下载并重命名为payment分支</span><br><span class="line">git checkout -b payment origin/pay</span><br></pre></td></tr></table></figure>
<h4 id="拉取远程分支最新代码">拉取远程分支最新代码</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure>
<h4 id="删除远程分支">删除远程分支</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push origin --delete [branch name]</span><br></pre></td></tr></table></figure>
<h2 id="代理设置">代理设置</h2>
<p>在执行<code>git clone</code>时，该方法只适用于http方式，不适用于ssh方式。</p>
<p>ssh方式推荐使用proxychains(linux)。</p>
<h3 id="socks代理">socks代理</h3>
<p>在<code>参数设置</code>-<code>Core:基础设置</code>中可以查看本地<code>socks</code>端口号，一般为1080。</p>
<p>在命令行中使用以下命令设置git代理：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global http.proxy socks5://127.0.0.1:10808</span><br><span class="line">git config --global https.proxy socks5://127.0.0.1:10808</span><br></pre></td></tr></table></figure>
<h3 id="http代理">http代理</h3>
<p>把v2rayN设为全局模式，打开win10的设置中代理，可以看到<code>socks</code>端口号。</p>
<p>在命令行中使用以下命令设置git代理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global http.proxy http://127.0.0.1:10809</span><br><span class="line">git config --global https.proxy https://127.0.0.1:10809</span><br></pre></td></tr></table></figure>
<h3 id="取消代理">取消代理</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br><span class="line">git config --global --unset https.proxy</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>常用技巧</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux常用指令</title>
    <url>/2022/09/09/linux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="linux常用指令">Linux常用指令</h1>
<h2 id="档案与目录管理">档案与目录管理</h2>
<h3 id="切换路径">切换路径</h3>
<h4 id="绝对路径">绝对路径</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 绝对路径，路径前+“/” 表示绝对路径</span><br><span class="line">cd /home/xc  </span><br></pre></td></tr></table></figure>
<h4 id="相对路径">相对路径</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd test </span><br></pre></td></tr></table></figure>
<h4 id="显示当前路径">显示当前路径</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pwd</span><br></pre></td></tr></table></figure>
<h4 id="清空终端信息">清空终端信息</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clear</span><br></pre></td></tr></table></figure>
<h3 id="显示文件列表">显示文件列表</h3>
<h4 id="当前目录下的文件目录名">当前目录下的文件、目录名</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ls</span><br></pre></td></tr></table></figure>
<h4 id="文件目录的详细信息">文件、目录的详细信息</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ll</span><br></pre></td></tr></table></figure>
<h4 id="指定文件名">指定文件名</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 当前目录下含有“test”的目录以及其详细信息</span><br><span class="line">ll test*</span><br></pre></td></tr></table></figure>
<h3 id="文件目录管理">文件目录管理</h3>
<h4 id="创建目录">创建目录</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir [list name]	</span><br></pre></td></tr></table></figure>
<h4 id="删除空目录">删除空目录</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rmdir [list name]	</span><br></pre></td></tr></table></figure>
<h4 id="删除文件">删除文件</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 文件名需要和ls中的对应</span><br><span class="line">rm [file name] 	</span><br></pre></td></tr></table></figure>
<h4 id="删除非空目录">删除非空目录</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 目录可不为空 </span><br><span class="line"># r-表示循环 f-表示force强力</span><br><span class="line">rm -rf [list name]	</span><br></pre></td></tr></table></figure>
<h4 id="复制目录或文件">复制目录或文件</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 复制文件 </span><br><span class="line"># 相对路径 or 绝对路径</span><br><span class="line">cp -r [origin file path] [destination file path]</span><br><span class="line"></span><br><span class="line"># 复制目录 </span><br><span class="line"># 相对路径 or 绝对路径</span><br><span class="line">cp -r [origin list path] [destination file path]</span><br></pre></td></tr></table></figure>
<h4 id="移动目录或文件">移动目录或文件</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 移动文件 </span><br><span class="line"># 相对路径 or 绝对路径</span><br><span class="line">mv [origin file path] [destination file path]</span><br><span class="line"></span><br><span class="line"># 移动目录 </span><br><span class="line"># 相对路径 or 绝对路径</span><br><span class="line">mv [origin list path] [destination file path]</span><br></pre></td></tr></table></figure>
<h2 id="管理文件">管理文件</h2>
<h3 id="创建文件">创建文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在当前路径下创建一个名为[file name]的文件, 需要后缀</span><br><span class="line">touch [file name] 	</span><br></pre></td></tr></table></figure>
<h3 id="查看文件">查看文件</h3>
<h4 id="显示文件内容">显示文件内容</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat [file name]</span><br></pre></td></tr></table></figure>
<h4 id="显示文件源码内容">显示文件源码内容</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat -A [file name]</span><br></pre></td></tr></table></figure>
<h4 id="按行显示">按行显示</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 一行一行显示文件内容</span><br><span class="line">more [file name]</span><br><span class="line"></span><br><span class="line"># 一行一行显示文件内容（可以翻页，但是没找到关闭的方式..）</span><br><span class="line">less [file name]</span><br></pre></td></tr></table></figure>
<h4 id="显示部分内容">显示部分内容</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 显示文件头几行，n表示显示的行数</span><br><span class="line">head -n [file name]	</span><br><span class="line"></span><br><span class="line"># 显示文件尾几行，n表示显示的行数</span><br><span class="line">tail -n [file name]	</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>常用技巧</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客搭建</title>
    <url>/2022/09/08/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h1 id="hexo博客搭建">Hexo博客搭建</h1>
<h2 id="推送博客文件">推送博客文件</h2>
<h3 id="清空之前的文件">清空之前的文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<h3 id="生成静态文章">生成静态文章</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br><span class="line"></span><br><span class="line"># 简写</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<h3 id="部署博客">部署博客</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo deploy </span><br><span class="line"></span><br><span class="line"># 简写 </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h2 id="本地测试">本地测试</h2>
<h3 id="生成静态文件">生成静态文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<h3 id="打开本地服务器">打开本地服务器</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<h2 id="写文章">写文章</h2>
<h3 id="新建文章">新建文章</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;title&quot;</span><br></pre></td></tr></table></figure>
<h3 id="新建草稿文件">新建草稿文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new draft &quot;title&quot;</span><br></pre></td></tr></table></figure>
<h3 id="新建界面">新建界面</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page &quot;title&quot;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>常用技巧</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>POJ-2182-线段树模板题</title>
    <url>/2023/10/24/poj2182/</url>
    <content><![CDATA[<h1 id="poj-2182-线段树模板题">POJ-2182-线段树模板题</h1>
<h2 id="题目描述">题目描述</h2>
<p>N (2 &lt;= N &lt;= 8,000) cows have unique brands in the range 1..N.
In a spectacular display of poor judgment, they visited the neighborhood
'watering hole' and drank a few too many beers before dinner. When it
was time to line up for their evening meal, they did not line up in the
required ascending numerical order of their brands.</p>
<p>Regrettably, FJ does not have a way to sort them. Furthermore, he's
not very good at observing problems. Instead of writing down each cow's
brand, he determined a rather silly statistic: For each cow in line, he
knows the number of cows that precede that cow in line that do, in fact,
have smaller brands than that cow.</p>
<p>Given this data, tell FJ the exact ordering of the cows.</p>
<h2 id="示例">示例</h2>
<h3 id="input">Input</h3>
<p>Line 1: A single integer, N</p>
<p>Lines 2..N: These N-1 lines describe the number of cows that precede
a given cow in line and have brands smaller than that cow. Of course, no
cows precede the first cow in line, so she is not listed. Line 2 of the
input describes the number of preceding cows whose brands are smaller
than the cow in slot #2; line 3 describes the number of preceding cows
whose brands are smaller than the cow in slot #3; and so on.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="output">Output</h3>
<p>Lines 1..N: Each of the N lines of output tells the brand of a cow in
line. Line #1 of the output tells the brand of the first cow in line;
line 2 tells the brand of the second cow; and so on.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="ac代码">AC代码</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    1. 左移乘法，右移除法</span></span><br><span class="line"><span class="comment">    2. 移1位表示 *or/2，移2位表示 *or/4</span></span><br><span class="line"><span class="comment">    3. 移位运算一定要加括号</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1e4</span>+<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span>&#123;</span></span><br><span class="line">    <span class="comment">// len表示当前节点的长度</span></span><br><span class="line">    <span class="keyword">int</span> l, r, len; </span><br><span class="line">&#125;tree[N*<span class="number">4</span>];</span><br><span class="line"><span class="keyword">int</span> pre[N], ans[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 建立线段树</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_tree</span><span class="params">(<span class="keyword">int</span> root, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span>&#123;</span><br><span class="line">    tree[root].l = left; tree[root].r = right;</span><br><span class="line">    tree[root].len = right-left+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(left == right) <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">build_tree</span>(root&lt;&lt;<span class="number">1</span>, left, (left+right)&gt;&gt;<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">build_tree</span>((root&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>, ((left+right)&gt;&gt;<span class="number">1</span>)+<span class="number">1</span>, right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 寻找当前节点下第k大的元素</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">quary</span><span class="params">(<span class="keyword">int</span> root, <span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">    tree[root].len--;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(tree[root].l == tree[root].r) <span class="keyword">return</span> tree[root].l;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果左子树节点数量小于k，则从右子树寻找第 k-tree[root&lt;&lt;2].len 个元素</span></span><br><span class="line">    <span class="keyword">if</span>(tree[root&lt;&lt;<span class="number">1</span>].len&lt;k) <span class="keyword">return</span> <span class="built_in">quary</span>((root&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>, k-tree[root&lt;&lt;<span class="number">1</span>].len);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果大于k，表示左子树节点数量足够，继续从左子树寻找</span></span><br><span class="line">    <span class="keyword">if</span>(tree[root&lt;&lt;<span class="number">1</span>].len &gt;= k) <span class="keyword">return</span> <span class="built_in">quary</span>(root&lt;&lt;<span class="number">1</span>, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++) <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;pre[i]);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">build_tree</span>(<span class="number">1</span>, <span class="number">1</span>, n);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=n<span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--)&#123;</span><br><span class="line">        ans[i] = <span class="built_in">quary</span>(<span class="number">1</span>, pre[i]+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++) cout &lt;&lt; ans[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    cout &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">5</span></span><br><span class="line"><span class="comment">0 1 2 1 0</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="http://poj.org/problem?id=2182">2182 -- Lost Cows
(poj.org)</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>线段树</tag>
      </tags>
  </entry>
  <entry>
    <title>POJ-3468</title>
    <url>/2023/10/25/poj3468/</url>
    <content><![CDATA[<h1 id="poj-3468">POJ-3468</h1>
<h2 id="题目描述">题目描述</h2>
<p>You have <em>N</em> integers, <em>A</em>1, <em>A</em>2, ... ,
<em>AN</em>. You need to deal with two kinds of operations. One type of
operation is to add some given number to each number in a given
interval. The other is to ask for the sum of numbers in a given
interval.</p>
<h2 id="示例">示例</h2>
<h3 id="input">Input</h3>
<p>The first line contains two numbers <em>N</em> and <em>Q</em>. 1 ≤
<em>N</em>,<em>Q</em> ≤ 100000. The second line contains <em>N</em>
numbers, the initial values of <em>A</em>1, <em>A</em>2, ... ,
<em>AN</em>. -1000000000 ≤ <em>Ai</em> ≤ 1000000000. Each of the next
<em>Q</em> lines represents an operation. "C <em>a</em> <em>b</em>
<em>c</em>" means adding <em>c</em> to each of <em>Aa</em>,
<em>Aa</em>+1, ... , <em>Ab</em>. -10000 ≤ <em>c</em> ≤ 10000. "Q
<em>a</em> <em>b</em>" means querying the sum of <em>Aa</em>,
<em>Aa</em>+1, ... , <em>Ab</em>.</p>
<p><strong>Sample Input</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">10 5</span><br><span class="line">1 2 3 4 5 6 7 8 9 10</span><br><span class="line">Q 4 4</span><br><span class="line">Q 1 10</span><br><span class="line">Q 2 4</span><br><span class="line">C 3 6 3</span><br><span class="line">Q 2 4</span><br></pre></td></tr></table></figure>
<h3 id="output">Output</h3>
<p>You need to answer all <em>Q</em> commands in order. One answer in a
line.</p>
<p><strong>Sample Output</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">4</span><br><span class="line">55</span><br><span class="line">9</span><br><span class="line">15</span><br></pre></td></tr></table></figure>
<p><strong>Hint</strong></p>
<p>The sums may exceed the range of 32-bit integers.</p>
<h2 id="ac代码">AC代码</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// #include&lt;bits/stdc++.h&gt;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="comment">// using namespace std;</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> Max = <span class="number">1e5</span>+<span class="number">2</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> lson rt&lt;&lt;1, l, mid</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> rson (rt&lt;&lt;1)+1, mid+1, r</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> l,r;</span><br><span class="line">    ll sum,add;</span><br><span class="line">    <span class="comment">/* data */</span></span><br><span class="line">&#125;tree[Max&lt;&lt;<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push_up</span><span class="params">(<span class="keyword">int</span> rt)</span></span>&#123;   <span class="comment">// 从下往上更新sum</span></span><br><span class="line">    tree[rt].sum = tree[rt&lt;&lt;<span class="number">1</span>].sum+tree[(rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>].sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push_down</span><span class="params">(<span class="keyword">int</span> rt)</span></span>&#123; <span class="comment">// 向下更新add和sum</span></span><br><span class="line">    <span class="keyword">if</span>(tree[rt].add!=<span class="number">0</span>)&#123;</span><br><span class="line">        tree[rt&lt;&lt;<span class="number">1</span>].add += tree[rt].add;</span><br><span class="line">        tree[(rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>].add += tree[rt].add;</span><br><span class="line">        tree[rt&lt;&lt;<span class="number">1</span>].sum += (tree[rt&lt;&lt;<span class="number">1</span>].r-tree[rt&lt;&lt;<span class="number">1</span>].l+<span class="number">1</span>)*tree[rt].add;</span><br><span class="line">        tree[(rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>].sum += (tree[(rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>].r-tree[(rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>].l+<span class="number">1</span>)*tree[rt].add;</span><br><span class="line">        tree[rt].add = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build</span><span class="params">(<span class="keyword">int</span> rt, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span>&#123;   <span class="comment">// 建线段树</span></span><br><span class="line">    tree[rt].l = l; tree[rt].r = r;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(l == r)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%lld&quot;</span>, &amp;tree[rt].sum);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> mid = (l+r)&gt;&gt;<span class="number">1</span>;</span><br><span class="line">    <span class="built_in">build</span>(lson);</span><br><span class="line">    <span class="built_in">build</span>(rson);</span><br><span class="line">    <span class="built_in">push_up</span>(rt);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(<span class="keyword">int</span> rt, <span class="keyword">int</span> a, <span class="keyword">int</span> b, ll c)</span></span>&#123;   <span class="comment">// 【a, b】区间每个数+c，使用lazy</span></span><br><span class="line">    <span class="keyword">if</span>(tree[rt].l&gt;=a &amp;&amp; tree[rt].r&lt;=b)&#123; <span class="comment">// 如果更新的区间 包含了 当前节点的区间</span></span><br><span class="line">        tree[rt].sum += (tree[rt].r-tree[rt].l+<span class="number">1</span>)*c;</span><br><span class="line">        tree[rt].add += c;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">push_down</span>(rt);</span><br><span class="line">    <span class="keyword">int</span> mid = (tree[rt].l+tree[rt].r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(a &lt;= mid) <span class="built_in">update</span>(rt&lt;&lt;<span class="number">1</span>, a, b, c);</span><br><span class="line">    <span class="keyword">if</span>(b &gt; mid) <span class="built_in">update</span>((rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>, a, b, c);</span><br><span class="line">    <span class="built_in">push_up</span>(rt);    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">quary</span><span class="params">(<span class="keyword">int</span> rt, <span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;   <span class="comment">// 查询【Na, Nb】区间和</span></span><br><span class="line">    <span class="keyword">if</span>(tree[rt].l&gt;=a &amp;&amp; tree[rt].r&lt;=b)&#123; <span class="comment">// 如果更新的区间 包含了 当前节点的区间</span></span><br><span class="line">        <span class="keyword">return</span> tree[rt].sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">push_down</span>(rt);</span><br><span class="line">    ll ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> mid = (tree[rt].l+tree[rt].r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span>(a &lt;= mid) ans += <span class="built_in">quary</span>(rt&lt;&lt;<span class="number">1</span>, a, b);</span><br><span class="line">    <span class="keyword">if</span>(b &gt; mid) ans += <span class="built_in">quary</span>((rt&lt;&lt;<span class="number">1</span>)+<span class="number">1</span>, a, b);</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, q, a, b;</span><br><span class="line">    ll c;</span><br><span class="line">    <span class="keyword">char</span> qu;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cin &gt;&gt; n &gt;&gt; q;</span></span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;n,&amp;q);</span><br><span class="line">    <span class="built_in">build</span>(<span class="number">1</span>, <span class="number">1</span>, n);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(q--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot; %c&quot;</span>, &amp;qu);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(qu == <span class="string">&#x27;Q&#x27;</span>)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d %d&quot;</span>, &amp;a, &amp;b);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%lld\n&quot;</span>, <span class="built_in">quary</span>(<span class="number">1</span>, a, b));</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%lld&quot;</span>, &amp;a,&amp;b, &amp;c);</span><br><span class="line">            <span class="built_in">update</span>(<span class="number">1</span>, a, b, c);</span><br><span class="line">        &#125; </span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// system(&quot;pause&quot;);</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">10 5</span></span><br><span class="line"><span class="comment">1 2 3 4 5 6 7 8 9 10</span></span><br><span class="line"><span class="comment">Q 4 4</span></span><br><span class="line"><span class="comment">Q 1 10</span></span><br><span class="line"><span class="comment">Q 2 4</span></span><br><span class="line"><span class="comment">C 3 6 3</span></span><br><span class="line"><span class="comment">Q 2 4</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">10 2</span></span><br><span class="line"><span class="comment">1 2 3 4 5 6 7 8 9 10</span></span><br><span class="line"><span class="comment">C 3 6 3</span></span><br><span class="line"><span class="comment">Q 2 4</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="http://poj.org/problem?id=3468">3468 -- A Simple Problem
with Integers (poj.org)</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>线段树</tag>
        <tag>区间求和</tag>
      </tags>
  </entry>
  <entry>
    <title>【Transformer】模型总结</title>
    <url>/2024/08/02/transformer/</url>
    <content><![CDATA[<h1 id="transformer模型总结">【Transformer】模型总结</h1>
<h2 id="模型架构">模型架构</h2>
<p>采用<strong>encoder-decoder的架构</strong></p>
<p>encoder将输入<span class="math inline">\((x_1,,x_n)\)</span>映射到连续的中间表达<span class="math inline">\((z_1,,z_n)\)</span></p>
<p>decoder再采用<strong>自回归</strong>的方式输出序列<span class="math inline">\((y_1,,y_n)\)</span></p>
<p>（将前一个生成的符号添加到输入，接着生成下一个（类似RNN））</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724172359971.png" alt="image-20240724172359971">
<figcaption aria-hidden="true">image-20240724172359971</figcaption>
</figure>
<h3 id="编码器">编码器</h3>
<p>Transformer编码器由N个EncoderBlock堆叠而成，每个EncoderBlock有两个子层：</p>
<ul>
<li>Multi-Head Attention (Self-Attention)
<ul>
<li>Add (残差连接)</li>
<li>Norm (层归一化)</li>
</ul></li>
<li>Positionwise Feed Forward Network
<ul>
<li>Add (残差连接)</li>
<li>Norm (层归一化)</li>
</ul></li>
</ul>
<p><span class="math inline">\(EncoderBlock_i\)</span>的输出给到下一层，作为<span class="math inline">\(EncoderBlock_{i+1}（i=1,2,..,n-1）\)</span>的输入</p>
<p>将<span class="math inline">\(EncoderBlock_{n}\)</span>的输出给到解码器的各层中</p>
<h3 id="解码器">解码器</h3>
<p>Transformer编码器由N个DecoderBlock堆叠而成，每个DecoderBlock有三个子层：</p>
<ul>
<li><p>Masked Multi-Head Attention (Self-Attention)</p>
<ul>
<li><p>Add (残差连接)</p></li>
<li><p>Norm (层归一化)</p></li>
</ul></li>
<li><p>Multi-Head Attention (Co-Attention)</p>
<ul>
<li>Add (残差连接)</li>
<li>Norm (层归一化)</li>
</ul></li>
<li><p>Positionwise Feed Forward Network</p>
<ul>
<li>Add (残差连接)</li>
<li>Norm (层归一化)</li>
</ul></li>
</ul>
<p><span class="math inline">\(DecoderBlock_i\)</span>的输出给到下一层，作为<span class="math inline">\(DecoderBlock_{i+1}（i=1,2,..,n-1）\)</span>的输入</p>
<h2 id="注意力机制">注意力机制</h2>
<p>Query Key-Value</p>
<p>通过计算Q与每个Key之间的<strong>关联度</strong>得到一个权重，通过softmax得到每个Key对应Value的权重，计算Value的<strong>加权和</strong>作为Query的输出</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724155036667.png" alt="image-20240724155036667">
<figcaption aria-hidden="true">image-20240724155036667</figcaption>
</figure>
<h3 id="带掩码的softmax">带掩码的Softmax</h3>
<p>在Decoder中并非所有的值都应该被纳入到注意力汇聚中。推理t时刻，需要屏蔽到t时刻以后的输入，即将
<span class="math inline">\(k_t, k_{t+1}, ,,k_n\)</span>
换成非常小的数（例如<span class="math inline">\(1e^{-11}\)</span>），
经过softmax后得到值就是0（或者一些padding的无效字符）。</p>
<p>为了仅将有意义的词元作为值来获取注意力汇聚，
可以指定一个有效序列长度（即词元的个数），
以便在计算softmax时过滤掉超出指定范围的位置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="缩放点积注意力">缩放点积注意力</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724155820490.png" alt="image-20240724155820490">
<figcaption aria-hidden="true">image-20240724155820490</figcaption>
</figure>
<p>transformer中采用Scaled Dot-Product Attention的方式计算注意力函数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724152933911.png" alt="image-20240724152933911">
<figcaption aria-hidden="true">image-20240724152933911</figcaption>
</figure>
<p>其中Key、Value的维度是<span class="math inline">\(d_k,d_v\)</span>，
并要求Query和Key具有相同的维度<span class="math inline">\(d_k\)</span></p>
<p>将Query与所有的Key做<strong>内积</strong>，再除以<span class="math inline">\(\sqrt{d_k}\)</span></p>
<p>（内积相当于计算余弦值，越大表示相似度越高，如果两个向量正交余弦值=0，表示不相关）</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724155617867.png" alt="image-20240724155617867">
<figcaption aria-hidden="true">image-20240724155617867</figcaption>
</figure>
<p>假设查询和键的所有元素都是独立的随机变量，
并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为𝑑。</p>
<p>为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1，
再将点积除以𝑑（即“缩放”）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，d)</span></span><br><span class="line">        <span class="comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span></span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span></span><br><span class="line">        </span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度(计算转置)</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
<h3 id="多头注意力">多头注意力</h3>
<p>当给定相同的查询、键和值时，希望模型可以基于相同的注意力机制学习到不同的行为，
然后将不同的行为作为知识组合起来， 捕获序列内各种关系 。</p>
<p>为此，与其只使用单独一个注意力汇聚，
我们可以用独立学习得到的ℎ组不同的 <em>线性投影</em>（linear
projections）来变换查询、键和值。
然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。
（类似CNN中多个输出通道）</p>
<p>最后将这ℎ个注意力汇聚的输出拼接在一起，
并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724164641275.png" alt="image-20240724164641275">
<figcaption aria-hidden="true">image-20240724164641275</figcaption>
</figure>
<p>计算方法（<span class="math inline">\(d_{model}=512\)</span>​），h=8个头，每次将其投影到低维64维的空间计算注意力函数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724161309436.png" alt="image-20240724161309436">
<figcaption aria-hidden="true">image-20240724161309436</figcaption>
</figure>
<p>多头注意力机制具体实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, </span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, num_heads, </span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span></span><br><span class="line">        <span class="comment"># queries，keys，values的形状: (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状: (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状: </span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)</span></span><br><span class="line">        </span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)</span></span><br><span class="line">        <span class="comment"># 将多头注意力的计算合并到一次计算</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 为了能够使多个头并行计算，定义的两个转置函数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads)</span></span><br><span class="line">    <span class="comment"># 将第2，3维度交换位置</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads, 查询或者“键－值”对的个数, num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose_output函数反转了transpose_qkv函数的操作。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transpose_output</span>(<span class="params">X, num_heads</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="自注意机制">自注意机制</h3>
<p>模型的输入同时作为 Key Value Qurey</p>
<p>和自己计算相似度（向量内积）最大=1</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724161741228.png" alt="image-20240724161741228" style="zoom: 33%;"></p>
<p>在计算Decoder时使用掩码（Mask）</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724162135199.png" alt="image-20240724162135199" style="zoom: 50%;"></p>
<p>Decoder第二个子层中使用Encoder的输出作为Key-Value，上一层的输出作为Query。</p>
<p>在编码器和解码器中传递信息</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724162544352.png" alt="image-20240724162544352" style="zoom:50%;"></p>
<h2 id="基于位置的前馈网络">基于位置的前馈网络</h2>
<p>基于位置的：前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP）</p>
<p>两层的感知机，并使用Relu激活函数</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724162813771.png" alt="image-20240724162813771">
<figcaption aria-hidden="true">image-20240724162813771</figcaption>
</figure>
<p>输入大小：（批量大小，时间步数或序列长度，隐单元数或特征维度）</p>
<p>输出大小：（批量大小，时间步数，ffn_num_outputs）</p>
<p>（pytorch默认对最后一个维度进行计算）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionWiseFFN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                 **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Transformer vs RNN</p>
</blockquote>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724163422153.png" alt="image-20240724163422153" style="zoom:50%;"></p>
<h2 id="残差连接和层规范化">残差连接和层规范化</h2>
<p>层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化。</p>
<p>对于BatchNorm来说，样本长度对计算方差影响比较大。
而LayerNorm针对每个样本计算方差</p>
<p>批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p>
<p>两个维度的情况</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724154231420.png" alt="image-20240724154231420" style="zoom: 80%;"></p>
<p>三个维度的情况</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724154415275.png" alt="image-20240724154415275" style="zoom:80%;"></p>
<p><span class="math inline">\(LayerNorm(x+Sublayer(x))\)</span>
使用残差连接和层规范化来实现<code>AddNorm</code>类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<h2 id="位置编码">位置编码</h2>
<p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，
而自注意力则因为并行计算而放弃了顺序操作。</p>
<p>为了使用序列的顺序信息，通过在输入表示中添加 位置编码（positional
encoding）来注入绝对的或相对的位置信息。</p>
<p>位置编码可以通过学习得到也可以直接固定得到。
（Transformer中使用固定的位置编码，Bert位置编码通过学习得来）</p>
<blockquote>
<p>计算注意力时只关注Key和Query向量之间的相似度。和该Query在序列中的相对位置无关，因此引入位置编码</p>
</blockquote>
<p>（也就表示同一个Query不管在序列中任何位置计算出的注意力是相同）（在输入时加入时序信息）</p>
<p>Transformer中使用基于正弦函数和余弦函数的固定位置编码</p>
<p>将Encoder和Decoder将输入映射到<span class="math inline">\(d_{model}=512\)</span>的向量，加
上一个位置向量（计算如下）即加入了位置信息。</p>
<p>假设输入表示<span class="math inline">\(𝑋∈𝑅^{𝑛×𝑑}\)</span>
包含一个序列中𝑛个词元的𝑑维嵌入表示。 位置编码使用相同形状的位置嵌入矩阵$
𝑃∈𝑅^{𝑛×𝑑}$</p>
<p>输出𝑋+𝑃， 矩阵第𝑖行、第2𝑗列和2𝑗+1列上的元素为</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240725095145276.png" alt="image-20240725095145276">
<figcaption aria-hidden="true">image-20240725095145276</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240725095645828.png" alt="image-20240725095645828">
<figcaption aria-hidden="true">image-20240725095645828</figcaption>
</figure>
<h2 id="编码器-1">编码器</h2>
<p>编码器使用<strong>N=6</strong>独立层，每层包含两个子层</p>
<ul>
<li><strong>multi-head self-attention
mechanism（多头注意力机制）</strong></li>
<li><strong>position-wise fully connected feed-forward
network.（简单说就是全连接层（MLP））</strong></li>
</ul>
<p>两个子层间采用<strong>残差连接</strong></p>
<p>并使用<strong>层归一化</strong>处理 <span class="math inline">\(LayerNorm(x+Sublayer(x))\)</span></p>
<p>固定每个层的输出为<span class="math inline">\(d_{model}=512\)</span></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724180323352.png" alt="image-20240724180323352">
<figcaption aria-hidden="true">image-20240724180323352</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens</span>):</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>
<p>Encoder堆叠了num_layers个EncoderBlock类的实例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">d2l.Encoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        </span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        </span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<h2 id="解码器-1">解码器</h2>
<p>同样采用N=6的独立层</p>
<p>相较于encoder多第三个子层<strong>掩码的多头注意力机制</strong></p>
<p>（控制输入模型的序列部分可见。
例如预测第t时刻，不应该看到t时刻以后的输入）</p>
<p>Masked Multi-head Attention</p>
<p>在DecoderBlock类中实现的每个层包含了三个子层：</p>
<ol type="1">
<li>解码器自注意力</li>
<li>“编码器-解码器”注意力</li>
<li>基于位置的前馈网络</li>
</ol>
<p>子层之间通过残差连接和层规范化</p>
<p><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240725090842835.png" alt="image-20240725090842835" style="zoom:80%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, i, **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        </span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        </span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        </span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="comment"># state = [Encoder输出,mask长度, 直到当前时间步第i个块解码的输出表示]</span></span><br><span class="line">        <span class="comment"># X.shape = [batch_size, num_steps, d]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        	</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(<span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>构建了由num_layers个DecoderBlock实例组成的完整的Transformer解码器。</p>
<p>最后，通过一个全连接层计算所有vocab_size个可能的输出词元的预测值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">d2l.AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_heads, num_layers, dropout, **kwargs</span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># Embedding + Positional Encoding</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        </span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][i] = blk.attention2.attention.attention_weights</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br></pre></td></tr></table></figure>
<h2 id="模型分析">模型分析</h2>
<p>n为序列的长度，d为模型的维度<span class="math inline">\(d_{model}\)</span></p>
<table>
<colgroup>
<col style="width: 30%">
<col style="width: 23%">
<col style="width: 24%">
<col style="width: 22%">
</colgroup>
<thead>
<tr>
<th>Layer Type</th>
<th>Complexity per Layer</th>
<th>Sequential Operations</th>
<th>Maximum Path Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-Attention</td>
<td><span class="math inline">\(O(n^2d)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr>
<td>Recurrent</td>
<td><span class="math inline">\(O(nd^2)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr>
<td>Convolutional</td>
<td><span class="math inline">\(O(knd^2)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(log_k(n))\)</span></td>
</tr>
<tr>
<td>Self-Attention(restricted)</td>
<td><span class="math inline">\(O(rnd)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(n/r)\)</span></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>LLM</category>
      </categories>
  </entry>
  <entry>
    <title>二叉树遍历</title>
    <url>/2021/10/09/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/</url>
    <content><![CDATA[<h1 id="二叉树遍历">二叉树遍历</h1>
<h2 id="概述">概述</h2>
<ul>
<li><p>二叉树是一种经常用到的数据结构，用来模拟具有树状结构性质的数据集合。</p></li>
<li><p>二叉树是一种更为典型的树状结构。如它名字所描述的那样，二叉树是每个节点最多有两个子树的树结构，通常子树被称作“左子树”和“右子树”。</p></li>
</ul>
<p>定义二叉树的节点</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Definition for a binary tree node.</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TreeNode</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    TreeNode *left;</span><br><span class="line">    TreeNode *right;</span><br><span class="line">    <span class="built_in">TreeNode</span>() : <span class="built_in">val</span>(<span class="number">0</span>), <span class="built_in">left</span>(<span class="literal">nullptr</span>),<span class="built_in">right</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">TreeNode</span>(<span class="keyword">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">left</span>(<span class="literal">nullptr</span>), <span class="built_in">right</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    <span class="built_in">TreeNode</span>(<span class="keyword">int</span> x, TreeNode *left, TreeNode right) : <span class="built_in">val</span>(x), <span class="built_in">left</span>(left), <span class="built_in">right</span>(right) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>常见的二叉树遍历方式有<strong>前序遍历、中序遍历、后序遍历、层序遍历</strong>。下面实现二叉树遍历的四种方式：</p>
<h2 id="遍历方式">遍历方式</h2>
<h3 id="前序遍历">前序遍历</h3>
<p>前序遍历的顺序是 <strong>根节点-&gt;左节点-&gt;右节点</strong></p>
<p>下列二叉树的前序遍历为： A B D E C F</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090012434.png" alt="image-20211009130611162">
<figcaption aria-hidden="true">image-20211009130611162</figcaption>
</figure>
<h4 id="递归实现">递归实现</h4>
<p>利用递归实现前序遍历十分简单</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode* root)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(root, ans);</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root, vector&lt;<span class="keyword">int</span>&gt; ans)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;left);</span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="栈实现方法">栈实现方法</h4>
<p>利用栈实现前序遍历的核心是将树节点push到栈中然后在合适的时间从栈中弹出。</p>
<ul>
<li>首先将左节点push到栈中，push的同时访问当前节点数据。直到当前节点的左节点为nullptr。</li>
<li>判断栈是否为空，如果不为空就弹出当前栈顶节点，并push栈顶节点的右节点。</li>
<li>迭代上述操作。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    stack&lt;TreeNode *&gt; mySta;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    TreeNode* cur = root;</span><br><span class="line">    <span class="keyword">while</span>(cur != <span class="literal">nullptr</span> || !mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        <span class="keyword">while</span>(cur != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            mySta.<span class="built_in">push</span>(cur);</span><br><span class="line">            ans.<span class="built_in">push_back</span>(cur-&gt;val);</span><br><span class="line">            <span class="comment">// mySta.push(cur);</span></span><br><span class="line">            cur = cur-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(!mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            cur = mySta.<span class="built_in">top</span>();</span><br><span class="line">            mySta.<span class="built_in">pop</span>();</span><br><span class="line">            cur = cur-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="中序遍历">中序遍历</h3>
<p>中序遍历的顺序是 <strong>左节点-&gt;根节点-&gt;右节点</strong></p>
<p>下列二叉树的中序遍历为： D B E A F C</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090013617.png" alt="image-20211009135839077">
<figcaption aria-hidden="true">image-20211009135839077</figcaption>
</figure>
<h4 id="递归实现-1">递归实现</h4>
<p>利用递归实现中序遍历十分简单</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode* root)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(root, ans);</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root, vector&lt;<span class="keyword">int</span>&gt; ans)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;left);</span><br><span class="line">    ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="栈实现方法-1">栈实现方法</h4>
<p>利用栈实现中序遍历的核心与前序遍历相同，只是需要注意访问节点数据的时机有所不同。</p>
<ul>
<li>首先将左节点push到栈中直到当前节点的左节点为nullptr。</li>
<li>判断栈是否为空，如果不为空就访问当前栈顶节点，并弹出当前栈顶节点，并push栈顶节点的右节点。</li>
<li>迭代上述操作。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">        stack&lt;TreeNode*&gt; mySta;</span><br><span class="line">        <span class="comment">// mySta.push(root);</span></span><br><span class="line"></span><br><span class="line">        TreeNode *cur = root;</span><br><span class="line">        <span class="keyword">while</span>(cur != <span class="literal">nullptr</span> || !mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            <span class="keyword">while</span>(cur != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                mySta.<span class="built_in">push</span>(cur);</span><br><span class="line">                cur = cur-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(!mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">                cur = mySta.<span class="built_in">top</span>();</span><br><span class="line">                mySta.<span class="built_in">pop</span>();</span><br><span class="line">                ans.<span class="built_in">push_back</span>(cur-&gt;val);</span><br><span class="line">                cur = cur-&gt;right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="后序遍历">后序遍历</h3>
<p>后序遍历的顺序是 <strong>左节点-&gt;右节点-&gt;根节点</strong></p>
<p>下列二叉树的后序遍历为： D E B F C A</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090014059.jpeg" alt="image-20211009140543901">
<figcaption aria-hidden="true">image-20211009140543901</figcaption>
</figure>
<h4 id="递归实现-2">递归实现</h4>
<p>利用递归实现前序遍历十分简单</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">postorderTraversal</span><span class="params">(TreeNode* root)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(root, ans);</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root, vector&lt;<span class="keyword">int</span>&gt; ans)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;left);</span><br><span class="line">    <span class="built_in">dfs</span>(root-&gt;right);</span><br><span class="line">    ans.<span class="built_in">push_back</span>(root-&gt;val);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="栈实现方法-2">栈实现方法</h4>
<p>利用栈实现二叉树的后序遍历的核心与前序遍历相同。但需要注意的是，如果直接使用栈来实现后序遍历会相对比较麻烦，因为需要控制第二次访问根节点才能将栈顶元素弹出，从而需要一个额外的栈来实现节点的访问计数。因此我将讨论后序遍历与前序遍历之间的关系，来寻找更加简便的方法。</p>
<p>根据前面的讨论我们可以得知</p>
<ul>
<li><strong>前序遍历</strong>访问节点的顺序是
<strong>根节点-&gt;左节点-&gt;右节点</strong></li>
<li><strong>后序遍历</strong>访问节点的顺序是
<strong>左节点-&gt;右节点-&gt;根节点</strong></li>
</ul>
<p>假设我们将前序遍历的左右节点访问顺序交换，我们将会得到
<strong>根节点-&gt;右节点-&gt;左节点</strong> 的访问顺序。</p>
<p>接着将上述的顺序进行reverse。得到的顺序是
<strong>左节点-&gt;右节点-&gt;根节点</strong>
恰好和后序遍历的顺序相同。</p>
<p>因此我们只需要将前序遍历的栈实现方式中的所有<strong>左节点改为右节点</strong>，<strong>右节点改为左节点</strong>。同时在最后对结果进行<strong>reverse</strong>即可。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">postorderTraversal</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    stack&lt;TreeNode *&gt; mySta;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    TreeNode* cur = root;</span><br><span class="line">    <span class="keyword">while</span>(cur != <span class="literal">nullptr</span> || !mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        <span class="keyword">while</span>(cur != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">            mySta.<span class="built_in">push</span>(cur);</span><br><span class="line">            ans.<span class="built_in">push_back</span>(cur-&gt;val);</span><br><span class="line">            <span class="comment">// mySta.push(cur);</span></span><br><span class="line">            cur = cur-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(!mySta.<span class="built_in">empty</span>())&#123;</span><br><span class="line">            cur = mySta.<span class="built_in">top</span>();</span><br><span class="line">            mySta.<span class="built_in">pop</span>();</span><br><span class="line">            cur = cur-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">reverse</span>(ans.<span class="built_in">begin</span>(),ans.<span class="built_in">end</span>());</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="层序遍历">层序遍历</h3>
<p>层序遍历就是<strong>逐层遍历树结构</strong>。</p>
<p><strong>广度优先搜索</strong>是一种广泛运用在树或图这类数据结构中，遍历或搜索的算法。
该算法从一个根节点开始，首先访问节点本身。
然后遍历它的相邻节点，其次遍历它的二级邻节点、三级邻节点，以此类推。</p>
<p>当我们在树中进行广度优先搜索时，我们访问的节点的顺序是按照层序遍历顺序的。</p>
<p>下列二叉树的后序遍历为：A B C D E F</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090014780.png" alt="image-20211009142500875">
<figcaption aria-hidden="true">image-20211009142500875</figcaption>
</figure>
<h4 id="广度优先搜索实现">广度优先搜索实现</h4>
<p>广度优先搜索的核心是<strong>队列</strong>。将队首元素出队，如果该节点有子节点则将子节点入队。直到队首节点没有子节点，并且队列中没有节点时，遍历结束。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;vector&lt;<span class="keyword">int</span>&gt;&gt; <span class="built_in">levelOrder</span>(TreeNode* root) &#123;</span><br><span class="line">    queue&lt;TreeNode*&gt; myQueue;</span><br><span class="line">    vector&lt;vector&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    myQueue.<span class="built_in">push</span>(root);</span><br><span class="line">    <span class="keyword">while</span>(!myQueue.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        <span class="keyword">int</span> cnt = myQueue.<span class="built_in">size</span>();</span><br><span class="line">        vector&lt;<span class="keyword">int</span>&gt; temp;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;cnt; i++)&#123;</span><br><span class="line">            TreeNode *cur = myQueue.<span class="built_in">front</span>();</span><br><span class="line">            myQueue.<span class="built_in">pop</span>();</span><br><span class="line">            temp.<span class="built_in">push_back</span>(cur-&gt;val);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;left != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                myQueue.<span class="built_in">push</span>(cur-&gt;left);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(cur-&gt;right != <span class="literal">nullptr</span>)&#123;</span><br><span class="line">                myQueue.<span class="built_in">push</span>(cur-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ans.<span class="built_in">push_back</span>(temp);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
        <category>图论</category>
      </categories>
      <tags>
        <tag>二叉树</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>【Transformer】位置编码</title>
    <url>/2024/08/02/%E3%80%90Transformer%E3%80%91%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
    <content><![CDATA[<h1 id="transformer位置编码">【Transformer】位置编码</h1>
<p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的通过中间状态保存顺序信息，
但是Transformer中使用自注意力并行计算而无法保存序列的顺序信息。</p>
<p>因此在Transformer中为了使用序列的顺序信息，通过在输入表示中添加
<strong>位置编码</strong>（positional
encoding）来在输入中加入位置信息。</p>
<p>位置编码可以通过学习得到也可以直接固定得到。</p>
<ul>
<li>Transformer中使用基于正弦函数和余弦函数的固定位置编码。</li>
<li>Bert通过学习的方式获得位置编码</li>
</ul>
<p>假设输入表示<span class="math inline">\(X∈R^{n×d}\)</span>
包含一个序列中n个词元的d维嵌入表示。</p>
<p>位置编码使用相同形状的位置嵌入矩阵 <span class="math inline">\(P∈R^{n×d}\)</span> 输出<span class="math inline">\(X+P\)</span>，
矩阵第i行、第2j列和2j+1列上的元素为： <span class="math display">\[
\begin{align}
p_{i,2j}&amp;=sin(\frac{i}{10000^{2j/d}})\\
p_{i,2j+1}&amp;=cos(\frac{i}{10000^{2j/d}})
\end{align}
\]</span> 在位置嵌入矩阵P中，
行代表词元在序列中的位置（num_step），列代表位置编码的不同维度(d_model/encoding_dim)。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240802184231821.png" alt="image-20240802184231821">
<figcaption aria-hidden="true">image-20240802184231821</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        base = torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(<span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens) <span class="comment"># [num_hiden/2]</span></span><br><span class="line">        X = X / base <span class="comment"># [max_len, num_hiden/2] 广播机制</span></span><br><span class="line">        </span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X) <span class="comment"># 切片获得偶数列</span></span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br><span class="line"></span><br><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim))) <span class="comment"># [batch_size, num_steps, encoding_dim]</span></span><br></pre></td></tr></table></figure>
<p>绘制P矩阵前60行的6-9列可以看出P矩阵的第6列和第7列的频率高于第8列和第9列。
第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240802184327328.png" alt="image-20240802184327328">
<figcaption aria-hidden="true">image-20240802184327328</figcaption>
</figure>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>LLM</category>
      </categories>
  </entry>
  <entry>
    <title>历史散文、诗人</title>
    <url>/2021/10/04/%E5%8E%86%E5%8F%B2%E6%95%A3%E6%96%87%E3%80%81%E8%AF%97%E4%BA%BA/</url>
    <content><![CDATA[<h1 id="历史散文诗人">历史散文、诗人</h1>
<h2 id="历史散文">历史散文</h2>
<p><strong>《左传》：</strong>我国第一部叙事详细 形式完备的编年体史书。
（左丘明著）</p>
<p><strong>《春秋》：</strong>我国第一部编年体历史著作 （比左传早）</p>
<p><strong>《尚书》：</strong>我国第一部历史散文著作</p>
<p><strong>《国语》：</strong>我国第一部国别体史书。 （左丘明著）</p>
<p><strong>《战国策》：</strong>我国第一部国别体杂史。 （刘向著）</p>
<h2 id="屈原">屈原</h2>
<ul>
<li><p>字原、名平。自称云名正则，字灵均。</p></li>
<li><p>楚辞体的创始人。</p></li>
<li><p>是我国第一个爱国主义诗人、我国第一个浪漫主义诗人。</p></li>
<li><p>是我国浪漫主义文学奠基人，开辟了“香草美人”的传统。</p></li>
<li><p>被称为辞赋之祖、中华诗祖。</p></li>
</ul>
<p><strong>代表作：</strong>《九章》、《九歌》、《天问》、《离骚》。</p>
<ul>
<li><p>《楚辞》是我国第一部浪漫主义诗歌总集，是我国浪漫主义文学源头。（不是屈原一人写）</p></li>
<li><p>《离骚》和《国风》并称风骚。</p></li>
<li><p>《离骚》是我国诗歌中最长的浪漫主义政治抒情诗！</p></li>
</ul>
<h2 id="西汉">西汉</h2>
<ul>
<li><p>诗歌分为 汉乐府（五言为主）</p></li>
<li><p>《古诗十九首》为五言古诗体
被称为“一字千金”和“五言之冠冕”</p></li>
</ul>
<p><strong>汉乐府：</strong>汉代乐府机关</p>
<p><strong>汉乐府特色：</strong>以五言及杂言为主，口语化，题材叙事性，继承现代主义传统</p>
<p><strong>汉代四大家：</strong>司马相如、杨雄、班固、张衡。</p>
<p><strong>史书（司马光）：</strong>我国第一部纪传体通史。</p>
<ul>
<li>史书分为本纪 30世家、70列传、10表8书</li>
</ul>
<p><strong>汉书（班固）：</strong>我国第一部纪传体断代史。</p>
<p><strong>陌上桑语言特点：</strong>幽默诙谐，具有喜剧色彩，口语化，铺陈直叙的手法夸赞罗敷。</p>
<h2 id="李斯秦代">李斯（秦代）</h2>
<ul>
<li><p>法家学派</p></li>
<li><p>被称为 "秦朝文学一枝独秀”。</p></li>
<li><p>谏逐客书选自《史书 李斯列传》 （是奏章体政论文/议论文）</p></li>
</ul>
<p><strong>四君：</strong>秦穆公、秦孝公、秦惠公、秦昭王。</p>
<p><strong>期：</strong>服丧一年 大功：服丧九个月 小功：服丧五个月</p>
<h2 id="陶渊明">陶渊明</h2>
<ul>
<li><p>名潜、字元亮/渊明。私益号：靖节先生。自号：五柳先生。</p></li>
<li><p>田园诗创始人。</p></li>
<li><p>是我国最早大量创造田园诗的诗人。</p></li>
</ul>
<p><strong>称号：</strong>古今隐逸之宗/百世田园之主/千古隐逸之宗</p>
<p><strong>诗歌思想：</strong>厌倦官场、崇尚自然。</p>
<p><strong>诗歌风格：</strong>质朴自然、冲淡和平。</p>
<p><strong>诗歌特色：</strong>语言自然、朴素率真、善用白描、写意勾勒。</p>
<p><strong>文体：</strong>五言古体诗。</p>
<p><strong>题材：</strong>田园诗。</p>
<p><strong>作品集：</strong>《陶渊明集》</p>
<p><strong>代表作：</strong>《归园田居》、《桃花源记》、《归去来兮辞》</p>
<p><strong>山水诗开创者：</strong>谢灵运（谢康乐、大谢）、
谢朓（小谢）</p>
<ul>
<li>王国维在《人间词话》中说“无我之境，以物观物，故不知何者为我，何者为能”充分体现陶渊明《饮酒》诗歌的精神内涵</li>
</ul>
<h2 id="张若虚初唐后期">张若虚（初唐后期）</h2>
<ul>
<li><p>“吴中四士”之一（张若虚、贺知章、包融、张旭）</p></li>
<li><p>张若虚现存诗歌：《代答闺梦还》、《春江花月夜》</p></li>
<li><p>文体：古乐府诗/旧题、七言古诗、长篇歌行体。</p></li>
<li><p>长篇歌行体 是鲍照创照（题目中有歌、行、谣、引 的乐府诗）</p></li>
<li><p>闻一多代表作：《死水》《红烛》</p></li>
<li><p>评价《春江花月夜》是“诗中的诗，顶峰中的顶峰”“孤篇压全答”“孤篇横绝，竞为大家”“圣唐/唐代第一诗”</p></li>
</ul>
<h2 id="李白圣唐">李白（圣唐）</h2>
<ul>
<li><p>字太白、号青莲居士。诗仙，谪仙人。</p></li>
<li><p>浪漫主义诗人 。</p></li>
<li><p>诗歌：歌颂祖国大好河山，表现理想与现实的矛盾，风格豪放飘逸。</p></li>
<li><p>“唐代最伟大的诗人之一”</p></li>
</ul>
<p><strong>作品：</strong>《李太白集》</p>
<p><strong>古诗体：</strong>蜀道难、将进酒、梦游天老吟留别（楚词体）</p>
<p>​ 宣州谢脁楼饯别校书叔云</p>
<p><strong>《行路难》文体：</strong>乐府古诗、歌行体、七言古诗。选自《李白集校注》</p>
<h2 id="王伟盛唐">王伟（盛唐）</h2>
<ul>
<li><p>字摩诘（诗佛）</p></li>
<li><p>盛唐时期，山水田园诗派（描写自然风光，农村景象以及安逸恬淡的隐居生活，多用白描）</p></li>
<li><p>代表诗人：王维、孟浩然</p></li>
<li><p>被称为“天下之宗”。</p></li>
</ul>
<p><strong>作品集：</strong>《王右丞集》</p>
<p>苏轼评价王维诗歌：诗中有画，画中有诗</p>
<p>开创水墨山水画派</p>
]]></content>
      <categories>
        <category>HSY</category>
      </categories>
      <tags>
        <tag>语文</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2024/07/30/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h1 id="动态规划">动态规划</h1>
<h2 id="背包问题">背包问题</h2>
<h3 id="背包">0-1背包</h3>
<p>转移方程：<span class="math inline">\(dp[i][j]=max(dp[i-1][j],
dp[i-1][j-v[i]]+w[i])\)</span></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> v[N], w[N], dp[N][N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"></span><br><span class="line"><span class="comment">// dp[i][j] 为考虑1-i个物品，背包容积为j时的最大价值</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= m; j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= v[i]) dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j-v[i]]+w[i], dp[i<span class="number">-1</span>][j]); </span><br><span class="line">        <span class="keyword">else</span> dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在计算 <span class="math inline">\(dp[i][j]\)</span>
时，只会用到<span class="math inline">\(dp[i-1][j]\)</span>，不会用到比<span class="math inline">\(i\)</span>更早的状态。</p>
<p>因此可以考虑因此可以去掉第一个维度，反复利用同一个一维数组。</p>
<p>转移方程：$dp[j] = max(dp[j], dp[i-1][j-v[i]]+w[i]) $</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> v[N], w[N], dp[N][N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = m; j &gt;= <span class="number">1</span>; j--)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= v[i]) dp[j] = <span class="built_in">max</span>(dp[j-v[i]]+w[i], dp[j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="完全背包">完全背包</h3>
<p>转移方程：<span class="math inline">\(dp[i][j]=max(dp[i-1][j],
dp[i][j-v[i]]+w[i])\)</span></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> v[N], w[N], dp[N][N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= m; j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= v[i]) dp[i][j] = <span class="built_in">max</span>(dp[i][j-v[i]]+w[i], dp[i<span class="number">-1</span>][j]);</span><br><span class="line">        <span class="keyword">else</span> dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="多重背包">多重背包</h3>
<p>转移方程：<span class="math inline">\(dp[i][j]=max(dp[i-1][j],
dp[i-1][j-k*v[i]]+w[i]*k), k=0,1,..,s[i]\)</span></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> v[N], w[N], s[N], dp[N][N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line">   </span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= m; j++)&#123;</span><br><span class="line">        dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt;= s[i] &amp;&amp; v[i]*k &lt;= j; k++) </span><br><span class="line">            dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i<span class="number">-1</span>][j-v[i]*k]+k*w[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="分组背包">分组背包</h3>
<p>每组物品有若干个，同一组内的物品最多只能选一个</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// s[i] 第i组物品个数</span></span><br><span class="line"><span class="comment">// v[i][j] i组中第j个物品的体积</span></span><br><span class="line"><span class="comment">// w[i][j] i组中第j个物品的价值</span></span><br><span class="line"><span class="keyword">int</span> s[N], v[N][N], w[N][N], dp[N][N];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;  <span class="comment">// 遍历组编号</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=m; j++)&#123;	<span class="comment">// 遍历背包容积</span></span><br><span class="line">        dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=s[i]; k++)&#123;  <span class="comment">// 遍历组内物品</span></span><br><span class="line">            <span class="keyword">if</span>(v[i][k] &lt;= j) </span><br><span class="line">                dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i<span class="number">-1</span>][j-v[i][k]]+w[i][k]);</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="线性dp">线性DP</h2>
<ul>
<li>子序列不要求在原序列中连续</li>
<li>字串要求连续</li>
</ul>
<blockquote>
<p><a href="https://leetcode.cn/problems/climbing-stairs/?envType=study-plan-v2&amp;envId=dynamic-programming">70.
爬楼梯 - 力扣（LeetCode）</a></p>
<p><a href="https://leetcode.cn/problems/min-cost-climbing-stairs/description/?envType=study-plan-v2&amp;envId=dynamic-programming">746.
使用最小花费爬楼梯 - 力扣（LeetCode）</a></p>
<p><a href="https://leetcode.cn/problems/house-robber/description/?envType=study-plan-v2&amp;envId=dynamic-programming">198.
打家劫舍 - 力扣（LeetCode）</a></p>
</blockquote>
<h3 id="最长递增子序列">最长递增子序列</h3>
<p>给定一个长度为 N
的数列，求数值严格单调递增的子序列的长度最长是多少。</p>
<p>初始化 <span class="math inline">\(dp[i] = 1, (i=1,..,n)\)</span>
表示以<span class="math inline">\(nums[i]\)</span>
结尾的最长单调递增子序列的长度</p>
<p><span class="math inline">\(dp[i] = \underset{j&lt;i\ and\ nums[j]
&lt; nums[i]}{max}\{dp[j] + 1\}\)</span></p>
<p><span class="math inline">\(ans =
max(dp[1],dp[2],...,dp[n])\)</span></p>
<p>时间复杂度<span class="math inline">\(O(n^2)\)</span></p>
<blockquote>
<p>n大于<span class="math inline">\(1e^5\)</span>时 <span class="math inline">\(O(n^2)\)</span>时间复杂度会导致超时</p>
</blockquote>
<p>定义<span class="math inline">\(dp[i]\)</span>表示长度为<span class="math inline">\(i+1\)</span>的最长递增子序列最后一个数字的最小值</p>
<p>利用<span class="math inline">\(dp\)</span>数组的单调性，通过二分查找找到大于等于<span class="math inline">\(nums[i]\)</span>位置</p>
<p>时间复杂度<span class="math inline">\(O(n^2)\)</span></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1e5</span>+<span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> dp[N], nums[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n; cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n;i++) <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;nums[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(ans == <span class="number">0</span> || dp[ans<span class="number">-1</span>] &lt; nums[i]) dp[ans++] = nums[i];</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">int</span> l = <span class="number">0</span>, r = ans<span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">while</span>(l &lt; r)&#123;</span><br><span class="line">                <span class="keyword">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span>(dp[mid] &gt;= nums[i]) r = mid;</span><br><span class="line">                <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(dp[l] &gt; nums[i]) dp[l] = nums[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="最长公共子序列">最长公共子序列</h3>
<p>给定两个长度分别为 N 和 M 的字符串 A 和 B，求既是 A 的子序列又是 B
的子序列的字符串长度最长是多少。</p>
<p><span class="math inline">\(dp[i][j]\)</span> 表示字符串$A[0:i] <span class="math inline">\(与\)</span>B[0:j]$的最长公共子序列长度</p>
<p>初始化: <span class="math inline">\(dp[i][j] = 0\)</span>
任何序列和空串的最长公共子序列长度都为0</p>
<p>转移方程:</p>
<p>（1）<span class="math inline">\(A[i]=B[i]\)</span> 时，<span class="math inline">\(dp[i][j] = dp[i-1][j-1] + 1\)</span></p>
<p>（2）<span class="math inline">\(A[i]\neq B[i]\)</span> 时，<span class="math inline">\(dp[i][j] = max(dp[i-1][j],
dp[i][j-1])\)</span></p>
<h3 id="编辑距离">编辑距离</h3>
<p>给出字符串A和B，求将A转换成B所使用的最少操作数</p>
<p>对每个字符可进行如下三种操作：</p>
<ul>
<li>插入一个字符</li>
<li>删除一个字符</li>
<li>替换一个字符</li>
</ul>
<p><span class="math inline">\(dp[i][j]\)</span> 表示字符串<span class="math inline">\(A[0:i]\)</span>变成<span class="math inline">\(B[0:j]\)</span>的最少操作数（重点关注各自的最后字符）</p>
<p>初始化： （任何序列和空串的编辑距离均为序列长度）</p>
<ul>
<li><span class="math inline">\(dp[i][0]=i,(i=1,2,..,n)\)</span></li>
<li><span class="math inline">\(dp[0][j]=j,(j=1,2,...,n)\)</span></li>
</ul>
<p>转移方程</p>
<p><span class="math inline">\(if\ A[i]==B[i],\ dp[i][j] =
dp[i-1][j-1]\)</span></p>
<p><span class="math inline">\(else\ dp[i][j]=min(dp[i-1][j],
dp[j][i-1], dp[i-1][j-1]) + 1\)</span></p>
<p>其中</p>
<ul>
<li><span class="math inline">\(dp[i-1][j],dp[i][j-1]\)</span>
可以看成插入或者删除一个字符</li>
<li><span class="math inline">\(dp[i-1][j-1]\)</span>为交换两个字符</li>
</ul>
<h2 id="区间dp">区间DP</h2>
<p>所有的区间dp问题枚举时，第一维通常是枚举区间长度，并且一般 len = 1
时用来初始化，枚举从 len = 2 开始；第二维枚举起点 i （右端点 j
自动获得，j = i + len - 1）</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> len = <span class="number">1</span>; len &lt;= n; len++) &#123;         <span class="comment">// 区间长度</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i + len - <span class="number">1</span> &lt;= n; i++) &#123; <span class="comment">// 枚举起点</span></span><br><span class="line">        <span class="keyword">int</span> j = i + len - <span class="number">1</span>;                 <span class="comment">// 区间终点</span></span><br><span class="line">        <span class="keyword">if</span> (len == <span class="number">1</span>) dp[i][j] = 初始值</span><br><span class="line">		<span class="keyword">else</span>&#123;</span><br><span class="line">        	 <span class="keyword">for</span> (<span class="keyword">int</span> k = i; k &lt; j; k++) &#123;        <span class="comment">// 枚举分割点，构造状态转移方程</span></span><br><span class="line">                dp[i][j] = <span class="built_in">min</span>(dp[i][j], dp[i][k] + dp[k + <span class="number">1</span>][j] + w[i][j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="石头合并">石头合并</h3>
<p>合并 N 堆石子，每次只能合并相邻的两堆石子，求最小代价</p>
<p>（最后一次合并一定是左边连续的一部分和右边连续的一部分进行合并）</p>
<p><span class="math inline">\(dp[i][j]\)</span> 表示将 𝑖 到 𝑗
这一段石子合并成一堆的方案的集合，属性 Min</p>
<p>利用前缀和计算区间和</p>
<p>转移方程：</p>
<ol type="1">
<li><p><span class="math inline">\(i&lt;j\)</span> 时，<span class="math inline">\(dp[i][j] = \underset{i\le k\le j-1}{min}
\{dp[i][k] + dp[k+1][j] + s[j] - s[i-1]\}\)</span></p></li>
<li><p><span class="math inline">\(i=j\)</span> 时， <span class="math inline">\(dp[i][j] = 0\)</span> （合并一堆石子代价为
0）</p></li>
</ol>
<h2 id="计数类dp">计数类DP</h2>
<h3 id="整数划分">整数划分</h3>
<p>一个正整数n可以表示成若干个正整数之和，形如：<span class="math inline">\(n=n_1+n_2+…+n_k\)</span>，其中<span class="math inline">\(n_1≥n_2≥…≥n_k,k≥1\)</span>。</p>
<p>这样的一种表示称为正整数n的一种划分。给定一个正整数n，求出n共有多少种不同的划分方法。</p>
<blockquote>
<p>完全背包的解法 <span class="math inline">\(O(n^3)\)</span></p>
</blockquote>
<p>状态定义: <span class="math inline">\(dp[i][j]\)</span>
从1-i中选体积刚好为j的方案数量,</p>
<p>状态计算： <span class="math inline">\(dp[i][j] = \underset{k*i\le
j}{\sum} \{dp[i-1][j-i*k]\}\)</span></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240730131454674.png" alt="image-20240730131454674">
<figcaption aria-hidden="true">image-20240730131454674</figcaption>
</figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> dp[N][N]; <span class="comment">// dp[i][j] 从1-i中选体积刚好为j的方案数量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; i++) dp[i][<span class="number">0</span>] = <span class="number">1</span>;  <span class="comment">// 一个数也不选也是一种方案</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; j - k * i &gt;= <span class="number">0</span>; k++)&#123;</span><br><span class="line">            dp[i][j] += dp[i<span class="number">-1</span>][j - i*k];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>完全背包优化 <span class="math inline">\(O(n^2)\)</span></p>
</blockquote>
<p><span class="math inline">\(dp[i][j] =
dp[i-1][j]+dp[i-1][j-i]+dp[i-1][j-2i]+dp[i-1][j-3i]+...+dp[i-1][j-ki]\)</span></p>
<p><span class="math inline">\(dp[i][j-i] = \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \
dp[i-1][j-i]+dp[i-1][j-2i]+dp[i-1][j-3i]+...+dp[i-1][j-ki]\)</span></p>
<p>将<span class="math inline">\(dp[i][j-i]\)</span>代入 <span class="math inline">\(dp[i][j]\)</span> 得到 <span class="math inline">\(dp[i][j] = dp[i-1][j]+dp[i][j-i]\)</span></p>
<p>优化一维：<span class="math inline">\(dp[j] =
dp[j]+dp[j-i]\)</span></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> dp[N];</span><br><span class="line">dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt;= n; j++)&#123;</span><br><span class="line">        dp[j] = dp[j] + dp[j - i]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>计数DP方案</p>
</blockquote>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240730135056199.png" alt="image-20240730135056199">
<figcaption aria-hidden="true">image-20240730135056199</figcaption>
</figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> dp[N][N];</span><br><span class="line">dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= i; j++)&#123;</span><br><span class="line">        dp[i][j] = (dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + dp[i - j][j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++) res += dp[n][i];</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="数位统计dp">数位统计DP</h2>
<h2 id="状态压缩dp">状态压缩DP</h2>
<h2 id="树形dp">树形DP</h2>
<h2 id="记忆化搜索">记忆化搜索</h2>
]]></content>
      <categories>
        <category>算法</category>
        <category>DP</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>基础算法</title>
    <url>/2024/07/30/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="基础算法">基础算法</h1>
<h2 id="常用头文件">常用头文件</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;bitset&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cctype&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;climits&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> pair &lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; pii;</span><br><span class="line"><span class="keyword">typedef</span> vector &lt;<span class="keyword">int</span>&gt; vi;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXN=<span class="number">1e5</span>+<span class="number">5</span>, MAXM=<span class="number">4e5</span>+<span class="number">5</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF=<span class="number">0x3f3f3f3f</span>;</span><br></pre></td></tr></table></figure>
<h2 id="排序">排序</h2>
<h3 id="快速排序">快速排序</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span> nums[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> x = nums[l + r &gt;&gt; <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> i = l - <span class="number">1</span>, j = r + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt; j)&#123;</span><br><span class="line">        <span class="keyword">do</span> i++; <span class="keyword">while</span>(nums[i] &lt; x); </span><br><span class="line">        <span class="keyword">do</span> j--; <span class="keyword">while</span>(nums[j] &gt; x);</span><br><span class="line">        <span class="keyword">if</span>(i &lt; j) <span class="built_in">swap</span>(nums[i], nums[j]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">quick_sort</span>(nums, l, j);</span><br><span class="line">    <span class="built_in">quick_sort</span>(nums, j + <span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/787/">785. 快速排序 -
AcWing题库</a></p>
</blockquote>
<h3 id="归并排序">归并排序</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span> nums[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l &gt;= r) <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> mid = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">merge_sort</span>(nums, l, mid);</span><br><span class="line">    <span class="built_in">merge_sort</span>(nums, mid + <span class="number">1</span>, r);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> i = l, j = mid + <span class="number">1</span>, idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid &amp;&amp; j &lt;= r)&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums[i] &lt; nums[j]) tmp[idx++] = nums[i++];</span><br><span class="line">        <span class="keyword">else</span> tmp[idx++] = nums[j++];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= mid) tmp[idx++] = nums[i++];</span><br><span class="line">    <span class="keyword">while</span>(j &lt;= r) tmp[idx++] = nums[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = l, j = <span class="number">0</span>; i &lt;= r; i++, j++) nums[i] = tmp[j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/789/">787. 归并排序 -
AcWing题库</a></p>
</blockquote>
<h2 id="二分查找">二分查找</h2>
<h3 id="整数二分查找">整数二分查找</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bSearch</span><span class="params">(<span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)&#123;</span><br><span class="line">        <span class="keyword">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;    <span class="comment">// check()判断mid是否满足性质</span></span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bSearch</span><span class="params">(<span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)&#123;</span><br><span class="line">        <span class="keyword">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) l = mid;</span><br><span class="line">        <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/791/">789. 数的范围 -
AcWing题库</a></p>
<p><a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/description/">34.
在排序数组中查找元素的第一个和最后一个位置 - 力扣（LeetCode）</a></p>
</blockquote>
<h3 id="浮点数二分查找">浮点数二分查找</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">double</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">bSearch</span><span class="params">(<span class="keyword">double</span> l, <span class="keyword">double</span> r)</span></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> eps = <span class="number">1e-6</span>;   <span class="comment">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class="line">    <span class="keyword">while</span> (r - l &gt; eps)&#123;</span><br><span class="line">        <span class="keyword">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://www.acwing.com/problem/content/792/">790.
数的三次方根 - AcWing题库</a></p>
<h2 id="高精度计算">高精度计算</h2>
<h3 id="高精度加法">高精度加法</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C = A + B, A &gt;= 0, B &gt;= 0</span></span><br><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">addLargeNum</span><span class="params">(vector&lt;<span class="keyword">int</span>&gt; &amp;A, vector&lt;<span class="keyword">int</span>&gt; &amp;B)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (A.<span class="built_in">size</span>() &lt; B.<span class="built_in">size</span>()) <span class="keyword">return</span> <span class="built_in">addLargeNum</span>(B, A);</span><br><span class="line"></span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; C;</span><br><span class="line">    <span class="keyword">int</span> t = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>(); i ++ )&#123;</span><br><span class="line">        t += A[i];</span><br><span class="line">        <span class="keyword">if</span> (i &lt; B.<span class="built_in">size</span>()) t += B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(t % <span class="number">10</span>);</span><br><span class="line">        t /= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (t) C.<span class="built_in">push_back</span>(t);</span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/793/">791. 高精度加法
- AcWing题库</a></p>
</blockquote>
<h3 id="高精度减法">高精度减法</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C = A - B, 满足A &gt;= B, A &gt;= 0, B &gt;= 0， 判断大小 长度-字典序</span></span><br><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">sub</span><span class="params">(vector&lt;<span class="keyword">int</span>&gt; &amp;A, vector&lt;<span class="keyword">int</span>&gt; &amp;B)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; C;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, t = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>(); i ++ )&#123;</span><br><span class="line">        t = A[i] - t;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; B.<span class="built_in">size</span>()) t -= B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>((t + <span class="number">10</span>) % <span class="number">10</span>);</span><br><span class="line">        <span class="keyword">if</span> (t &lt; <span class="number">0</span>) t = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> t = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (C.<span class="built_in">size</span>() &gt; <span class="number">1</span> &amp;&amp; C.<span class="built_in">back</span>() == <span class="number">0</span>) C.<span class="built_in">pop_back</span>();</span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="高精度乘低精度">高精度乘低精度</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C = A * b, A &gt;= 0, b &gt;= 0</span></span><br><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">mul</span><span class="params">(vector&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; C;</span><br><span class="line">    <span class="keyword">int</span> t = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; A.<span class="built_in">size</span>() || t; i ++ )&#123;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; A.<span class="built_in">size</span>()) t += A[i] * b;</span><br><span class="line">        C.<span class="built_in">push_back</span>(t % <span class="number">10</span>);</span><br><span class="line">        t /= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (C.<span class="built_in">size</span>() &gt; <span class="number">1</span> &amp;&amp; C.<span class="built_in">back</span>() == <span class="number">0</span>) C.<span class="built_in">pop_back</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="高精度除以低精度">高精度除以低精度</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// A / b = C ... r, A &gt;= 0, b &gt; 0</span></span><br><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">div</span><span class="params">(vector&lt;<span class="keyword">int</span>&gt; &amp;A, <span class="keyword">int</span> b, <span class="keyword">int</span> &amp;r)</span></span>&#123;</span><br><span class="line">    vector&lt;<span class="keyword">int</span>&gt; C;</span><br><span class="line">    r = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = A.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i -- )&#123;</span><br><span class="line">        r = r * <span class="number">10</span> + A[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(r / b);</span><br><span class="line">        r %= b;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">reverse</span>(C.<span class="built_in">begin</span>(), C.<span class="built_in">end</span>());</span><br><span class="line">    <span class="keyword">while</span> (C.<span class="built_in">size</span>() &gt; <span class="number">1</span> &amp;&amp; C.<span class="built_in">back</span>() == <span class="number">0</span>) C.<span class="built_in">pop_back</span>();</span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="前缀和差分">前缀和&amp;差分</h2>
<h3 id="一维前缀和">一维前缀和</h3>
<p>已知一维数组，计算某个区间的和</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N];	<span class="comment">// 原数组  下标从1开始，方便计算</span></span><br><span class="line"><span class="keyword">int</span> S[N];	<span class="comment">// 前缀和数组</span></span><br><span class="line"><span class="keyword">int</span> l, r; 	<span class="comment">// 区间的左右坐标</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// S[i] = a[1] + a[2] + ... a[i]</span></span><br><span class="line">S[i] = S[i<span class="number">-1</span>] + a[i];</span><br><span class="line"></span><br><span class="line"><span class="comment">//a[l] + ... + a[r] = S[r] - S[l - 1]</span></span><br><span class="line">sum = S[r] - S[l]	</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://leetcode.cn/problems/maximum-subarray/description/?envType=study-plan-v2&amp;envId=top-100-liked">53.
最大子数组和 - 力扣（LeetCode）</a></p>
<p><a href="https://leetcode.cn/problems/subarray-sum-equals-k/description/?envType=study-plan-v2&amp;envId=top-100-liked">560.
和为 K 的子数组 - 力扣（LeetCode）</a></p>
</blockquote>
<h3 id="二维前缀和">二维前缀和</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N][N];</span><br><span class="line"><span class="keyword">int</span> S[N][N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// S[i, j] = 第i行j列格子左上部分所有元素的和</span></span><br><span class="line">S[i][j] = S[i<span class="number">-1</span>][j] + S[i][j<span class="number">-1</span>] - S[i<span class="number">-1</span>][j<span class="number">-1</span>] + S[i][j];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以(x1, y1)为左上角，(x2, y2)为右下角的子矩阵的和为：</span></span><br><span class="line"><span class="comment">// S[x2, y2] - S[x1 - 1, y2] - S[x2, y1 - 1] + S[x1 - 1, y1 - 1]</span></span><br><span class="line">sum = S[x2][y2] - S[x2][y1<span class="number">-1</span>] - S[x1<span class="number">-1</span>][y2] + S[x1<span class="number">-1</span>][y1<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="一维差分">一维差分</h3>
<p>用于可以让一个序列中某个区间内的所有值均加上或减去一个常数。相当于a是差分数组的前缀和</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N];	<span class="comment">// 原数组 	下标从1开始，方便计算</span></span><br><span class="line"><span class="keyword">int</span> B[N];	<span class="comment">// 差分数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算差分</span></span><br><span class="line">B[i] = a[i] - a[i<span class="number">-1</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 给区间[l, r]中的每个数加上c：B[l] += c, B[r + 1] -= c</span></span><br><span class="line">B[l] += c; </span><br><span class="line">B[r + <span class="number">1</span>] -= c;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 还原数组</span></span><br><span class="line">a[i] = a[i<span class="number">-1</span>] + B[i];</span><br></pre></td></tr></table></figure>
<h3 id="二维差分">二维差分</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[N][N];</span><br><span class="line"><span class="keyword">int</span> B[N][N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 给以(x1, y1)为左上角，(x2, y2)为右下角的子矩阵中的所有元素加上c：</span></span><br><span class="line"><span class="comment">// S[x1, y1] += c, S[x2 + 1, y1] -= c, S[x1, y2 + 1] -= c, S[x2 + 1, y2 + 1] += c</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> x1, <span class="keyword">int</span> y1, <span class="keyword">int</span> x2, <span class="keyword">int</span> y2, <span class="keyword">int</span> c)</span></span>&#123;</span><br><span class="line">    B[x1][y1] += c;</span><br><span class="line">    B[x2+<span class="number">1</span>][y1] -= c;</span><br><span class="line">    B[x1][y2+<span class="number">1</span>] -= c;</span><br><span class="line">    B[x2+<span class="number">1</span>][y2+<span class="number">1</span>] += c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建差分数组时使用 insert(i,j,i,j,a[i][j])</span></span><br><span class="line"><span class="comment">// 还原时要记住 a是B的前缀和数组</span></span><br><span class="line">a[i][j] = a[i<span class="number">-1</span>][j] + a[i][j<span class="number">-1</span>] - a[i<span class="number">-1</span>][j<span class="number">-1</span>] + B[i][j];</span><br></pre></td></tr></table></figure>
<h2 id="位运算">位运算</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">求n的第k位数字: n &gt;&gt; k &amp; <span class="number">1</span></span><br><span class="line">返回n的最后一位<span class="number">1</span>：<span class="built_in">lowbit</span>(n) = n &amp; -n</span><br></pre></td></tr></table></figure>
<h2 id="双指针问题">双指针问题</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 具体逻辑</span></span><br><span class="line">    <span class="keyword">while</span> (j &lt; i &amp;&amp; <span class="built_in">check</span>(i, j)) j ++ ;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 具体问题的逻辑</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">不一定i, j都从一个方向走，可以双向奔赴♥！</span></span><br><span class="line"><span class="comment">常见问题分类：</span></span><br><span class="line"><span class="comment">    (1) 对于一个序列，用两个指针维护一段区间</span></span><br><span class="line"><span class="comment">    (2) 对于两个序列，维护某种次序，比如归并排序中合并两个有序序列的操作</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/801/">799.
最长连续不重复子序列 - AcWing题库</a></p>
<p><a href="https://www.acwing.com/problem/content/802/">800.
数组元素的目标和 - AcWing题库</a></p>
</blockquote>
<h2 id="离散化">离散化</h2>
<p>感觉使用map更加方便</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="keyword">int</span>&gt; alls; <span class="comment">// 存储所有待离散化的值</span></span><br><span class="line"><span class="built_in">sort</span>(alls.<span class="built_in">begin</span>(), alls.<span class="built_in">end</span>()); <span class="comment">// 将所有值排序</span></span><br><span class="line">alls.<span class="built_in">erase</span>(<span class="built_in">unique</span>(alls.<span class="built_in">begin</span>(), alls.<span class="built_in">end</span>()), alls.<span class="built_in">end</span>());   <span class="comment">// 去掉重复元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 二分求出x对应的离散化的值</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> <span class="comment">// 找到第一个大于等于x的位置</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>, r = alls.<span class="built_in">size</span>() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (alls[mid] &gt;= x) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> r + <span class="number">1</span>; <span class="comment">// 映射到1, 2, ...n</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="区间合并">区间合并</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将所有存在交集的区间合并</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(vector&lt;PII&gt; &amp;segs)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vector&lt;PII&gt; res;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">sort</span>(segs.<span class="built_in">begin</span>(), segs.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> st = <span class="number">-2e9</span>, ed = <span class="number">-2e9</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> seg : segs)</span><br><span class="line">        <span class="keyword">if</span> (ed &lt; seg.first)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (st != <span class="number">-2e9</span>) res.<span class="built_in">push_back</span>(&#123;st, ed&#125;);</span><br><span class="line">            st = seg.first, ed = seg.second;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> ed = <span class="built_in">max</span>(ed, seg.second);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (st != <span class="number">-2e9</span>) res.<span class="built_in">push_back</span>(&#123;st, ed&#125;);</span><br><span class="line"></span><br><span class="line">    segs = res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ======== by Xucong 贪心 ========</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Interval</span>&#123;</span>   <span class="comment">// 可以使用vector代替，sort比较vector的第一个元素</span></span><br><span class="line">    <span class="keyword">int</span> l, r;</span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> Interval &amp; t)&#123;</span><br><span class="line">        <span class="keyword">return</span> l &lt; t.l;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">vector&lt;Interval&gt; vec;</span><br><span class="line"><span class="built_in">sort</span>(vec.<span class="built_in">begin</span>(), vec.<span class="built_in">end</span>());  <span class="comment">// 按照左端点排序</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;vec.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">    <span class="keyword">int</span> right = vec[i].r;</span><br><span class="line">    <span class="keyword">int</span> j = i+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(j&lt;vec.<span class="built_in">size</span>() &amp;&amp; vec[j].l&lt;=right)&#123;</span><br><span class="line">        right = <span class="built_in">max</span>(vec[j].r, right);</span><br><span class="line">        j++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 具体逻辑 [vec[i], .., vec[j]] 合并到一个区间</span></span><br><span class="line">    i = j<span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/805/">803. 区间合并 -
AcWing题库</a></p>
<p><a href="https://leetcode.cn/problems/merge-intervals/?envType=study-plan-v2&amp;envId=top-100-liked">56.
合并区间 - 力扣（LeetCode）</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法</category>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title>唐代诗人</title>
    <url>/2021/10/05/%E5%94%90%E4%BB%A3%E8%AF%97%E4%BA%BA/</url>
    <content><![CDATA[<h1 id="唐代诗人">唐代诗人</h1>
<h2 id="诗种类">诗种类</h2>
<p><strong>古诗体：</strong></p>
<ul>
<li>没有严格的平仄、句数、字数、韵律要求。</li>
<li>以四言、五言、七言以及杂言为主。</li>
</ul>
<p><strong>近体诗（今体诗 格律诗）：</strong></p>
<ul>
<li>产生于隋唐，盛行于唐。</li>
<li>一种讲究平仄、对仗、押韵的诗歌体。</li>
<li>律诗和绝句的通称。</li>
</ul>
<p><strong>律诗：</strong></p>
<ul>
<li>四联（颔联首联颈联尾联）</li>
<li>颔联 颈联需要对仗、对偶。</li>
<li>严格的用韵要求</li>
</ul>
<p><strong>绝句：</strong>没有严格的用韵要求</p>
<h2 id="杜甫盛唐">杜甫（盛唐）</h2>
<ul>
<li><p>杜少凌，杜工部，诗圣，诗歌界的孔子。</p></li>
<li><p>现实主义诗人。</p></li>
<li><p>诗歌深刻的反映了唐朝由盛转衰过程中的社会风貌和时代苦难。</p></li>
<li><p>其诗被被称为诗史。</p></li>
</ul>
<p><strong>诗歌风格：</strong>沉郁顿挫</p>
<p><strong>作品集：</strong>《杜少凌集》，《杜工部集》。</p>
<p><strong>作品：</strong>三吏三别、《新安吏》、《石壕吏》、《潼关吏》、《新婚别》、《无家别》、《垂老别》。</p>
<ul>
<li><p>《春望》感时花溅泪，恨别鸟惊心。 双关、五言律诗</p></li>
<li><p>《登高》老来多病，孤苦无依。 七言律诗</p></li>
</ul>
<p><strong>边塞诗派：</strong>高适、王昌龄、岑参。</p>
<h2 id="高适">高适</h2>
<ul>
<li>子达夫</li>
</ul>
<p><strong>作品集：</strong>《高常侍集》</p>
<p><strong>作品：</strong>别董大、燕歌行。</p>
<h2 id="王昌龄">王昌龄</h2>
<ul>
<li>字少伯</li>
</ul>
<p><strong>世称：</strong>王江宁、王龙标</p>
<p>“七绝圣手”、“诗歌夫子 王江宁”</p>
<p><strong>作品集：</strong>《王江宁集》</p>
<h2 id="岑参岑家州">岑参（岑家州）</h2>
<p><strong>作品集：</strong>《岑家州集》</p>
<p><strong>作品：</strong>白雪歌送武判官归京</p>
<h2 id="白居易中唐">白居易（中唐）</h2>
<ul>
<li><p>字乐天、号乐山居士、香山居士、江州司马、醉吟先生。</p></li>
<li><p>诗魔、诗王。</p></li>
<li><p>现代主义诗人。</p></li>
</ul>
<p><strong>主张：</strong>文章合为时而著，歌诗合为时而作。</p>
<p><strong>诗歌风格：</strong>平易晓畅（通俗易懂）</p>
<p><strong>作品集：</strong>《白氏长庆集》 “现存诗歌为唐人之冠”</p>
<p><strong>作品：</strong>琵琶行、长恨歌、卖炭翁、忆江南。</p>
<p><strong>新乐府：</strong>以新题写时事，不在以乐为标准。
首创为杜甫，“元白”倡导。</p>
<h2 id="韩愈">韩愈</h2>
<ul>
<li><p>字退之，韩昌龄、韩文公。</p></li>
<li><p>崇尚秦汉散文、反对六朝骈俪文风。</p></li>
</ul>
<p><strong>古文运动：</strong>排斥道教、佛教、崇尚儒学。</p>
<p><strong>文学主张：</strong>言之有物、辞必己出、惟陈言之务去。
文道合一、文以顺从、文以载道。</p>
<p><strong>地位：</strong>唐宋散文八大家之一
、千古文章四大家之一<strong>（柳宗元、韩愈、苏轼、欧阳修）</strong></p>
<p><strong>作品集：</strong>《昌龄先生集》</p>
<h2 id="柳宗元">柳宗元</h2>
<ul>
<li>字子厚、柳河东。</li>
<li>唐宋八大家之一 千古文章四大家之一</li>
<li>永州八记之首——始得西山宴游记</li>
</ul>
<p><strong>主张：</strong>针砭时弊，反对宦官专权、割据。反映民生疾苦，主张人以明道。</p>
<p><strong>寓言三则：</strong>哀溺之序、黔之驴、蝜蝂传.</p>
<p><strong>作品集：</strong>《柳河东集》</p>
<h2 id="刘禹锡">刘禹锡</h2>
<ul>
<li><p>字梦得、刘宾客。</p></li>
<li><p>诗豪。 “刘白”——刘禹锡和白居易</p></li>
</ul>
<p><strong>风格：</strong>清新明朗</p>
<p>作品集：《刘宾客集》、《刘禹锡集》、《刘梦得集》</p>
<p><strong>作品：</strong>陋室铭 秋词 西山塞怀古</p>
<p>怀古或咏史诗都是用典和对比</p>
<h2 id="李商隐">李商隐</h2>
<ul>
<li><p>字义山、号玉溪生、樊南生。</p></li>
<li><p>“小李杜”——李商隐、杜牧。</p></li>
<li><p>无题诗的开创人</p></li>
</ul>
<p><strong>风格：</strong>婉转缠绵、缠绵悱恻</p>
<p><strong>作品诗：</strong>《李义山集》、《樊南文集》</p>
<h2 id="杜牧">杜牧</h2>
<ul>
<li>字牧之</li>
</ul>
<p><strong>作品集：</strong>《樊川文集》</p>
<p><strong>作品：</strong>清明、阿房宫赋、泊秦淮、赤壁。</p>
]]></content>
      <categories>
        <category>HSY</category>
      </categories>
      <tags>
        <tag>语文</tag>
      </tags>
  </entry>
  <entry>
    <title>交叉熵损失函数</title>
    <url>/2024/07/31/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="交叉熵损失函数">交叉熵损失函数</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># dim=0 按列进行计算，dim=1 按行进行计算</span></span><br><span class="line"><span class="comment"># softmax ——&gt; log_softmax --&gt; NLLLOSS--&gt; cross_entropy</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[ 1.3962, -0.2568, -0.7142,  1.1941,  0.5695],</span></span><br><span class="line"><span class="string">        [-0.7136, -1.0663,  1.7642,  0.5170, -0.1858],</span></span><br><span class="line"><span class="string">        [ 0.0424, -0.3354, -0.9049,  0.6952,  1.3032]], requires_grad=True)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">y = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([4, 3, 0])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="softmax">Softmax</h2>
<p>Softmax是网络输出后第一步操作，其公式可表示为：</p>
<p><span class="math display">\[
\frac{e^{v_{y_n}}}{\sum_{m=1}^K e^{v_m}}
\]</span>
由于网络的输出有正有负，有大有小，Softmax主要是将输出概率标准化到 <span class="math inline">\([0,1]\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 原理： 1、所有值取指数  2、每个值分别除以总数（三个值相加），得到的三个值相加为一，构造类似概率</span></span><br><span class="line">a = torch.exp(x)</span><br><span class="line">a /= torch.unsqueeze(torch.<span class="built_in">sum</span>(a, dim=<span class="number">1</span>),dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">b = torch.softmax(x, dim=<span class="number">1</span>)  <span class="comment"># softmax 得到的值,0 - 1 之间</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手算结果：<span class="subst">&#123;a&#125;</span>, \nsoftmax结果：<span class="subst">&#123;b&#125;</span>&quot;</span>)  <span class="comment"># 验证 torch.softmax 计算过程</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">手算结果：tensor([[0.3911, 0.3240, 0.0895, 0.1785, 0.0168],</span></span><br><span class="line"><span class="string">        [0.3382, 0.1688, 0.3726, 0.0407, 0.0798],</span></span><br><span class="line"><span class="string">        [0.2213, 0.1558, 0.2684, 0.3061, 0.0484]], grad_fn=&lt;DivBackward0&gt;), </span></span><br><span class="line"><span class="string">softmax结果：tensor([[0.3911, 0.3240, 0.0895, 0.1785, 0.0168],</span></span><br><span class="line"><span class="string">        [0.3382, 0.1688, 0.3726, 0.0407, 0.0798],</span></span><br><span class="line"><span class="string">        [0.2213, 0.1558, 0.2684, 0.3061, 0.0484]], grad_fn=&lt;SoftmaxBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="log_softmax">Log_Softmax</h2>
<p>在Softmax之后进行log(以e为底)，其公式可表示为： <span class="math display">\[
\log \left(\frac{e^{v_{y_n}}}{\sum_{m=1}^K e^{v_m}}\right)
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在softmax的基础上对每个元素求log</span></span><br><span class="line">c = torch.log(a)  <span class="comment"># 给每个元素取对数</span></span><br><span class="line">d = torch.log_softmax(x, dim=<span class="number">1</span>)  <span class="comment"># 一定为负值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手算结果：<span class="subst">&#123;c&#125;</span>, \nlog_softmax结果：<span class="subst">&#123;d&#125;</span>&quot;</span>)  <span class="comment"># 验证 torch.log_softmax 计算过程</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">手算结果：tensor([[0.3911, 0.3240, 0.0895, 0.1785, 0.0168],</span></span><br><span class="line"><span class="string">        [0.3382, 0.1688, 0.3726, 0.0407, 0.0798],</span></span><br><span class="line"><span class="string">        [0.2213, 0.1558, 0.2684, 0.3061, 0.0484]], grad_fn=&lt;DivBackward0&gt;), </span></span><br><span class="line"><span class="string">softmax结果：tensor([[0.3911, 0.3240, 0.0895, 0.1785, 0.0168],</span></span><br><span class="line"><span class="string">        [0.3382, 0.1688, 0.3726, 0.0407, 0.0798],</span></span><br><span class="line"><span class="string">        [0.2213, 0.1558, 0.2684, 0.3061, 0.0484]], grad_fn=&lt;SoftmaxBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="nllloss">NLLLoss</h2>
<p>NLLLoss损失，即对Log_Softmax之后的结果，将样本标签对应位置的数值进行相加，再除以样本量，最后再去负号，因为log之后是负数，损失需要转换为正值。</p>
<p>对 $[− 1.5425 , − 1.4425 , − 1.3425 , − 1.2425 ] <span class="math inline">\(和\)</span> [ − 1.3863 , − 1.3863 , − 1.3863 , −
1.3863 ] <span class="math inline">\(标签对应位置\)</span>target = [2,
3]<span class="math inline">\(上的数值相加除样本数量再取负，即：\)</span>$
-=1.3644 $$</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \operatorname{NLL}(\log (\operatorname{softmax}(\text { input })),
\text { target })=-\Sigma_{\mathrm{i}=1}^{\mathrm{n}} \text { OneHot
}(\text { target })_{\mathrm{i}} \times \log
\left(\operatorname{softmax}(\text { input })_{\mathrm{i}}\right)
(\text { input } \left.\in \mathbf{R}^{\mathbf{m} \times
\mathbf{n}}\right)
\end{aligned}
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将y作为index，对应x的数相加，除以样本量，取负</span></span><br><span class="line">e = - torch.<span class="built_in">sum</span>(c[np.arange(<span class="built_in">len</span>(y)), y]) /  <span class="built_in">len</span>(y)</span><br><span class="line"></span><br><span class="line">nll_loss = torch.nn.NLLLoss()</span><br><span class="line">f = nll_loss(c, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;手算结果：<span class="subst">&#123;e&#125;</span>, NLLLoss结果<span class="subst">&#123;f&#125;</span>&quot;</span>)  <span class="comment"># 验证 torch.log_softmax 计算过程</span></span><br><span class="line"><span class="comment"># 手算结果：1.8381218910217285, NLLLoss结果1.8381218910217285</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="crossentropyloss">CrossEntropyLoss</h2>
<p><span class="math inline">\(CrossEntropy\_Loss = Softmax + Log +
NLLLoss = Log\_Softmax + NLLLoss\)</span></p>
<p><span class="math display">\[
-\frac{1}{N} \sum_{n=1}^N \log \left(\frac{e^{v_{y_n}}}{\sum_{m=1}^K
e^{v_m}}\right)
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cross_loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">g = cross_loss(x, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CrossEntropyLoss：<span class="subst">&#123;g&#125;</span>, NLLLoss结果<span class="subst">&#123;f&#125;</span>&quot;</span>)  <span class="comment"># 验证 torch.CrossEntropyLoss 计算过程</span></span><br><span class="line"><span class="comment"># CrossEntropyLoss：1.8381218910217285, NLLLoss结果1.8381218910217285</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>搜索与图论</title>
    <url>/2024/07/30/%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%9B%BE%E8%AE%BA/</url>
    <content><![CDATA[<h1 id="搜索与图论">搜索与图论</h1>
<h2 id="树和图的存储">树和图的存储</h2>
<p>树是一种特殊的图，与图的存储方式相同。
对于无向图中的边ab，存储两条有向边a-&gt;b, b-&gt;a。
因此我们可以只考虑有向图的存储。</p>
<ol type="1">
<li><p>邻接矩阵：g[a][b] 存储边a-&gt;b</p></li>
<li><p>邻接表 (前向星)：</p></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1e5</span>+<span class="number">10</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> to, ne, w;</span><br><span class="line">    </span><br><span class="line">&#125;node[N*<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> head[N], idx;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加一条边 a -w-&gt; b</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addEdge</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> w)</span></span>&#123;</span><br><span class="line">    node[idx].to = b;</span><br><span class="line">    node[idx].ne = head[a];</span><br><span class="line">    node[idx].w = w;</span><br><span class="line">    head[a] = idx++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化</span></span><br><span class="line">idx = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">memset</span>(head, <span class="number">-1</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(head));</span><br></pre></td></tr></table></figure>
<h2 id="树与图的遍历">树与图的遍历</h2>
<p>时间复杂度 O(n+m) n 表示点数， m表示边数</p>
<h3 id="深度优先遍历">深度优先遍历</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> vis[N]; <span class="comment">// 存储访问情况</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> root)</span></span>&#123;</span><br><span class="line">    vis[root] = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = head[root]; i!=<span class="number">-1</span>; i = node[i].ne)&#123;</span><br><span class="line">        <span class="keyword">int</span> j = node[i].to;</span><br><span class="line">        <span class="keyword">if</span>(!vis[j]) <span class="built_in">dfs</span>(j);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="广度优先搜索">广度优先搜索</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">queue&lt;<span class="keyword">int</span>&gt; que;</span><br><span class="line">vis[<span class="number">1</span>] = <span class="literal">true</span>; <span class="comment">// 表示1号点已经被遍历过</span></span><br><span class="line">q.<span class="built_in">push</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (que.<span class="built_in">size</span>())&#123;</span><br><span class="line">    <span class="keyword">int</span> cur = que.<span class="built_in">front</span>();</span><br><span class="line">    que.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = head[cur]; i != <span class="number">-1</span>; i = node[i].ne)&#123;</span><br><span class="line">        <span class="keyword">int</span> j = node[i].to;</span><br><span class="line">        <span class="keyword">if</span> (!vis[j])&#123;</span><br><span class="line">            vis[j] = <span class="literal">true</span>; <span class="comment">// 表示点j已经被遍历过</span></span><br><span class="line">            que.<span class="built_in">push</span>(j);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="拓扑排序">拓扑排序</h3>
<p>时间复杂度 O(n+m), n表示点数，m表示边数</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">queue&lt;<span class="keyword">int</span>&gt; que;</span><br><span class="line"><span class="comment">// d[i] 存储节点i的入度</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n; i++) </span><br><span class="line">    <span class="keyword">if</span>(d[i] == <span class="number">0</span>) que.<span class="built_in">push</span>(i);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(!que.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    <span class="keyword">int</span> cur = que.<span class="built_in">front</span>();</span><br><span class="line">    que.<span class="built_in">pop</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = head[cur]; i != <span class="number">-1</span>; i = node[i].ne)&#123;</span><br><span class="line">        <span class="keyword">int</span> j = node[i].to;</span><br><span class="line">        <span class="keyword">if</span>(-- d[j] == <span class="number">0</span>) que.<span class="built_in">push</span>(j);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最短路径">最短路径</h2>
<h3 id="dijkstra求最短路径">Dijkstra求最短路径</h3>
<p>单源最短路径</p>
<p>时间复杂度 O(mlogn)，n点数，m边数</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; PII;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 链式前向星</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> to, ne, w;</span><br><span class="line">&#125;node[N];</span><br><span class="line"><span class="keyword">int</span> head[N], idx;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 表示距离以及是否确定最短距离</span></span><br><span class="line"><span class="keyword">int</span> dis[N], vis[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dijkstra</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">memset</span>(dis, <span class="number">0x3f</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(dis));</span><br><span class="line">    dis[a] = <span class="number">0</span>;</span><br><span class="line">    priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap;</span><br><span class="line">    heap.<span class="built_in">push</span>(&#123;<span class="number">0</span>, a&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(heap.<span class="built_in">size</span>())&#123;</span><br><span class="line">        <span class="keyword">auto</span> t = heap.<span class="built_in">top</span>(); heap.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">int</span> cur = t.second;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(vis[cur]) <span class="keyword">continue</span>;</span><br><span class="line">        vis[cur] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = head[cur]; i!=<span class="number">-1</span>; i=node[i].ne)&#123;</span><br><span class="line">            <span class="keyword">int</span> j = node[i].to;</span><br><span class="line">            <span class="keyword">if</span>(dis[j] &gt; dis[cur] + node[i].w)&#123;</span><br><span class="line">                dis[j] = dis[cur] + node[i].w;</span><br><span class="line">                heap.<span class="built_in">push</span>(&#123;dis[j], j&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(dis[b] == <span class="number">0x3f3f3f3f</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> dis[b];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="bellman-ford算法">Bellman-Ford算法</h3>
<p>时间复杂度 O(nm), n表示点数，m表示边数</p>
<h3 id="spfa-算法队列优化的bellman-ford算法">Spfa
算法（队列优化的Bellman-Ford算法）</h3>
<p>时间复杂度 平均情况下 O(m)，最坏情况下 O(nm),
n表示点数，m表示边数</p>
<h3 id="spfa判断图中是否存在负环">Spfa判断图中是否存在负环</h3>
<p>时间复杂度是 O(nm), n表示点数，m表示边数</p>
<h3 id="floyd算法">Floyd算法</h3>
<p>多元最短路径</p>
<p>时间复杂度是 O(n3), n表示点数</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 初始化邻接矩阵每条边权为无穷大</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j ++ )</span><br><span class="line">        <span class="keyword">if</span> (i == j) d[i][j] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> d[i][j] = INF;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 算法结束后，d[a][b]表示a到b的最短距离</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">floyd</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= n; k ++ )</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j ++ )</span><br><span class="line">                d[i][j] = <span class="built_in">min</span>(d[i][j], d[i][k] + d[k][j]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最小生成树">最小生成树</h2>
<h3 id="prim算法">Prim算法</h3>
<p>最小生成树，类似Dijkstra算法</p>
<p>堆优化版本，时间复杂度是 O(mlogn)，n表示点数，m 表示边数</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//.. 前向星 node[M], head[N], idx;</span></span><br><span class="line"><span class="keyword">int</span> dis[N], vis[N];</span><br><span class="line"><span class="keyword">int</span> n, m;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">prim</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(dis, <span class="number">0x3f</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(dis));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">    priority_queue&lt;pll, vector&lt;pll&gt;, greater&lt;pll&gt;&gt; heap;</span><br><span class="line">    dis[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">    heap.<span class="built_in">push</span>(&#123;<span class="number">0</span>, <span class="number">1</span>&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(!heap.<span class="built_in">empty</span>())&#123;</span><br><span class="line">        <span class="keyword">auto</span> cur = heap.<span class="built_in">top</span>(); heap.<span class="built_in">pop</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> distance = cur.first, a = cur.second;</span><br><span class="line">        <span class="keyword">if</span>(vis[a]) <span class="keyword">continue</span>;</span><br><span class="line">        vis[a] = <span class="number">1</span>;</span><br><span class="line">        sum += distance; cnt++;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = head[a]; i!=<span class="number">-1</span>; i = node[i].ne)&#123;</span><br><span class="line">            <span class="keyword">int</span> b = node[i].to;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(!vis[b] &amp;&amp; dis[b] &gt; node[i].w)&#123;</span><br><span class="line">                dis[b] = node[i].w;</span><br><span class="line">                heap.<span class="built_in">push</span>(&#123;dis[b], b&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(cnt != n) <span class="keyword">return</span> INF;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="kruskal算法">Kruskal算法</h3>
<p>时间复杂度是 O(mlogm), n表示点数，m表示边数</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n, m;       <span class="comment">// n是点数，m是边数</span></span><br><span class="line"><span class="keyword">int</span> p[N];       <span class="comment">// 并查集的父节点数组</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span>&#123;</span>     <span class="comment">// 存储边</span></span><br><span class="line">    <span class="keyword">int</span> a, b, w;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> Edge &amp;e) <span class="keyword">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> w &lt; e.w;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; edges[M];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;     <span class="comment">// 并查集核心操作</span></span><br><span class="line">    <span class="keyword">if</span> (p[x] != x) p[x] = <span class="built_in">find</span>(p[x]);</span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kruskal</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">sort</span>(edges, edges + m);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ ) p[i] = i;    <span class="comment">// 初始化并查集</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">0</span>, cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i ++ )&#123;</span><br><span class="line">        <span class="keyword">int</span> a = edges[i].a, b = edges[i].b, w = edges[i].w;</span><br><span class="line"></span><br><span class="line">        a = <span class="built_in">find</span>(a), b = <span class="built_in">find</span>(b);</span><br><span class="line">        <span class="keyword">if</span> (a != b)&#123;     <span class="comment">// 如果两个连通块不连通，则将这两个连通块合并</span></span><br><span class="line">            p[a] = b;</span><br><span class="line">            res += w;</span><br><span class="line">            cnt ++ ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cnt &lt; n - <span class="number">1</span>) <span class="keyword">return</span> INF;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="二分图">二分图</h2>
<h3 id="染色法">染色法</h3>
<h3 id="匈牙利算法">匈牙利算法</h3>
]]></content>
      <categories>
        <category>算法</category>
        <category>图论</category>
      </categories>
      <tags>
        <tag>二分图</tag>
        <tag>链式前向星</tag>
        <tag>最小生成树</tag>
        <tag>最短距离</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构</title>
    <url>/2024/07/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id="数据结构">数据结构</h1>
<h2 id="单调栈monotone-stack">单调栈（Monotone Stack）</h2>
<p>常见模型：找出每个数左边离它最近的比它大/小的数</p>
<h3 id="单调递增栈">单调递增栈</h3>
<p>保证栈内元素单调递增。（从栈顶到栈底）</p>
<ul>
<li>只有比栈顶大的元素才能直接进栈</li>
<li>首先先将栈中比当前元素大的元素出栈
<ul>
<li>出栈时，<strong>新元素</strong>是<strong>出栈元素</strong>向后找第一个比其小的元素</li>
<li>所有出栈后，<strong>栈顶元素</strong>是<strong>新元素</strong>向前找第一个比其小的元素</li>
</ul></li>
<li>再将当前元素出栈</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">stack&lt;<span class="keyword">int</span>&gt; st;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">	<span class="keyword">while</span>(!st.<span class="built_in">empty</span>() &amp;&amp; st.top &gt;= nums[i])&#123; <span class="comment">// 判断栈顶是否符合单调递增关系</span></span><br><span class="line">        <span class="comment">// 新元素是出栈元素向后找第一个比其小的元素</span></span><br><span class="line">        st.<span class="built_in">pop</span>();</span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 栈顶元素是新元素向前找第一个比其小的元素</span></span><br><span class="line">    st.<span class="built_in">push</span>(nums[i]);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 具体逻辑.. </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/832/">830. 单调栈 -
AcWing题库</a></p>
</blockquote>
<h3 id="单调递减栈">单调递减栈</h3>
<p>栈内元素递减</p>
<p>（同上）</p>
<h2 id="单调队列">单调队列</h2>
<p>常见模型：找出滑动窗口中的最大值/最小值</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">deque&lt;Node&gt; que;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">    <span class="keyword">while</span>(!que.<span class="built_in">empty</span>() &amp;&amp; <span class="built_in">check_head</span>(que.<span class="built_in">front</span>())) que.<span class="built_in">pop_front</span>(); <span class="comment">// 判断队头是否滑出窗口</span></span><br><span class="line">    <span class="keyword">while</span>(!que.<span class="built_in">empty</span>() &amp;&amp; <span class="built_in">check</span>(que.<span class="built_in">back</span>(), i)) que.<span class="built_in">pop_back</span>();   <span class="comment">// 判断队尾是否符合单调</span></span><br><span class="line">    que.<span class="built_in">push_back</span>(i);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 具体逻辑</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.acwing.com/problem/content/156/">154. 滑动窗口 -
AcWing题库</a></p>
</blockquote>
<h2 id="并查集">并查集</h2>
<h3 id="朴素并查集">朴素并查集</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> p[N]; <span class="comment">//存储每个点的祖宗节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回x的祖宗节点</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p[x] != x) p[x] = <span class="built_in">find</span>(p[x]);</span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化，假定节点编号是1~n</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ ) p[i] = i;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 合并a和b所在的两个集合：</span></span><br><span class="line">p[<span class="built_in">find</span>(a)] = <span class="built_in">find</span>(b);</span><br></pre></td></tr></table></figure>
<h3 id="维护size的并查集">维护Size的并查集</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量</span></span><br><span class="line"><span class="keyword">int</span> p[N], size[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回x的祖宗节点</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p[x] != x) p[x] = <span class="built_in">find</span>(p[x]);</span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化，假定节点编号是1~n</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )&#123;</span><br><span class="line">    p[i] = i;</span><br><span class="line">    size[i] = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 合并a和b所在的两个集合：</span></span><br><span class="line">size[<span class="built_in">find</span>(b)] += size[<span class="built_in">find</span>(a)];</span><br><span class="line">p[<span class="built_in">find</span>(a)] = <span class="built_in">find</span>(b);</span><br></pre></td></tr></table></figure>
<h3 id="维护到祖宗节点距离的并查集">维护到祖宗节点距离的并查集</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> p[N], d[N];</span><br><span class="line"><span class="comment">//p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回x的祖宗节点</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p[x] != x)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> u = <span class="built_in">find</span>(p[x]);</span><br><span class="line">        d[x] += d[p[x]];</span><br><span class="line">        p[x] = u;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化，假定节点编号是1~n</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">&#123;</span><br><span class="line">    p[i] = i;</span><br><span class="line">    d[i] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 合并a和b所在的两个集合：</span></span><br><span class="line">p[<span class="built_in">find</span>(a)] = <span class="built_in">find</span>(b);</span><br><span class="line">d[<span class="built_in">find</span>(a)] = distance; <span class="comment">// 根据具体问题，初始化find(a)的偏移量</span></span><br></pre></td></tr></table></figure>
<h2 id="kmp-字符串匹配">KMP 字符串匹配</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">	<span class="comment">// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度</span></span><br><span class="line">求模式串的Next数组：</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>, j = <span class="number">0</span>; i &lt;= m; i ++ )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (j &amp;&amp; p[i] != p[j + <span class="number">1</span>]) j = ne[j];</span><br><span class="line">    <span class="keyword">if</span> (p[i] == p[j + <span class="number">1</span>]) j ++ ;</span><br><span class="line">    ne[i] = j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 匹配</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>, j = <span class="number">0</span>; i &lt;= n; i ++ )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (j &amp;&amp; s[i] != p[j + <span class="number">1</span>]) j = ne[j];</span><br><span class="line">    <span class="keyword">if</span> (s[i] == p[j + <span class="number">1</span>]) j ++ ;</span><br><span class="line">    <span class="keyword">if</span> (j == m)</span><br><span class="line">    &#123;</span><br><span class="line">        j = ne[j];</span><br><span class="line">        <span class="comment">// 匹配成功后的逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="trie树">Trie树</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> son[N][<span class="number">26</span>], cnt[N], idx;</span><br><span class="line"><span class="comment">// 0号点既是根节点，又是空节点</span></span><br><span class="line"><span class="comment">// son[][]存储树中每个节点的子节点</span></span><br><span class="line"><span class="comment">// cnt[]存储以每个节点结尾的单词数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入一个字符串</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">char</span> *str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; str[i]; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> u = str[i] - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="keyword">if</span> (!son[p][u]) son[p][u] = ++ idx;</span><br><span class="line">        p = son[p][u];</span><br><span class="line">    &#125;</span><br><span class="line">    cnt[p] ++ ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询字符串出现的次数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">char</span> *str)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; str[i]; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> u = str[i] - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="keyword">if</span> (!son[p][u]) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        p = son[p][u];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cnt[p];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>指针形式</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TrieNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">bool</span> isend;</span><br><span class="line">    TrieNode* child[<span class="number">26</span>];</span><br><span class="line">&#125;;</span><br><span class="line">TrieNode *root;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Trie</span>() &#123;</span><br><span class="line">    root = <span class="keyword">new</span> <span class="built_in">TrieNode</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = word.<span class="built_in">size</span>();</span><br><span class="line">    TrieNode* cur = root;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;len; i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> u = word[i]-<span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="keyword">if</span>(!cur-&gt;child[u]) cur-&gt;child[u] = <span class="keyword">new</span> <span class="built_in">TrieNode</span>();</span><br><span class="line">        cur = cur-&gt;child[u];</span><br><span class="line">    &#125;</span><br><span class="line">    cur-&gt;isend = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">search</span><span class="params">(string word)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> len = word.<span class="built_in">size</span>();</span><br><span class="line">    TrieNode* cur = root;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;len; i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> u = word[i] - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="keyword">if</span>(!cur-&gt;child[u]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        cur = cur-&gt;child[u];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cur-&gt;isend;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
        <category>数据结构</category>
      </categories>
  </entry>
  <entry>
    <title>数学知识</title>
    <url>/2024/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="数学知识">数学知识</h1>
<h2 id="模运算">模运算</h2>
<h4 id="两个数之和对m取余">两个数之和对m取余</h4>
<p>定义 <span class="math inline">\(a = k_1m+r_1\)</span>, <span class="math inline">\(b=k_2m+r1\)</span></p>
<p>计算两数相加对m取余的结果</p>
<p><span class="math display">\[
\begin{aligned}
(a+b)\%m&amp;=(k_1m+r_1+k_2m+r_2)\%m\\
&amp;=(r_1+r_2)\%m\\
&amp;=(a\%m+b\%m)\%m
\end{aligned}
\]</span></p>
<p>可以避免计算两数之和的结果，对于大数有效果</p>
<blockquote>
<p><a href="https://leetcode.cn/problems/find-the-divisibility-array-of-a-string/description/?envType=daily-question&amp;envId=2024-08-02">2575.
找出字符串的可整除数组 - 力扣（LeetCode）</a></p>
</blockquote>
]]></content>
      <categories>
        <category>算法</category>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title>Java文件管理器</title>
    <url>/2022/09/11/%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%99%A8%E7%9A%84java%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h1 id="java文件管理器">Java文件管理器</h1>
<blockquote>
<p>github项目：https://github.com/xclovehsy/fileManage</p>
</blockquote>
<h2 id="简介">1. 简介</h2>
<h3 id="项目需求">1.1. 项目需求</h3>
<p>运用面向对象程序设计思想，基于Java文件管理和I/O框架，实现基于图形界面的GUI文件管理器。</p>
<h3 id="实现功能">1.2. 实现功能</h3>
<ol type="1">
<li>实现文件夹创建、删除、进入。</li>
<li>实现当前文件夹下的内容罗列。</li>
<li>实现文件拷贝和文件夹拷贝（文件夹拷贝指深度拷贝，包括所有子目录和文件）。</li>
<li>实现指定文件的加密和解密。</li>
<li>实现指定文件和文件夹的压缩。</li>
<li>实现压缩文件的解压。</li>
<li>文件管理器具有图形界面。</li>
</ol>
<h3 id="开发平台">1.3. 开发平台</h3>
<p><strong>开发语言：</strong> Java</p>
<p><strong>开发平台：</strong> Intellij IDEA 2021.2.2</p>
<h2 id="项目设计">2. 项目设计</h2>
<h3 id="mvc设计流程">2.1. MVC设计流程：</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021611627.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>MainFrame作为整个程序的主体，向用户展示GUI界面，同时接受用户的操作。通过响应事件调用FileManager中的文件操作方法，并从FileController中获取当前文件路径对GUI界面进行更新。</li>
<li>FileManager作为文件操作的主要对象，通过调用各种方法对文件进行操作，例如文件创建、删除、加密、解密等。</li>
<li>FileController中保存了文件管理器的当前节点信息以及各种设置信息，以提供MainFrame进行界面的更新。</li>
</ol>
<h3 id="程序架构设计">2.2. 程序架构设计</h3>
<h4 id="整体结构框架">2.2.1. 整体结构框架</h4>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021612017.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<h4 id="view结构">2.2.2. View结构</h4>
<ol type="1">
<li>MyTable中包含了自己定义JTree中的部分构件；</li>
<li>MyTree中包含了自己定义的JTable中的部分构件；</li>
<li>MainFrame则为主界面使得程序具有图形界面，主界面分成了几个Panel和一个Menubar。其中ChangePositionPan负责文件管理器上方文本区域，以及跳转到对应文件夹或打开文件功能；</li>
<li>MyMenubar负责菜单栏的显示以及对应按钮的响应事件；</li>
<li>ReturnPanel中包含了两个按钮负责返回上一级界面以及磁盘界面；</li>
<li>TableScrollPanel通过JTable展示文件信息（名称、修改事件、类型、大小等）；</li>
<li>TreeScrollPanel通过JTree展示文件管理器的文件存储结构，并通过点击文件夹节点，使ManFrame界面跳转的对应的文件夹界面。</li>
</ol>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021612021.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>对于Tree包，MyJTree负责显示目录树，MyJTreeNode负责构建目录树的结点并实现结点间的操作，而MyJTreeRender负责对目录树结点的图标渲染。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021612029.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>对于Table包，
MyJtableModel负责对数据进行整理，MyJTableCellRender负责对文件图标的渲染</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021612034.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<h4 id="fileoperation结构">2.2.3. FileOperation结构</h4>
<p>这个包中的的程序主要负责对文件的操作，其中FileController中保存了文件管理器的当前节点信息以及各种设置信息，以提供MainFrame进行界面的更新；</p>
<ol type="1">
<li>FileManager作为文件操作的主要对象，集成所有的文件操作，例如文件夹创建、删除、文件拷贝和文件夹拷贝等；</li>
<li>MainFrime通过调用FileManager中的方法间接调用其他几个类；</li>
<li>FileIcon负责获取文件的图标；</li>
<li>FileZip负责文件以及文件夹的压缩和解压缩；FileEncrypt通过IO流实现简单的文件加密操作；</li>
</ol>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021613183.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="uml类图">2.3. UML类图</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021615668.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="项目展示">3. 项目展示</h2>
<h3 id="主界面展示">3.1. 主界面展示</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021624967.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="创建删除进入">3.2. 创建、删除、进入</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021624582.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021625913.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021625247.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="隐藏文件">3.3. 隐藏文件</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021625083.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="文件压缩">3.4. 文件压缩</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209021626015.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<p>更多功能请查看说明文档</p>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>秦七子</title>
    <url>/2021/10/03/%E7%A7%A6%E4%B8%83%E5%AD%90/</url>
    <content><![CDATA[<h1 id="秦七子">秦七子</h1>
<p><strong>老庄孔墨孟荀韩非 子（无列子）</strong></p>
<h2 id="老子春秋末期">老子（春秋末期）</h2>
<ul>
<li><p>又称老聃，姓李、名耳、字伯阳。</p></li>
<li><p>道家学派创始人，与庄子合称为老庄。</p></li>
</ul>
<p><strong>核心思想：</strong>道法自然 贵柔守雌 无为而治 辩证法思想</p>
<p><strong>代表作：</strong>《老子》又称《道德经》万经之王 道德真经
德道经 五千言 老子五千文</p>
<p><strong>散文特点：</strong>具有深刻的哲理性和系统的思辨性</p>
<h2 id="庄子战国中期">庄子（战国中期）</h2>
<ul>
<li><p>名周，字子休/沐。</p></li>
<li><p>道家学派 和老子合称老庄。</p></li>
</ul>
<p><strong>思想核心：</strong>顺其自然，提倡无为而不为。</p>
<p><strong>代表作：</strong>《庄子》又称《南华经》</p>
<p><strong>散文特点：</strong>构思精巧、想象丰富、文笔恣肆、词藻瑰丽、</p>
<p>​ 采用寓言形式、善用连类比喻、富有浪漫色彩。</p>
<p><strong>“三玄”：</strong>《周易》《老子》《庄子》</p>
<p>秋水又名河泊与北海若</p>
<h2 id="孔子春秋末期">孔子（春秋末期）</h2>
<ul>
<li><p>字丘，名仲尼。</p></li>
<li><p>儒家学派创始人。</p></li>
<li><p>被称为“大成至圣，万世师表”，开创讲学风气。</p></li>
</ul>
<p><strong>思想核心：</strong>仁和礼（克己复礼）</p>
<ul>
<li><p>代表作《论语》是其孔子弟子和再传弟子的语录体散文集</p></li>
<li><p>《论语》是我国最早的语录体散文集</p></li>
</ul>
<p><strong>散文特点：</strong>通俗易懂 善用修辞 具有格言色彩</p>
<p><strong>最高理想：</strong>大同</p>
<p><strong>教育思想：</strong>有教无类 因材施教</p>
<h2 id="孟子战国中期">孟子(战国中期）</h2>
<ul>
<li>名轲，儒家学派，被称为亚圣。</li>
</ul>
<p><strong>思想核心：</strong>仁政和王道
主张名贵君轻的民本思想，反对暴政</p>
<p><strong>作品：</strong>《孟子》</p>
<ul>
<li>《孟子》是孟子与其弟子共同编写 是论说体 多选题中也是语录体</li>
</ul>
<p><strong>散文特点：</strong>欲擒故纵，引君入豰的论辩思想，采用各种比喻增强辩论的形象性和说服力，大量运用排偶</p>
<h2 id="荀子">荀子</h2>
<ul>
<li>名况，儒家学派。</li>
</ul>
<p><strong>核心思想：</strong>天行有常，人性本恶，重法爱民，隆礼敬士，人不可以已。</p>
<p><strong>代表作：</strong>《荀子》</p>
<h2 id="墨子">墨子</h2>
<ul>
<li><p>名翟</p></li>
<li><p>墨家代表</p></li>
</ul>
<p><strong>思想核心：</strong>兼爱 非攻 尚贤</p>
<p><strong>代表作：</strong>《墨子》</p>
<h2 id="韩非子">韩非子</h2>
<ul>
<li>法家创始人</li>
</ul>
<p><strong>思想核心：</strong>法术势</p>
<p><strong>代表作：</strong>《韩非子》</p>
]]></content>
      <categories>
        <category>HSY</category>
      </categories>
      <tags>
        <tag>语文</tag>
      </tags>
  </entry>
  <entry>
    <title>MLP手写数字识别</title>
    <url>/2022/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<h1 id="mlp手写数字识别">MLP手写数字识别</h1>
<blockquote>
<p>github项目：https://github.com/xclovehsy/digitalRecANNs</p>
</blockquote>
<h2 id="项目简介">项目简介</h2>
<p>构建一个神经网络，利用梯度下降法实现参数的更新，最终实现对0-9的10个手写数字的识别。</p>
<p>其中，MNIST 数据集在 http://yann.lecun.com/exdb/mnist/ 获取,
它包含了四个部分:</p>
<ul>
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47
MB, 包含 60,000 个样本)</li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60
KB, 包含 60,000 个标签)</li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB,
包含 10,000 个样本)</li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含
10,000 个标签)</li>
</ul>
<h2 id="算法思路">算法思路</h2>
<h3 id="数据预处理">数据预处理</h3>
<p>MNIST手写数字图片集为60000条训练数据以及10000条测试数据。每一条为785个数字。每一条数据的第一个为数字的正确值，接下来的784个为图片的像素点，可以组成一篇28×28的像素的图片。数据预处理为纵向的784个数字矩阵，标签为纵向的10个数字，该每个数值介于0.01到0.99代表了识别为对应数字的概率。</p>
<h3 id="模型架构">模型架构</h3>
<p>这里我采用三层的神经网络模型，即一个输入层、一个隐藏层、一个输出层。</p>
<h3 id="参数说明">参数说明</h3>
<ul>
<li>Rate为神经网络模型梯度下降算法的学习率</li>
<li>epochs为神经网络模型训练的次数</li>
<li>input_num为神经网络模型的输入层节点数量即784个图片像素</li>
<li>hide_num为隐藏层节点数量</li>
<li>output_num为输出层节点数量</li>
<li>激活函数选用scipy中的expit函数</li>
</ul>
<h3 id="函数定义">函数定义</h3>
<p>这里的神经网络模型主要有两个函数，一个训练函数net_train以及一个识别函数net_query。</p>
<ul>
<li><p>net_train训练函数通过传入的手写数字图片数据对神经网络模型进行训练，利用梯度下降算法对权重矩阵进行更新。识别函数通过图片数据以及神经网络模型进行计算的到对应数字的识别匹配概率。</p></li>
<li><p>load_mnist函数负责加载MNIST数据集中的数据，show_result函数随机获取测试集中的部分数字对神经网络模型的识别结果进行展示。</p></li>
</ul>
<h2 id="结果分析">结果分析</h2>
<h3 id="实验结果">实验结果</h3>
<p>当将神经网络训练次数设置为10时，训练测试神经网络模型结果如下，并给出测试集中随机20张手写数值识别结果如下；</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209072154162.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209072154171.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="结果分析-1">结果分析</h3>
<p>从实验结果可以看到，神经网络模型的正确识别率很高，在经过60000条数据训练后，识别的成功率高达95%以上。</p>
<p>将模型训练次数设置为1、5、10对神经网络模型进行训练并测试，可以看到当训练次数设置为1时，模型正确识别率为95.57%；当训练次数为5时，模型的正确识别率为97.23%；当训练次数设置为10时，模型的正确识别率为97.23%。</p>
<p>可以发现，模型识别的正确率随着模型训练次数的增加，在一定范围内增加。但是当超过一定的训练次数后，模型的识别正确率不会再增加，这有可能受限于神经网络的层数、隐藏层节点数量，以及手写数字图片的像素精度等因素。</p>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Java网络白板</title>
    <url>/2022/09/10/%E7%BD%91%E7%BB%9C%E7%99%BD%E6%9D%BF%E7%9A%84java%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h1 id="java网络白板">Java网络白板</h1>
<h1 id="网络白板">网络白板</h1>
<blockquote>
<p>github项目：https://github.com/xclovehsy/whiteBoard</p>
</blockquote>
<h2 id="简介">简介</h2>
<h3 id="项目需求">项目需求</h3>
<p>网络白板利用面相对象的思想设计适合可扩展的图形类集，利用java
gui的mvc模式设计用户的绘图流程，利用java的套接字编程实现多客户端的数据共享方法，利用多线程机制实现绘图和数据传输的并发控制机制</p>
<h3 id="实现功能">实现功能</h3>
<ol type="1">
<li>程序能够在窗体上根据用户的选择绘制不同形状（3个以上）的图形；</li>
<li>程序能够修改图形的属性（颜色和大小）和位置（利用鼠标移动选定的图形）；</li>
<li>网络客户端的程序能够协同绘图（一个用户绘图其他用户均可见绘图效果）；</li>
<li>程序能够以文件的形式保存绘图结果，下次启动程序后能够读取绘图结果文件再现绘图效果。</li>
</ol>
<p><strong>开发语言：</strong> Java</p>
<p><strong>开发平台：</strong> Intellij IDEA 2021.2.2</p>
<h2 id="项目设计">项目设计</h2>
<h3 id="mvc设计流程">MVC设计流程</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061101956.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>GUIServer作为服务器接受客户端发送的图形数据，并将图形数据写入Data.txt文件中保存，同时将图形数据发送到各个客户端。</li>
<li>用户操作OpenGLApp画板。每画一个图形，程序会将数据发送到服务器。同时接受服务器传输的图形数据进行绘图。</li>
<li>Data.txt文件用于保存图形数据，每当服务器开启时将会读取文件中上次保存的图形数据，并发送到各个客户端进行绘图。</li>
</ol>
<h3 id="程序架构设计">程序架构设计</h3>
<h4 id="整体结构框架">整体结构框架</h4>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061101514.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>该程序代码共有三个部分</p>
<ol type="1">
<li>app为网络白板客户的主程序</li>
<li>graph为图形包，里面包含了各种图形的数据，例如圆形、立方体、长方形等等。</li>
<li>server负责服务器数据的接受与发送，以及数据的保存</li>
<li>Data.txt用于保存图形数据</li>
</ol>
<hr>
<h4 id="程序结构">程序结构</h4>
<p>OpenGLApp为网络白板客户端的主程序，负责GUI的显示，图形的绘制、发送数据、接受服务器发送的数据等，同时实现了1、根据用户的选择绘制不同形状（3个以上）的图形；2、修改图形的属性（颜色和大小）和位置（利用鼠标移动选定的图形）等功能。</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061102231.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<h4 id="graph结构">Graph结构</h4>
<ol type="1">
<li>Shape类是图形类的抽象父类,
它包含一个抽象方法draw(),在他的派生类中都实现了draw（）方法、各自的属性和属性的修改方法；</li>
<li>Graphic是用来存储当前已有的对象和绘制已有的对象；</li>
<li>Cube、Circle、Rectangle等均是Shape的子类。</li>
</ol>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061103998.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong>Graph包类图如下（可见附件三）</strong></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061104406.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<h4 id="server结构">Server结构</h4>
<ol type="1">
<li>GUIServer为服务器端，接受客户端发送的图形数据，并将图形数据写入Data.txt文件中保存。</li>
<li>SeverThread多线程操作，可以并发的接受多个客户端的数据，并传送数据。</li>
</ol>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061105343.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="uml类图">UML类图</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061105680.jpg" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="项目展示">项目展示</h2>
<h3 id="主界面">主界面</h3>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061106996.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h3 id="功能测试">功能测试</h3>
<h4 id="绘制图形">绘制图形</h4>
<p><strong>程序能够在窗体上根据用户的选择绘制不同形状（3个以上）的图形；</strong></p>
<p>该程序可以根据用户需要绘制不同的图形，通过点击面板上不同的图形按钮设置当前绘制的图形种类。例如圆形、椭圆、立方体、长方形、三角形等</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061106005.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>#### 修改图形</p>
<p><strong>程序能够修改图形的属性（颜色和大小）和位置（利用鼠标移动选定的图形）；</strong></p>
<p>可以选择不同的线条颜色以及填充颜色</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061106011.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h4 id="网络协同">网络协同</h4>
<p><strong>网络客户端的程序能够协同绘图（一个用户绘图其他用户均可见绘图效果）；</strong></p>
<p>打开sever.GUISever服务器程序，等待其他画板的连接</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061107060.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>同时将网络白板客户端程序运行4次服务器连接上程序</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061107064.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061107082.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>在客户端1绘制图形将会传输其他客户端白板中。</li>
<li>在一个客户端中对图形进行修改，其他客户端的图形也会修改。即网络客户端的程序能够协同绘图（一个用户绘图其他用户均可见绘图效果）；</li>
</ul>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209061107093.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<hr>
<p><strong>更多功能请查看说明文档</strong></p>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>诗经、楚辞</title>
    <url>/2021/10/04/%E8%AF%97%E7%BB%8F%E3%80%81%E6%A5%9A%E8%BE%9E/</url>
    <content><![CDATA[<h1 id="诗经楚辞">诗经、楚辞</h1>
<h2 id="诗经">诗经</h2>
<ul>
<li><p>《诗经》又称《诗》和《诗三百》</p></li>
<li><p>是我国第一部（最早）现实主义诗歌总集</p></li>
<li><p>记录了周初到周末500年历史</p></li>
<li><p>共305篇</p></li>
</ul>
<p><strong>诗经六义：</strong>风雅颂 赋比兴</p>
<p><strong>风（内容）</strong>：十五国风，共160篇。多用复沓手法，多为民歌。</p>
<p><strong>雅（题材）</strong>：《大雅》和《小雅》105篇
多为贵族宫廷乐曲。</p>
<p><strong>颂（音乐）</strong>：《周颂》、《商颂》、《鲁颂》</p>
<p><strong>赋</strong>：铺陈直叙。</p>
<p><strong>比</strong>：以彼物必此物。</p>
<p><strong>兴</strong>：借物比喻，触物兴词，以次及彼。</p>
<p><strong>《诗经》手法：</strong>重章叠句、情景交融、起兴手法、虚实结合。</p>
<p><strong>诗经艺术特色：</strong>以四言为主，重章叠句，节奏简约明快，情致回环往复
多用比兴手法，意蕴丰赡含蓄</p>
<ul>
<li><p>诗经中的国风被称为现代主义诗歌源头</p></li>
<li><p>王国维在《人间词话》评价蒹葭是“最得风人深致”</p></li>
</ul>
<h2 id="楚辞">楚辞</h2>
<ul>
<li><p>是楚地民歌或者歌辞</p></li>
<li><p>楚辞体是一种新诗体，一般采用四言或八言</p></li>
</ul>
]]></content>
      <categories>
        <category>HSY</category>
      </categories>
      <tags>
        <tag>语文</tag>
      </tags>
  </entry>
  <entry>
    <title>组合逻辑电路</title>
    <url>/2021/10/09/%E7%BB%84%E5%90%88%E9%80%BB%E8%BE%91%E7%94%B5%E8%B7%AF/</url>
    <content><![CDATA[<h1 id="组合逻辑电路">组合逻辑电路</h1>
<h2 id="偶校验码生成器">偶校验码生成器</h2>
<p><strong>设某数字系统的数据交换按半字节进行（传输数据为4位），同时保证数据交换的正确性，设计一个偶校验码生成器。</strong></p>
<p>通过查阅资料得知：奇偶校验码是一种增加二进制传输系统最小距离的简单和广泛采用的方法。是一种通过增加<strong>冗余位</strong>使得码字中"1"的个数恒为奇数或偶数的编码方法。</p>
<p>现在定义四位的数据输入（半字节）</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="keyword">input</span> [<span class="number">0</span>:<span class="number">3</span>]in;</span><br></pre></td></tr></table></figure>
<p>接着定义五位偶检验码的输出。其中前4位对应数据输入的4位信息元，最后为1位偶校验元</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="keyword">output</span> [<span class="number">0</span>:<span class="number">4</span>]out;</span><br></pre></td></tr></table></figure>
<p>五位偶检验码的前4位为对应4位数据数据，可以直接对应。通过计算可以推导出偶检验码的计算公式，偶检验码
r = a<sub>0</sub>⊕a<sub>1</sub>⊕a<sub>2</sub>⊕a<sub>3</sub> （奇检验码 r
=
a<sub>0</sub>⊕a<sub>1</sub>⊕a<sub>2</sub>⊕a<sub>3</sub>⊕1）,其中a<sub>i</sub>表示4位传输数据中对应的码元。在verilog中使用"^"表示异或运算。</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">out[<span class="number">0</span>:<span class="number">3</span>] = in[<span class="number">0</span>:<span class="number">3</span>];   <span class="comment">//4位输出信息元对应4位传输数据</span></span><br><span class="line">out[<span class="number">4</span>] = in[<span class="number">0</span>]^in[<span class="number">1</span>]^in[<span class="number">2</span>]^in[<span class="number">3</span>];   <span class="comment">//偶校验元</span></span><br></pre></td></tr></table></figure>
<p>因此偶校验码生成器完整代码如下：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns/100ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> pccg(in,out);</span><br><span class="line"></span><br><span class="line"><span class="keyword">input</span> [<span class="number">0</span>:<span class="number">3</span>]in;</span><br><span class="line"><span class="keyword">output</span> [<span class="number">0</span>:<span class="number">4</span>]out;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">assign</span> out[<span class="number">0</span>:<span class="number">3</span>] = in[<span class="number">0</span>:<span class="number">3</span>];   <span class="comment">//4位输出信息元对应4位传输数据</span></span><br><span class="line"><span class="keyword">assign</span> out[<span class="number">4</span>] = in[<span class="number">0</span>]^in[<span class="number">1</span>]^in[<span class="number">2</span>]^in[<span class="number">3</span>];   <span class="comment">//偶校验元</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>
<p><strong>vivado生成的电路图如下：</strong></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090016846.png" alt="image-20211007213847079">
<figcaption aria-hidden="true">image-20211007213847079</figcaption>
</figure>
<h2 id="命令启停器">2. 命令启停器</h2>
<p><strong>为computer
house中烧咖啡的机器老鼠设计一个命令启停器。</strong></p>
<p>首先我定义一个总控制位En表示网关命令是否有效（1表示网关命令的有效，0表示网关命令无效）</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="keyword">wire</span> En;</span><br></pre></td></tr></table></figure>
<p>通过题目得知网关命令中如果同时出现矛盾的命令对（F与B、I与S、TUON与TUOF）话，表示网关命令的无效。因此En应该如下表示：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">En = ~(F*B + I*S + TUON*TUOF);</span><br></pre></td></tr></table></figure>
<p>在老鼠四个动作行为中。老鼠的前进（A动作）、老鼠的后退（C动作）由En、F、B来控制，对应的真值表为：</p>
<table>
<thead>
<tr>
<th style="text-align: center;">En</th>
<th style="text-align: center;">F</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>其中x表示该位可以为“1”也可以为“0”。如果当F和B同时为1时，En为0。因此包含在第一中情况中。</p>
<p>写出对应的verilog语言为：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">A = En*F</span><br><span class="line">C = En*B</span><br></pre></td></tr></table></figure>
<p>此外在老鼠四个动作行为中。咖啡的注入（D动作）、咖啡的停止注入（E动作）由En、I、S来控制，对应的真值表为：</p>
<table>
<thead>
<tr>
<th style="text-align: center;">En</th>
<th style="text-align: center;">I</th>
<th style="text-align: center;">S</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>写出对应的verilog语言为：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">D = En*I</span><br><span class="line">E = En*S</span><br></pre></td></tr></table></figure>
<p>因此烧咖啡机器老鼠的命令启停器的完整代码为：</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns/100ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> controller(F, B, I, S, TUON, TUOF, A, C, D, E);</span><br><span class="line"></span><br><span class="line"><span class="keyword">input</span> F, B, I, S, TUON, TUOF;</span><br><span class="line"><span class="keyword">output</span> A, C, D, E;</span><br><span class="line"><span class="keyword">wire</span> En;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assign</span> En = ~(F*B + I*S + TUON*TUOF);</span><br><span class="line"><span class="keyword">assign</span> A = En*F;</span><br><span class="line"><span class="keyword">assign</span> C = En*B;</span><br><span class="line"><span class="keyword">assign</span> D = En*I;</span><br><span class="line"><span class="keyword">assign</span> E = En*S;</span><br><span class="line"></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>
<p><strong>通过vivado综合生成的电路图如下：</strong></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209090015765.png" alt="image-20211007225740825">
<figcaption aria-hidden="true">image-20211007225740825</figcaption>
</figure>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>数字逻辑</tag>
      </tags>
  </entry>
  <entry>
    <title>旅行商问题</title>
    <url>/2022/09/09/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3%E6%97%85%E8%A1%8C%E5%95%86%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="旅行商问题">旅行商问题</h1>
<blockquote>
<p>github项目：https://github.com/xclovehsy/geneticAlgoTSP</p>
</blockquote>
<h2 id="项目描述">1. 项目描述</h2>
<p>一个旅行者需要到国内的10个城市旅行，各城市的坐标见cities.csv文档。请设计一个合理的线路使旅行者所行的路程之和最小。注意：每个城市只能访问一次，且最后要回到原来出来的城市。</p>
<h2 id="算法思路">2. 算法思路</h2>
<p><strong>遗传算法</strong>是一种模仿自然界生物进化机制的全局搜索和优化方法。其具有以下几个基本的特征，接下来是对各个特征详细的说明。</p>
<ul>
<li><p><strong>表现型：</strong>旅行依次经过的城市，即旅行的路径。例如“重庆—&gt;北京—&gt;上海—&gt;天津—&gt;成都—&gt;…
—&gt;重庆”这是一种旅行路径，这里需要注意的是最终需要回到起点。</p></li>
<li><p><strong>基因型：</strong>我根据cities.csv文件中的城市出现的顺序，对城市进行1.2.3….编码。然后旅行经过的城市依次以序号进行编码。同时因为最后要回到起点，因此起点和终点的序号应该相同。</p></li>
<li><p>这里我们根据cities.csv文件中城市的次序进行编码，即：北京-0、天津-1、上海-2、重庆-3、拉萨-4、乌鲁木齐-5、银川-6、呼和浩特-7、南宁-8、哈尔滨-9。</p></li>
<li><p><strong>编码：</strong>这里我选择的编码方式为：对城市进行1.2.3…次序编码，然后一条旅行路线为一个个体，编码方式为起点到终点依次经过城市的序号编码。</p></li>
<li><p><strong>进化：</strong>种群逐渐适应生存环境，即总的路径长度不断得到缩小。旅行线路的进化是以种群的形式进行的。</p></li>
<li><p><strong>适应度：</strong>度量某个旅行线路对于生存环境的适应程度。这里我使用旅行线路总的路径长度倒数作为种群的适应度</p></li>
<li><p><strong>选择：</strong>以一定的概率从种群中选择若干个个体。我使用轮盘赌的方式对种群中的个体进行选择。</p></li>
<li><p><strong>交叉：</strong>两个旅行线路的某一相同位置处城市编码，前后两串分别交叉组合形成两个新的旅行线路。也称基因重组或杂交；</p></li>
<li><p><strong>变异：</strong>以一定的概率对种群中的个体进行基因突变，即随机选择个体基因中两个城市的编号进行交换。</p></li>
</ul>
<h2 id="遗传算法步骤">3. 遗传算法步骤</h2>
<p>开始循环直至找到满意的解。</p>
<ol type="1">
<li>评估每条染色体所对应个体的适应度。</li>
<li>遵照适应度越高，选择概率越大的原则，从种群中选择两个个体作为父方和母方。</li>
<li>抽取父母双方的染色体，进行交叉，产生子代。</li>
<li>对子代的染色体进行变异。</li>
<li>重复2，3，4步骤，直到新种群的产生。</li>
</ol>
<p>结束循环。</p>
<p><strong>算法流程图：</strong></p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209072116751.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="程序代码">4. 程序代码</h2>
<p>这里一共有三个模块分别是Life.py、GA.py、TSP.py，每个模块的作用如下：</p>
<ul>
<li>Life.py为种群中的旅游线路个体类，里面包含旅行路线的基因型以及适应值</li>
<li>GA.py为遗传算法内，里面主要包含了遗传算法的一些函数，例如基因交叉、基因突变、轮盘赌选择个体、生成个体、生成新一代种群等函数。</li>
<li>TSP.py为旅行商问题的主要运行模块，包含对城市数据的读取，计算适应值、绘制适应度函数的进化曲线和最终选择的线路图等功能。</li>
</ul>
<p>具体代码请查看目录“lab3”</p>
<h2 id="运行结果">5. 运行结果</h2>
<p>这里设置一个种群中包含20个体，迭代200次后计算得出最优路径。gen表示迭代到第i代种群，dis表示该种群中最优个体的总旅行距离。</p>
<p><strong>运行结果如下：</strong></p>
<p>gen:1, dis:150.1067758288992</p>
<p>gen:2, dis:150.1067758288992</p>
<p>...</p>
<p>gen:199, dis:109.98824625831303</p>
<p>gen:200, dis:109.98824625831303</p>
<hr>
<p>经历过200次迭代后，最短路径为：</p>
<p>北京——&gt;呼和浩特, dis:4.8935</p>
<p>呼和浩特——&gt;银川, dis:5.8709</p>
<p>银川——&gt;乌鲁木齐, dis:19.0424</p>
<p>乌鲁木齐——&gt;拉萨, dis:14.1505</p>
<p>拉萨——&gt;重庆, dis:15.4347</p>
<p>重庆——&gt;南宁, dis:6.9833</p>
<p>南宁——&gt;上海, dis:15.5932</p>
<p>上海——&gt;哈尔滨, dis:15.4157</p>
<p>哈尔滨——&gt;天津, dis:11.5217</p>
<p>天津——&gt;北京, dis:1.0825</p>
<p>最短路径距离为：109.9882</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209072119886.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>图2适应度函数的进化曲线</p>
<figure>
<img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/img/202209072119894.gif" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>图3最终选择的线路图</p>
<h2 id="结果分析">6. 结果分析</h2>
<p>由适应度函数的进化曲线可得知，刚开始种群进化速度很快，当适应值接近最优值时进化速度放缓。由图像可知在接近100代种群时适应值收敛。说明遗传的收敛效果良好。</p>
]]></content>
      <categories>
        <category>CQU</category>
      </categories>
      <tags>
        <tag>遗传算法</tag>
      </tags>
  </entry>
</search>
