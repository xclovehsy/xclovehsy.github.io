<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>XcloveHsy's Blog</title><meta name="author" content="XuCong"><meta name="copyright" content="XuCong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Nothing is impossible">
<meta property="og:type" content="website">
<meta property="og:title" content="XcloveHsy&#39;s Blog">
<meta property="og:url" content="https://xclovehsy.github.io/index.html">
<meta property="og:site_name" content="XcloveHsy&#39;s Blog">
<meta property="og:description" content="Nothing is impossible">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg">
<meta property="article:author" content="XuCong">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg"><link rel="shortcut icon" href="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg"><link rel="canonical" href="https://xclovehsy.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'XcloveHsy\'s Blog',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-08-05 16:21:47'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><meta name="generator" content="Hexo 5.4.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/b4c744affd761ee33e8d78b900e090e.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="XcloveHsy's Blog"><img class="site-icon" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg"/><span class="site-name">XcloveHsy's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa-solid fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">XcloveHsy's Blog</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/xclovehsy" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xclovehsy@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/a30916bee2712965ab529e04926fb19.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Decoder"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder">【Transformer】Decoder</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-05T01:28:22.000Z" title="发表于 2024-08-05 09:28:22">2024-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T01:28:22.481Z" title="更新于 2024-08-05 09:28:22">2024-08-05</time></span></div><div class="content">
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805154949444.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Encoder"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder">【Transformer】Encoder</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-05T01:28:07.000Z" title="发表于 2024-08-05 09:28:07">2024-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T08:13:01.215Z" title="更新于 2024-08-05 16:13:01">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">【Transformer】Encoder
Transformer编码器由N个EncoderBlock堆叠而成，每个EncoderBlock有两个子层：

Multi-Head Attention (Self-Attention)

Add (残差连接)
Norm (层归一化)

Positionwise Feed Forward Network

Add (残差连接)
Norm (层归一化)


\(EncoderBlock_i\)的输出给到下一层，作为\(EncoderBlock_{i+1}（i=1,2,..,n-1）\)的输入
将\(EncoderBlock_{n}\)的输出给到解码器的各层中
\(EncoderBlock_{n}\)的结构如下所示


image-20240805154949444

前面已实现MultiHeadAttention、AddNorm、PositionWiseFFN等模块，这里首先实现EncoderBlock。
其中包含两个子层： 多头注意力机制、基于位置前馈神经网络
每个子层之间均使用残差连接和Layer ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805144748910.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Residual and LayerNorm"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm">【Transformer】Residual and LayerNorm</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-05T01:21:48.000Z" title="发表于 2024-08-05 09:21:48">2024-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T07:38:17.517Z" title="更新于 2024-08-05 15:38:17">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">【Transformer】Residual and
LayerNorm

Layer Normal
Layer Normalization
的作用是把神经网络中隐藏层归一为标准正态分布，也就是$ i.i.d
$独立同分布，以起到加快训练速度，加速收敛的作用
其中\(\mu_j\)表示均值，\(\sigma^2_j\)为方差，加 \(\epsilon\)是为了防止分母为 0 \[
LayerNorm(x)=\frac{x_{ij}-\mu _j}{\sqrt{\sigma^2_j+\epsilon}}
\]
层规范化和批量规范化的目标相同，但Layer
Normal是基于特征维度进行规范化。
对于Batch Normal来说，样本长度对计算方差影响比较大。 而Layer
Normal针对每个样本计算方差
对于两个维度的情况，Batch Normal对于每个feature来做normal，Layer
Normal则以样本为单位做normal


image-20240805143242244

对于三个维度的情况，在自然语言处理任务中，句子的长度往往 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805140934105.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Position-wise FFN"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN">【Transformer】Position-wise FFN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-05T01:12:38.000Z" title="发表于 2024-08-05 09:12:38">2024-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T06:23:55.642Z" title="更新于 2024-08-05 14:23:55">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">Position-wise FFN
Transformer的每个子层的注意力机制之间都包含一个前馈神经网络


image-20240805142347660

对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），因此该前馈网络是“Position-wise”
并且使用Relu作为激活函数 \[
FFN(x)=max(0,xW_1+b_1)W_2+b_2
\]
实现PositionWiseFFN类，
输入X通过三层的MLP计算得到输入（python默认对最后一个维度进行推理）
1234567891011class PositionWiseFFN(nn.Module):    &quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,                 **kwargs):        super(PositionWiseFFN, self).__init__ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805113648793.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Attention"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention">【Transformer】Attention</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-02T11:30:57.000Z" title="发表于 2024-08-02 19:30:57">2024-08-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T06:22:18.585Z" title="更新于 2024-08-05 14:22:18">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">【Transformer】Attention
注意力机制简单来说就是已知一个Query，以及一些Key-Value对
通过计算Query与每个Key之间的关联度，并通过Softmax计算得到每个Key对应Value的权重
利用权重计算Value的加权和作为Query的输出
如下图，Q1与K2之间的关联度比较高，从而对应V2在计算输出时会给出更大的权重。
与K5关联度比较小，从而V5的权重也会小一些


image-20240805113648793

Masked Softmax Operation
注意力机制通过计算关联度的Softmax值得到一个概率分布作为注意力权重。
但是在Transformer训练过程时，Decoder可以获得对应的所有输出，例如推理“I
love you”中的love词时不应该将"love
you"放入到注意力的计算中。因此在某些情况下，并非所有的值都应该被放入到注意力计算中。
首先定义sequence_mask函数，传入2Dtensor以及对应valid_len(1D)将超过指定范围的元素设置为指定的value
123456 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/08/02/%E3%80%90Transformer%E3%80%91Positional%20Encoding/" title="【Transformer】Positional Encoding"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240802184231821.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Positional Encoding"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Positional%20Encoding/" title="【Transformer】Positional Encoding">【Transformer】Positional Encoding</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-02T11:13:19.000Z" title="发表于 2024-08-02 19:13:19">2024-08-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T05:42:54.359Z" title="更新于 2024-08-05 13:42:54">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">【Transformer】Positional
Encoding
在处理词元序列时，循环神经网络是逐个的重复地处理词元的通过中间状态保存顺序信息，
但是Transformer中使用自注意力并行计算而无法保存序列的顺序信息。
因此在Transformer中为了使用序列的顺序信息，通过在输入表示中添加
位置编码（positional
encoding）来在输入中加入位置信息。
位置编码可以通过学习得到也可以直接固定得到。

Transformer中使用基于正弦函数和余弦函数的固定位置编码。
Bert通过学习的方式获得位置编码

假设输入表示\(X∈R^{n×d}\)
包含一个序列中n个词元的d维嵌入表示。
位置编码使用相同形状的位置嵌入矩阵 \(P∈R^{n×d}\) 输出\(X+P\)，
矩阵第i行、第2j列和2j+1列上的元素为： \[
\begin{align}
p_{i,2j}&amp;=sin(\frac{i}{10000^{2j/d}})\\
p_{i,2j+1}&amp;=cos(\frac{i}{10000^{2j/d}})
\end{ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/08/02/%E3%80%90Transformer%E3%80%91%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/" title="【Transformer】模型总结"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240724172359971.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】模型总结"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/02/%E3%80%90Transformer%E3%80%91%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/" title="【Transformer】模型总结">【Transformer】模型总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-02T02:17:37.000Z" title="发表于 2024-08-02 10:17:37">2024-08-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-05T07:47:38.277Z" title="更新于 2024-08-05 15:47:38">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/">LLM</a></span></div><div class="content">【Transformer】模型总结
模型架构
采用encoder-decoder的架构
encoder将输入\((x_1,,x_n)\)映射到连续的中间表达\((z_1,,z_n)\)
decoder再采用自回归的方式输出序列\((y_1,,y_n)\)
（将前一个生成的符号添加到输入，接着生成下一个（类似RNN））


image-20240724172359971

解码器
Transformer编码器由N个DecoderBlock堆叠而成，每个DecoderBlock有三个子层：

Masked Multi-Head Attention (Self-Attention)

Add (残差连接)
Norm (层归一化)

Multi-Head Attention (Co-Attention)

Add (残差连接)
Norm (层归一化)

Positionwise Feed Forward Network

Add (残差连接)
Norm (层归一化)


\(DecoderBlock_i\)的输出给到下一层，作为\(De ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" title="数学知识"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/17b264d340f81d56756a5e4ab518a2a.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数学知识"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" title="数学知识">数学知识</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-02T01:42:12.000Z" title="发表于 2024-08-02 09:42:12">2024-08-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-02T02:32:30.760Z" title="更新于 2024-08-02 10:32:30">2024-08-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/">数学</a></span></div><div class="content">数学知识
模运算
两个数之和对m取余
定义 \(a = k_1m+r_1\), \(b=k_2m+r1\)
计算两数相加对m取余的结果
\[
\begin{aligned}
(a+b)\%m&amp;=(k_1m+r_1+k_2m+r_2)\%m\\
&amp;=(r_1+r_2)\%m\\
&amp;=(a\%m+b\%m)\%m
\end{aligned}
\]
可以避免计算两数之和的结果，对于大数有效果

2575.
找出字符串的可整除数组 - 力扣（LeetCode）

</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/07/31/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="交叉熵损失函数"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/5dbf44b475b1c4135659fac21aee018.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="交叉熵损失函数"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/07/31/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="交叉熵损失函数">交叉熵损失函数</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-31T11:28:37.000Z" title="发表于 2024-07-31 19:28:37">2024-07-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-02T02:32:56.731Z" title="更新于 2024-08-02 10:32:56">2024-08-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="content">交叉熵损失函数
1234567891011121314151617import torchimport numpy as npimport torch.nn as nn# dim=0 按列进行计算，dim=1 按行进行计算# softmax ——&gt; log_softmax --&gt; NLLLOSS--&gt; cross_entropyx = torch.randn(3, 5, requires_grad=True)&quot;&quot;&quot;tensor([[ 1.3962, -0.2568, -0.7142,  1.1941,  0.5695],        [-0.7136, -1.0663,  1.7642,  0.5170, -0.1858],        [ 0.0424, -0.3354, -0.9049,  0.6952,  1.3032]], requires_grad=True)&quot;&quot;&quot;y = torch.empty(3, dtype=torch.long).random_(5)&quot;&quot;&quot; ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89/" title="【RL】Multi-Agent Learning"><img class="post-bg" src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20231206092127067.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【RL】Multi-Agent Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/07/31/RL%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89/" title="【RL】Multi-Agent Learning">【RL】Multi-Agent Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-31T11:21:06.000Z" title="发表于 2024-07-31 19:21:06">2024-07-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-02T03:40:54.823Z" title="更新于 2024-08-02 11:40:54">2024-08-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="content">【RL】Multi-Agent Learning
一个系统中存在多个决策者
面临的困难

环境非平稳变化（马尔科夫特性不再满足），每个智能体所对应的环境包含了其他智能体的策略，其他智能体策略的变化导致环境变化的非平稳性。
局部可观，智能体只能观测到环境的部分状态
集中式学习不可行，需要一个集中式控制中心与智能体之间进行大量的信息交互，神经网络的输入输出维度会随智能体数目指数增大，难以收敛。

基本概念
Agent的关系

Fully
cooperative，两个Agent需要相互配合才能完成任务（工业机器人）
Fully
competitive，一方的收益是另一方的损失（捕食者和猎物）
Mixed Cooperative &amp;
competitive，既有竞争又有合作（足球机器人，队伍内是合作，队伍间是博弈）
Self-interested，利己主义，只想最大化自己的利益，动作可能让别人获益or损失，不关心别人的利益（股票的期货交易系统、无人车）

State &amp; Action
There are \(n\) agents.
Le ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">XuCong</div><div class="author-info__description">Nothing is impossible</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xclovehsy" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xclovehsy@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/a30916bee2712965ab529e04926fb19.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Decoder"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Decoder/" title="【Transformer】Decoder">【Transformer】Decoder</a><time datetime="2024-08-05T01:28:22.000Z" title="发表于 2024-08-05 09:28:22">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805154949444.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Encoder"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Encoder/" title="【Transformer】Encoder">【Transformer】Encoder</a><time datetime="2024-08-05T01:28:07.000Z" title="发表于 2024-08-05 09:28:07">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805144748910.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Residual and LayerNorm"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Residual%20and%20LayerNorm/" title="【Transformer】Residual and LayerNorm">【Transformer】Residual and LayerNorm</a><time datetime="2024-08-05T01:21:48.000Z" title="发表于 2024-08-05 09:21:48">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805140934105.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Position-wise FFN"/></a><div class="content"><a class="title" href="/2024/08/05/%E3%80%90Transformer%E3%80%91Position-wise%20FFN/" title="【Transformer】Position-wise FFN">【Transformer】Position-wise FFN</a><time datetime="2024-08-05T01:12:38.000Z" title="发表于 2024-08-05 09:12:38">2024-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention"><img src="https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/image-20240805113648793.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Transformer】Attention"/></a><div class="content"><a class="title" href="/2024/08/02/%E3%80%90Transformer%E3%80%91Attention/" title="【Transformer】Attention">【Transformer】Attention</a><time datetime="2024-08-02T11:30:57.000Z" title="发表于 2024-08-02 19:30:57">2024-08-02</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list expandBtn" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CQU/"><span class="card-category-list-name">CQU</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/HSY/"><span class="card-category-list-name">HSY</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"><span class="card-category-list-name">常用技巧</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item parent"><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">机器学习</span><span class="card-category-list-count">12</span><i class="fas fa-caret-left "></i></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/LLM/"><span class="card-category-list-name">LLM</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">强化学习</span><span class="card-category-list-count">5</span></a></li></ul></li><li class="card-category-list-item parent"><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/"><span class="card-category-list-name">算法</span><span class="card-category-list-count">10</span><i class="fas fa-caret-left "></i></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/DP/"><span class="card-category-list-name">DP</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/%E5%85%B6%E4%BB%96/"><span class="card-category-list-name">其他</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/%E5%9B%9E%E6%BA%AF/"><span class="card-category-list-name">回溯</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/%E5%9B%BE%E8%AE%BA/"><span class="card-category-list-name">图论</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/"><span class="card-category-list-name">数学</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="card-category-list-name">数据结构</span><span class="card-category-list-count">3</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 1.15em; color: rgb(113, 144, 124)">回溯</a><a href="/tags/leetcode/" style="font-size: 1.15em; color: rgb(68, 43, 107)">leetcode</a><a href="/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/" style="font-size: 1.15em; color: rgb(161, 92, 54)">概率统计</a><a href="/tags/%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(71, 90, 133)">策略学习</a><a href="/tags/%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0/" style="font-size: 1.15em; color: rgb(146, 175, 126)">价值学习</a><a href="/tags/actor-critic/" style="font-size: 1.15em; color: rgb(189, 101, 81)">actor&critic</a><a href="/tags/c/" style="font-size: 1.3em; color: rgb(18, 77, 62)">c++</a><a href="/tags/STL/" style="font-size: 1.15em; color: rgb(125, 132, 4)">STL</a><a href="/tags/python/" style="font-size: 1.3em; color: rgb(78, 136, 38)">python</a><a href="/tags/uniapp/" style="font-size: 1.15em; color: rgb(4, 122, 98)">uniapp</a><a href="/tags/git/" style="font-size: 1.15em; color: rgb(51, 200, 77)">git</a><a href="/tags/hexo/" style="font-size: 1.15em; color: rgb(116, 140, 196)">hexo</a><a href="/tags/linux/" style="font-size: 1.15em; color: rgb(56, 75, 96)">linux</a><a href="/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/" style="font-size: 1.3em; color: rgb(108, 21, 9)">线段树</a><a href="/tags/%E5%8C%BA%E9%97%B4%E6%B1%82%E5%92%8C/" style="font-size: 1.15em; color: rgb(175, 133, 189)">区间求和</a><a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 1.15em; color: rgb(157, 170, 151)">二叉树</a><a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 1.15em; color: rgb(179, 119, 91)">数据结构</a><a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 1.15em; color: rgb(56, 102, 112)">动态规划</a><a href="/tags/%E8%AF%AD%E6%96%87/" style="font-size: 1.45em; color: rgb(34, 156, 118)">语文</a><a href="/tags/%E4%BA%8C%E5%88%86%E5%9B%BE/" style="font-size: 1.15em; color: rgb(181, 173, 46)">二分图</a><a href="/tags/%E9%93%BE%E5%BC%8F%E5%89%8D%E5%90%91%E6%98%9F/" style="font-size: 1.15em; color: rgb(72, 154, 6)">链式前向星</a><a href="/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/" style="font-size: 1.15em; color: rgb(140, 142, 168)">最小生成树</a><a href="/tags/%E6%9C%80%E7%9F%AD%E8%B7%9D%E7%A6%BB/" style="font-size: 1.15em; color: rgb(176, 50, 106)">最短距离</a><a href="/tags/mlp/" style="font-size: 1.15em; color: rgb(43, 169, 155)">mlp</a><a href="/tags/java/" style="font-size: 1.3em; color: rgb(102, 108, 112)">java</a><a href="/tags/%E6%95%B0%E5%AD%97%E9%80%BB%E8%BE%91/" style="font-size: 1.15em; color: rgb(133, 92, 124)">数字逻辑</a><a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 1.15em; color: rgb(187, 28, 169)">遗传算法</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><span class="card-archive-list-count">12</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/10/"><span class="card-archive-list-date">十月 2021</span><span class="card-archive-list-count">6</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">36</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">40.7k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-08-05T08:21:46.595Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://xc-figure.oss-cn-hangzhou.aliyuncs.com/blogImg/b4c744affd761ee33e8d78b900e090e.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By XuCong</div><div class="footer_custom_text">Hi, welcome to xclovehsy's blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  getScript('https://sdk.jinrishici.com/v2/browser/jinrishici.js').then(() => {
    jinrishici.load(result =>{
      if (true) {
        const sub = []
        const content = result.data.content
        sub.unshift(content)
        typedJSFn.init(sub)
      } else {
        document.getElementById('subtitle').textContent = result.data.content
      }
    })
  })
}
typedJSFn.run(subtitleType)
</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>